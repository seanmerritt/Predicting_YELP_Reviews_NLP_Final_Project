{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "10seqSSOBVOZ",
        "f9PQ7ER3COSh",
        "ZJpUazTCCc7M",
        "VyYOYwg9DUzK",
        "FKdBK6pqXNxb",
        "DNhOoVWSdb1s",
        "V2i8SdCLy1jH",
        "bwW49H_cy5za",
        "QeEy3jtp0azV",
        "B0F_7viw8uE8",
        "_cpdvDWq8xLV",
        "oUyzdynCBVLl",
        "J0HfjpAZlrc6",
        "Bxko2uAtvbOz",
        "8G_OwUvxa_Qa",
        "H5vUXiWrvqE8"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanmerritt/Predicting_YELP_Reviews_NLP_Final_Project/blob/main/NLP_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5gM8sYiUciN"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QDy5GQPBzDo"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PMBFcb7cbYe"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5a8aDI9p-vQ"
      },
      "source": [
        "## Crawl websites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhrHI3FZqDk2"
      },
      "source": [
        "# Grab review content: username, rating, rating date, rounded rating, and review content\n",
        "# Mark each review with doctorID\n",
        "def getReviewContent(result: ResultSet, index: int, soup: BeautifulSoup):\n",
        "    name_list = []\n",
        "    rating_list = []\n",
        "    date_list = []\n",
        "    review_content_list = []\n",
        "    rounde_rating = soup.find_all(class_ = 'i-stars__373c0___sZu0')[0]['aria-label'].split()[0]\n",
        "    for i in range(len(result)):\n",
        "        review_number = len(result[i].find_all(class_ = 'comment__373c0__Nsutg'))\n",
        "        if review_number > 1:\n",
        "            name = result[i].find(class_ = 'fs-block css-m6anxm').text\n",
        "            rating = result[i].find_all(class_ = 'i-stars__373c0___sZu0')\n",
        "            date = result[i].find_all(class_ = 'css-e81eai')\n",
        "            review_content = result[i].find_all(class_='comment__373c0__Nsutg')\n",
        "            for j in range(review_number):\n",
        "                name_list.append(name)\n",
        "                rating_list.append(rating[j]['aria-label'].split()[0])\n",
        "                date_list.append(date[j].text)\n",
        "                review_content_list.append(review_content[j].text)\n",
        "        else:\n",
        "            name = result[i].find(class_ = 'fs-block css-m6anxm').text\n",
        "            rating = result[i].find(class_ = 'i-stars__373c0___sZu0')['aria-label'].split()[0]\n",
        "            date = result[i].find(class_ = 'css-e81eai').text\n",
        "            review_content = result[i].find(class_='comment__373c0__Nsutg').text\n",
        "\n",
        "            name_list.append(name)\n",
        "            rating_list.append(rating)\n",
        "            date_list.append(date)\n",
        "            review_content_list.append(review_content)\n",
        "    return {'doctorID': [index]*len(name_list), \n",
        "            'username': name_list, \n",
        "            'rating': rating_list, \n",
        "            'date_of_review': date_list, \n",
        "            'review_content': review_content_list, \n",
        "            'rounded_rating': [rounde_rating]*len(name_list)}\n",
        "\n",
        "# Confirm if current page has reviews. return a boolean value.\n",
        "def haveReview(result: ResultSet):\n",
        "    if len(result) == 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "# Confirm if current url is the same as url in original dataset, and return the current url on yelp.\n",
        "def getCurrentUrl(url: str):\n",
        "    try:\n",
        "        res = urllib.request.urlopen(url)\n",
        "        finalurl = res.geturl()\n",
        "        if finalurl==url:\n",
        "            pass\n",
        "        else:\n",
        "            url = finalurl\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise SystemExit(e)\n",
        "    return url\n",
        "\n",
        "# Main function to crawl websites\n",
        "def main():\n",
        "    temp_list = []\n",
        "    df_url = pd.read_excel('file path of dataset storing urls')\n",
        "\n",
        "# the number in range refers to doctorID in original dataset.\n",
        "    for i in range(294, 296):\n",
        "        url = df_url['URL'][i]\n",
        "        url = getCurrentUrl(url)\n",
        "        print(\"the \" + str(i) + \"th URL is being crawled\")\n",
        "        try:\n",
        "            response = requests.get(url).text\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            raise SystemExit(e)\n",
        "        time.sleep(random.randint(20, 30))\n",
        "        # Use BeautifulSoup to parse HTML\n",
        "        soup = BeautifulSoup(response,'html.parser')\n",
        "        result = soup.findAll(class_=\"review__373c0__3MsBX\")\n",
        "        index = 0\n",
        "        # a while loop to go through all subpages of each business, until there is no review in current page.\n",
        "        while(haveReview(result)):\n",
        "            temp_dict = getReviewContent(result, i, soup)\n",
        "            temp_list.append(temp_dict)\n",
        "            index += 1\n",
        "            try:\n",
        "                response = requests.get(url + \"?start=\" + str(index*10)).text\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                raise SystemExit(e)\n",
        "            # Use BeautifulSoup to parse HTML\n",
        "            soup = BeautifulSoup(response,'html.parser')\n",
        "            result = soup.findAll(class_=\"review__373c0__3MsBX\")\n",
        "        print(\"the \" + str(i) + \"th URL is finished\")\n",
        "    \n",
        "    df = pd.DataFrame(temp_list[0])\n",
        "    for i in range(1, len(temp_list)):\n",
        "        tmp_df = pd.DataFrame(temp_list[i])\n",
        "        df = df.append(tmp_df, ignore_index=True)\n",
        "    df.to_csv('file path to store data', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaTLCEhysYuy"
      },
      "source": [
        "## Compile dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUV7Ael29vyQ",
        "outputId": "ae61fd14-3e59-4389-ad6b-7557cfc270f7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaZ5WYGxXx-B"
      },
      "source": [
        "# This is a function to create rounded rating according to average rating of business\n",
        "def roundRating(rating: float):\n",
        "    newRating = int(rating)\n",
        "    dif = rating - newRating\n",
        "    if dif >= 0.75:\n",
        "        return newRating + 1\n",
        "    elif dif < 0.75 and dif >= 0.25:\n",
        "        return newRating + 0.5\n",
        "    else:\n",
        "        return newRating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HgutS5iAVvJ"
      },
      "source": [
        "df_all = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/url_0_to_2606.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "rqUZHJQEC4bw",
        "outputId": "f23fbef6-103e-47c3-e420-0d674763a754"
      },
      "source": [
        "df_all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>George S.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/3/2021</td>\n",
              "      <td>Love this healthcare provider!! Dr Singh is th...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Andrew P.</td>\n",
              "      <td>1</td>\n",
              "      <td>12/15/2020</td>\n",
              "      <td>Ravinder Singh attempted to scold me once I fo...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Brian S.</td>\n",
              "      <td>5</td>\n",
              "      <td>12/31/2020</td>\n",
              "      <td>The docs in this office are always professiona...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Mina R.</td>\n",
              "      <td>5</td>\n",
              "      <td>4/11/2021</td>\n",
              "      <td>Dr. Singh is one of the best physicians I've c...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52686</th>\n",
              "      <td>2367</td>\n",
              "      <td>C S.</td>\n",
              "      <td>5</td>\n",
              "      <td>4/29/2019</td>\n",
              "      <td>Excellent Doctor, he determined my problem, ra...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52687</th>\n",
              "      <td>2367</td>\n",
              "      <td>Breanna S.</td>\n",
              "      <td>1</td>\n",
              "      <td>6/21/2017</td>\n",
              "      <td>I waited well over a month for an appointment ...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52688</th>\n",
              "      <td>2367</td>\n",
              "      <td>Tiffany T.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/5/2019</td>\n",
              "      <td>Dr. Saad was very kind and comforting. He seem...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52689</th>\n",
              "      <td>2367</td>\n",
              "      <td>Dee C.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/25/2017</td>\n",
              "      <td>A kind, intelligent physician who listens, and...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52690</th>\n",
              "      <td>2367</td>\n",
              "      <td>Sky B.</td>\n",
              "      <td>1</td>\n",
              "      <td>4/11/2017</td>\n",
              "      <td>Waited 3 months for my appintment. They schedu...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52691 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       doctorID  ... rounded_rating\n",
              "0             0  ...            4.5\n",
              "1             0  ...            4.5\n",
              "2             0  ...            4.5\n",
              "3             0  ...            4.5\n",
              "4             0  ...            4.5\n",
              "...         ...  ...            ...\n",
              "52686      2367  ...            4.0\n",
              "52687      2367  ...            4.0\n",
              "52688      2367  ...            4.0\n",
              "52689      2367  ...            4.0\n",
              "52690      2367  ...            4.0\n",
              "\n",
              "[52691 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy8rWhwNDRAA"
      },
      "source": [
        "df_all_sorted = df_all.sort_values(by=['doctorID'], ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exp2vaZzDa7J"
      },
      "source": [
        "df_all_sorted = df_all_sorted.reset_index().drop(['index'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "mgSq790QD0s5",
        "outputId": "158958b2-c4a9-4df6-d9e3-c9468e7e8200"
      },
      "source": [
        "df_all_sorted.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Brian J.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>Large waiting room and welcoming  staff. Dr. S...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Stephani P.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/2/2015</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   doctorID  ... rounded_rating\n",
              "0         0  ...            4.5\n",
              "1         0  ...            4.5\n",
              "2         0  ...            4.5\n",
              "3         0  ...            4.5\n",
              "4         0  ...            4.5\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk1ncSEkcUOu"
      },
      "source": [
        "df_all_sorted.to_csv('/group_project/url_0_to_2606_sorted.csv', index=False)/group_project/url_0_to_2606_sorted.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zONeKNReCC9",
        "outputId": "cefafd31-7aea-4ed3-9d9a-64dd1d81a5b3"
      },
      "source": [
        "num_list = [i for i in range(0, 2606)]\n",
        "doctorIDs = list(df_all_sorted['doctorID'])\n",
        "missing_ID = []\n",
        "for num in num_list:\n",
        "  if num not in doctorIDs:\n",
        "    missing_ID.append(num)\n",
        "print(len(missing_ID))\n",
        "print(missing_ID)\n",
        "# These are the missing doctorIDs in the dataset. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112\n",
            "[16, 60, 65, 126, 151, 164, 221, 241, 249, 271, 278, 283, 301, 315, 318, 329, 333, 340, 350, 386, 401, 402, 416, 418, 423, 427, 435, 448, 450, 462, 465, 471, 479, 490, 491, 494, 495, 498, 503, 509, 515, 523, 561, 575, 604, 636, 638, 640, 784, 795, 804, 815, 829, 870, 879, 970, 971, 973, 979, 992, 1009, 1010, 1017, 1050, 1061, 1064, 1096, 1110, 1134, 1162, 1188, 1190, 1197, 1448, 1476, 1499, 1523, 1529, 1554, 1603, 1607, 1639, 1651, 1675, 1738, 1765, 1767, 1777, 1848, 1894, 1917, 2034, 2090, 2099, 2107, 2129, 2134, 2143, 2193, 2208, 2223, 2233, 2249, 2289, 2377, 2431, 2432, 2433, 2434, 2435, 2436, 2437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY0zIa_XUd4t"
      },
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1j_mFgFBN4Z"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8tfki5pA9iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e34377a-8f78-4de5-de79-a00848dc6e6e"
      },
      "source": [
        "# 4/1AX4XfWiwqqnbmlSsHv7Hg9CDf-amMUeZpLPFuDlcly2fTlHaBROfrOkcgpI\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSUEVlr6-Med",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a35a3c0-29e1-4617-b5cd-4aa6eb16a8b9"
      },
      "source": [
        "import pandas as pd\n",
        "# read a local csv file into Pandas DataFrame\n",
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df= pd.read_csv('url_0_to_2606_sorted.csv')\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10seqSSOBVOZ"
      },
      "source": [
        "## Contraction expansion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g--1LloIBwXR",
        "outputId": "aa2bd827-0f2e-4418-9540-4db334c28333"
      },
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.2.0-py3-none-any.whl (283 kB)\n",
            "\u001b[K     |████████████████████████████████| 283 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 36.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85450 sha256=4ecfe669e2fdfdd0ff0af80843cb7a8478c0244f3d0e05b37a7401aad8b88e4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSyVhimXBnbO"
      },
      "source": [
        "def contraction_expansion(text):\n",
        "  expanded_word = []    \n",
        "  for word in text.split():\n",
        "  # using contractions.fix to expand\n",
        "    expanded_word.append(contractions.fix(word))   \n",
        "   \n",
        "  return  ' '.join(expanded_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsifXUTqB5qr"
      },
      "source": [
        "df['expanded'] = df['review_content'].apply(contraction_expansion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9PQ7ER3COSh"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7xVPwpJCRTJ",
        "outputId": "a1d00063-de08-4673-efed-942005d9c213"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import wordpunct_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkHa8QpcCVI0"
      },
      "source": [
        "df['tokens'] = df['expanded'].apply(wordpunct_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJpUazTCCc7M"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm7_qr1lChf1",
        "outputId": "52acb117-bd16-4cd7-db18-1cde42b6dcd5"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "#create a function to lemmatize words based on different POS tag types; \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdJd_1WICkj2"
      },
      "source": [
        "df['tags'] = df['tokens'].apply(pos_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0noo6PqC9-m"
      },
      "source": [
        "def lemmatize_POS (tagged_columns):\n",
        "  wnl = WordNetLemmatizer()\n",
        "  text=[]\n",
        "  for word, tag in tagged_columns:\n",
        "    if tag.startswith('NN'):\n",
        "      token = wnl.lemmatize(word, pos='n') # n is NOUN \n",
        "    elif tag.startswith('VB'):\n",
        "      token = wnl.lemmatize(word, pos='v') # v is VERB\n",
        "    elif tag.startswith('JJ'):\n",
        "      token = wnl.lemmatize(word, pos='a') # a is ADJ\n",
        "    elif tag.startswith('RB'):\n",
        "      token = wnl.lemmatize(word, pos='r') # r is ADV\n",
        "    else:\n",
        "      token = word\n",
        "    text.append(token)\n",
        "  #cleaned_corpus.append(text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W-QhnFnDHNA"
      },
      "source": [
        "df['lemmas'] = df['tags'].apply(lemmatize_POS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyYOYwg9DUzK"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb6RIsbUDWhB",
        "outputId": "05c60266-26a1-493c-fd06-73a5c979387a"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from string import punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8KNp6EMDXP-"
      },
      "source": [
        "def preprocessing(tokens):\n",
        "  tokens = [token.lower() for token in tokens] # lowercasing\n",
        "  \n",
        "  tokens = [token for token in tokens if not token.isdigit()] # remove digit - isdigit is a python buit-in method. \n",
        "\n",
        "  tokens = [token for token in tokens if token not in punctuation] # remove punctuations\n",
        "\n",
        "  mystopwords = set(stopwords.words(\"english\")) # we use english stopwords list.\n",
        "  tokens = [token for token in tokens if token not in mystopwords] # remove stopwords\n",
        "  \n",
        "  tokens = [token for token in tokens if len(token)>=3] # remove tokens with one or two characters\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O9Nl00WDaLd"
      },
      "source": [
        "df['final'] = df['lemmas'].apply(preprocessing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE9peyKUGDNg",
        "outputId": "23dcf435-27c7-4f12-91dc-38ccce52409f"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>[After, seeing, 4, ., 5, stars, reviews, on, a...</td>\n",
              "      <td>[(After, IN), (seeing, VBG), (4, CD), (., .), ...</td>\n",
              "      <td>[After, see, 4, ., 5, star, review, on, a, fam...</td>\n",
              "      <td>[see, star, review, family, practice, rancho, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>[I, just, switched, this, month, to, Rancho, W...</td>\n",
              "      <td>[(I, PRP), (just, RB), (switched, VBD), (this,...</td>\n",
              "      <td>[I, just, switch, this, month, to, Rancho, Wel...</td>\n",
              "      <td>[switch, month, rancho, wellness, base, referr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, are, very...</td>\n",
              "      <td>[(I, PRP), (love, VBP), (Rancho, NNP), (wellne...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, be, very,...</td>\n",
              "      <td>[love, rancho, wellness, organize, professiona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Brian J.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>Large waiting room and welcoming  staff. Dr. S...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Large waiting room and welcoming staff. Dr. Si...</td>\n",
              "      <td>[Large, waiting, room, and, welcoming, staff, ...</td>\n",
              "      <td>[(Large, JJ), (waiting, VBG), (room, NN), (and...</td>\n",
              "      <td>[Large, wait, room, and, welcoming, staff, ., ...</td>\n",
              "      <td>[large, wait, room, welcoming, staff, singh, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Stephani P.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/2/2015</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>[My, family, and, I, have, been, going, to, Dr...</td>\n",
              "      <td>[(My, PRP$), (family, NN), (and, CC), (I, PRP)...</td>\n",
              "      <td>[My, family, and, I, have, be, go, to, Dr, ., ...</td>\n",
              "      <td>[family, singh, year, actually, follow, anothe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                              final\n",
              "0           0  ...  [see, star, review, family, practice, rancho, ...\n",
              "1           1  ...  [switch, month, rancho, wellness, base, referr...\n",
              "2           2  ...  [love, rancho, wellness, organize, professiona...\n",
              "3           3  ...  [large, wait, room, welcoming, staff, singh, p...\n",
              "4           4  ...  [family, singh, year, actually, follow, anothe...\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfDf_ZLCGSjN",
        "outputId": "27feedd5-dc20-4d35-cefc-67ba6dc69c5b"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df.to_pickle('prepared_data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daf93cnMUfbt"
      },
      "source": [
        "# Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AsC9f-kzeI6"
      },
      "source": [
        "* word embeddings - Sean\n",
        "* tf-idf - Sonia\n",
        "* NER tags - Siyu\n",
        "* summary statistics - Sean \n",
        "* aggregated by business"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQUMYCLeWKm7",
        "outputId": "a3086ca0-675a-4405-d61c-58f205e87f3c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "# read a local csv file into Pandas DataFrame\n",
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df = pd.read_pickle('prepared_data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEu6V6KRab2Z"
      },
      "source": [
        "## Summary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnoMB9fhaawF"
      },
      "source": [
        "def lexical_diversity(text):\n",
        "    \"\"\"\n",
        "    A measure of the lexical richness of the text\n",
        "    \"\"\"\n",
        "    return len(text)/(len(set(text))+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsy3mz_7ajA-"
      },
      "source": [
        "review_table = []\n",
        "for index, row in df.iterrows():\n",
        "  review_table.append([len(row['tokens']), len(row['final']), lexical_diversity(row['final'])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "XyNSuUyRatoa",
        "outputId": "10394443-61e6-4b82-84d8-cdaab2ad8850"
      },
      "source": [
        "ReviewTable = pd.DataFrame(review_table, columns = ['Message Length', 'Clean Length', 'Lexical Diversity'])\n",
        "ReviewTable[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message Length</th>\n",
              "      <th>Clean Length</th>\n",
              "      <th>Lexical Diversity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>74</td>\n",
              "      <td>32</td>\n",
              "      <td>1.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>148</td>\n",
              "      <td>59</td>\n",
              "      <td>1.229167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "      <td>39</td>\n",
              "      <td>1.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43</td>\n",
              "      <td>18</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>95</td>\n",
              "      <td>33</td>\n",
              "      <td>1.137931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Message Length  Clean Length  Lexical Diversity\n",
              "0              74            32           1.142857\n",
              "1             148            59           1.229167\n",
              "2              90            39           1.181818\n",
              "3              43            18           1.000000\n",
              "4              95            33           1.137931"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "8nj7BXPLa8AO",
        "outputId": "0e49ae1f-349f-4067-9660-027e8d83a6f5"
      },
      "source": [
        "ReviewTable.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message Length</th>\n",
              "      <th>Clean Length</th>\n",
              "      <th>Lexical Diversity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>52691.000000</td>\n",
              "      <td>52691.000000</td>\n",
              "      <td>52691.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>134.395912</td>\n",
              "      <td>53.765634</td>\n",
              "      <td>1.135090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>129.286803</td>\n",
              "      <td>50.591810</td>\n",
              "      <td>0.183093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>54.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>95.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>1.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>166.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>1.222222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1159.000000</td>\n",
              "      <td>467.000000</td>\n",
              "      <td>17.142857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Message Length  Clean Length  Lexical Diversity\n",
              "count    52691.000000  52691.000000       52691.000000\n",
              "mean       134.395912     53.765634           1.135090\n",
              "std        129.286803     50.591810           0.183093\n",
              "min          1.000000      0.000000           0.000000\n",
              "25%         54.000000     22.000000           1.000000\n",
              "50%         95.000000     38.000000           1.107143\n",
              "75%        166.000000     66.000000           1.222222\n",
              "max       1159.000000    467.000000          17.142857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "nYWgw0C9itlF",
        "outputId": "697cbf22-3e54-4a10-e6aa-9713351e9bd0"
      },
      "source": [
        "ReviewTable.corr()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message Length</th>\n",
              "      <th>Clean Length</th>\n",
              "      <th>Lexical Diversity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Message Length</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992989</td>\n",
              "      <td>0.731130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Clean Length</th>\n",
              "      <td>0.992989</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.731535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lexical Diversity</th>\n",
              "      <td>0.731130</td>\n",
              "      <td>0.731535</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Message Length  Clean Length  Lexical Diversity\n",
              "Message Length           1.000000      0.992989           0.731130\n",
              "Clean Length             0.992989      1.000000           0.731535\n",
              "Lexical Diversity        0.731130      0.731535           1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJw3HK3SjOWR"
      },
      "source": [
        "df['Length'] = df['final'].apply(len)\n",
        "df['Lexical Diversity'] = df['final'].apply(lexical_diversity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uID4YUV7rtZ",
        "outputId": "4162fdca-8a97-42ec-e324-fc207958a3b3"
      },
      "source": [
        "print(df['rating'].describe(),df['rounded_rating'].describe())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count    52691.000000\n",
            "mean         3.091704\n",
            "std          1.889864\n",
            "min          1.000000\n",
            "25%          1.000000\n",
            "50%          4.000000\n",
            "75%          5.000000\n",
            "max          5.000000\n",
            "Name: rating, dtype: float64 count    52691.000000\n",
            "mean         3.100330\n",
            "std          0.990923\n",
            "min          1.000000\n",
            "25%          2.500000\n",
            "50%          3.000000\n",
            "75%          4.000000\n",
            "max          5.000000\n",
            "Name: rounded_rating, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRQJjtuv8XsU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-m9fC_z-oPF",
        "outputId": "18c3a0bb-820e-44a0-f591-3697d87dc467"
      },
      "source": [
        "df['rating'].value_counts().sort_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    21729\n",
              "2     2606\n",
              "3     1566\n",
              "4     2684\n",
              "5    24106\n",
              "Name: rating, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "jR2_8Kxc8tjt",
        "outputId": "e7e59cb6-516b-4d3f-8564-4da4eb9c4edd"
      },
      "source": [
        "ax = df['rating'].value_counts().sort_index().plot(kind='bar',\n",
        "                                    figsize=(14,8),\n",
        "                                    title=\"Count Ratings\")\n",
        "ax.set_xlabel(\"Ratings\")\n",
        "ax.set_ylabel(\"Frequency\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Frequency')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "8YYP0yAs-JGj",
        "outputId": "68d36fbd-895f-43a9-ef4f-d00ef0bff92d"
      },
      "source": [
        "ax = df['rounded_rating'].value_counts().sort_index().plot(kind='bar',\n",
        "                                    figsize=(14,8),\n",
        "                                    title=\"Count Rounded Ratings\")\n",
        "ax.set_xlabel(\"Rounded Ratings\")\n",
        "ax.set_ylabel(\"Frequency\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Frequency')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKdBK6pqXNxb"
      },
      "source": [
        "## Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3woRc8YyXRFf",
        "outputId": "741a012b-25b9-4952-d99e-90fc523851e3"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "pre_ft_vectors = api.load('glove-twitter-200')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gBYRgKjXtfj"
      },
      "source": [
        "review_list = df['final'].to_list()\n",
        "review_list = [' '.join(message) for message in review_list]\n",
        "corpus = [doc.split() for doc in review_list]\n",
        "id2word = gensim.corpora.Dictionary(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnmLxG1ParKp"
      },
      "source": [
        "from gensim.corpora import Dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F39l_fIoaOzE"
      },
      "source": [
        "pickle.dump(corpus, open('corpus.pkl','wb'))\n",
        "pickle.dump(id2word, open('id2word.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_rmC3duamE1"
      },
      "source": [
        "one_perc = len(df.index)*0.01\n",
        "# create a token list with a minimal document frequency count (number of documents which a token occurs)\n",
        "token_list =[]\n",
        "for token, count in id2word.dfs.items(): # .dfs returns doctument frequecy based on token id (int)\n",
        "  if count > one_perc:\n",
        "    token_list.append(id2word.get(token)) #.get() retrieve the actual tokens based on id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvVb9YKBXwRq"
      },
      "source": [
        "vector_list = [pre_ft_vectors[word] for word in token_list if word in pre_ft_vectors.vocab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fy8xFPIXxwC"
      },
      "source": [
        "words = [word for word in token_list if word in pre_ft_vectors.vocab]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMkXn_2bXziD"
      },
      "source": [
        "vec_zip = zip(token_list, vector_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "GAqfBOngX6qi",
        "outputId": "26e37076-cfa7-4d08-edc3-473f53c7b60c"
      },
      "source": [
        "word_vec_dict = dict(vec_zip)\n",
        "df_word = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
        "df_word.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>see</th>\n",
              "      <td>0.381280</td>\n",
              "      <td>0.952290</td>\n",
              "      <td>-0.22360</td>\n",
              "      <td>-0.000701</td>\n",
              "      <td>-0.25439</td>\n",
              "      <td>0.501150</td>\n",
              "      <td>1.03390</td>\n",
              "      <td>0.137270</td>\n",
              "      <td>-0.098981</td>\n",
              "      <td>0.29085</td>\n",
              "      <td>0.075632</td>\n",
              "      <td>0.259070</td>\n",
              "      <td>-0.787890</td>\n",
              "      <td>-0.098828</td>\n",
              "      <td>-0.318430</td>\n",
              "      <td>-0.22705</td>\n",
              "      <td>0.219920</td>\n",
              "      <td>-0.256800</td>\n",
              "      <td>-0.023992</td>\n",
              "      <td>-0.22362</td>\n",
              "      <td>0.209010</td>\n",
              "      <td>-0.063092</td>\n",
              "      <td>-0.019972</td>\n",
              "      <td>0.51637</td>\n",
              "      <td>-0.010969</td>\n",
              "      <td>0.54527</td>\n",
              "      <td>-0.055798</td>\n",
              "      <td>-0.212030</td>\n",
              "      <td>0.228810</td>\n",
              "      <td>-0.467870</td>\n",
              "      <td>-0.13539</td>\n",
              "      <td>0.42674</td>\n",
              "      <td>-0.018245</td>\n",
              "      <td>0.108020</td>\n",
              "      <td>-0.129970</td>\n",
              "      <td>0.097118</td>\n",
              "      <td>0.14715</td>\n",
              "      <td>-0.107070</td>\n",
              "      <td>0.189470</td>\n",
              "      <td>-0.110050</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012852</td>\n",
              "      <td>0.14578</td>\n",
              "      <td>-0.18293</td>\n",
              "      <td>0.111010</td>\n",
              "      <td>-0.317700</td>\n",
              "      <td>-0.056756</td>\n",
              "      <td>-0.16985</td>\n",
              "      <td>-0.299240</td>\n",
              "      <td>-0.353910</td>\n",
              "      <td>0.33189</td>\n",
              "      <td>-0.405190</td>\n",
              "      <td>0.22092</td>\n",
              "      <td>-0.038152</td>\n",
              "      <td>0.34441</td>\n",
              "      <td>-0.216490</td>\n",
              "      <td>0.18566</td>\n",
              "      <td>-0.196000</td>\n",
              "      <td>-0.200140</td>\n",
              "      <td>-0.025878</td>\n",
              "      <td>-0.22317</td>\n",
              "      <td>0.33448</td>\n",
              "      <td>0.162810</td>\n",
              "      <td>0.31304</td>\n",
              "      <td>-0.16490</td>\n",
              "      <td>-0.245370</td>\n",
              "      <td>-0.181240</td>\n",
              "      <td>0.58900</td>\n",
              "      <td>-0.246050</td>\n",
              "      <td>0.19595</td>\n",
              "      <td>0.28030</td>\n",
              "      <td>-0.028354</td>\n",
              "      <td>0.353060</td>\n",
              "      <td>0.203930</td>\n",
              "      <td>-0.34911</td>\n",
              "      <td>-0.088513</td>\n",
              "      <td>-0.478440</td>\n",
              "      <td>0.52337</td>\n",
              "      <td>-0.407160</td>\n",
              "      <td>-0.109480</td>\n",
              "      <td>-0.249810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>star</th>\n",
              "      <td>-0.082023</td>\n",
              "      <td>0.301080</td>\n",
              "      <td>-0.51508</td>\n",
              "      <td>-0.659520</td>\n",
              "      <td>-0.14611</td>\n",
              "      <td>-0.611460</td>\n",
              "      <td>-0.26568</td>\n",
              "      <td>-0.115780</td>\n",
              "      <td>0.656240</td>\n",
              "      <td>-0.68257</td>\n",
              "      <td>0.030081</td>\n",
              "      <td>-0.077002</td>\n",
              "      <td>-0.122150</td>\n",
              "      <td>0.727990</td>\n",
              "      <td>-0.049490</td>\n",
              "      <td>0.11321</td>\n",
              "      <td>0.543070</td>\n",
              "      <td>-0.131100</td>\n",
              "      <td>0.047132</td>\n",
              "      <td>-0.33657</td>\n",
              "      <td>-0.247140</td>\n",
              "      <td>-0.362380</td>\n",
              "      <td>0.266210</td>\n",
              "      <td>-0.56173</td>\n",
              "      <td>-0.001253</td>\n",
              "      <td>-0.31764</td>\n",
              "      <td>-0.529790</td>\n",
              "      <td>-0.258670</td>\n",
              "      <td>0.231630</td>\n",
              "      <td>-0.148280</td>\n",
              "      <td>0.67383</td>\n",
              "      <td>-0.25845</td>\n",
              "      <td>0.137580</td>\n",
              "      <td>-0.293200</td>\n",
              "      <td>0.125820</td>\n",
              "      <td>0.603140</td>\n",
              "      <td>0.70976</td>\n",
              "      <td>0.381670</td>\n",
              "      <td>0.320950</td>\n",
              "      <td>-0.541920</td>\n",
              "      <td>...</td>\n",
              "      <td>0.089628</td>\n",
              "      <td>0.56048</td>\n",
              "      <td>-0.12480</td>\n",
              "      <td>-0.348510</td>\n",
              "      <td>-0.366770</td>\n",
              "      <td>0.454830</td>\n",
              "      <td>0.17677</td>\n",
              "      <td>-0.024119</td>\n",
              "      <td>-0.086595</td>\n",
              "      <td>-0.16427</td>\n",
              "      <td>-0.081961</td>\n",
              "      <td>-0.02514</td>\n",
              "      <td>0.257180</td>\n",
              "      <td>0.48749</td>\n",
              "      <td>0.946640</td>\n",
              "      <td>0.54783</td>\n",
              "      <td>0.092482</td>\n",
              "      <td>-0.186610</td>\n",
              "      <td>-0.502150</td>\n",
              "      <td>0.48761</td>\n",
              "      <td>0.32144</td>\n",
              "      <td>-0.246030</td>\n",
              "      <td>0.16073</td>\n",
              "      <td>-0.44192</td>\n",
              "      <td>0.013751</td>\n",
              "      <td>0.339960</td>\n",
              "      <td>0.19815</td>\n",
              "      <td>-0.044714</td>\n",
              "      <td>0.15840</td>\n",
              "      <td>0.74563</td>\n",
              "      <td>-0.088035</td>\n",
              "      <td>-0.377520</td>\n",
              "      <td>-0.164360</td>\n",
              "      <td>-0.64797</td>\n",
              "      <td>0.390690</td>\n",
              "      <td>0.025142</td>\n",
              "      <td>0.40283</td>\n",
              "      <td>-0.062974</td>\n",
              "      <td>-0.013045</td>\n",
              "      <td>-0.898590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>review</th>\n",
              "      <td>-0.106200</td>\n",
              "      <td>0.379150</td>\n",
              "      <td>-0.41743</td>\n",
              "      <td>-0.457380</td>\n",
              "      <td>-0.28988</td>\n",
              "      <td>-0.095960</td>\n",
              "      <td>0.26371</td>\n",
              "      <td>-0.005914</td>\n",
              "      <td>0.172610</td>\n",
              "      <td>-0.84185</td>\n",
              "      <td>-0.117890</td>\n",
              "      <td>0.140080</td>\n",
              "      <td>-0.999540</td>\n",
              "      <td>-0.037367</td>\n",
              "      <td>0.089276</td>\n",
              "      <td>-0.27306</td>\n",
              "      <td>-0.026919</td>\n",
              "      <td>-0.195370</td>\n",
              "      <td>0.383860</td>\n",
              "      <td>0.12481</td>\n",
              "      <td>0.097774</td>\n",
              "      <td>-0.307300</td>\n",
              "      <td>-0.322820</td>\n",
              "      <td>0.63777</td>\n",
              "      <td>-0.237020</td>\n",
              "      <td>0.45727</td>\n",
              "      <td>-0.082979</td>\n",
              "      <td>0.119080</td>\n",
              "      <td>-0.030425</td>\n",
              "      <td>-0.641100</td>\n",
              "      <td>0.46163</td>\n",
              "      <td>-0.10976</td>\n",
              "      <td>-0.444950</td>\n",
              "      <td>-0.576150</td>\n",
              "      <td>0.201300</td>\n",
              "      <td>-0.158640</td>\n",
              "      <td>0.34249</td>\n",
              "      <td>0.041172</td>\n",
              "      <td>0.505400</td>\n",
              "      <td>-0.061431</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.223580</td>\n",
              "      <td>0.24148</td>\n",
              "      <td>0.48776</td>\n",
              "      <td>-0.032543</td>\n",
              "      <td>0.231260</td>\n",
              "      <td>0.865050</td>\n",
              "      <td>-0.24501</td>\n",
              "      <td>-0.408440</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.54045</td>\n",
              "      <td>0.666130</td>\n",
              "      <td>0.62770</td>\n",
              "      <td>0.686070</td>\n",
              "      <td>-0.15754</td>\n",
              "      <td>-0.064552</td>\n",
              "      <td>-0.30684</td>\n",
              "      <td>0.457360</td>\n",
              "      <td>-0.479620</td>\n",
              "      <td>-0.055276</td>\n",
              "      <td>-0.23605</td>\n",
              "      <td>0.31689</td>\n",
              "      <td>0.069733</td>\n",
              "      <td>0.38509</td>\n",
              "      <td>0.52910</td>\n",
              "      <td>-0.030096</td>\n",
              "      <td>0.375660</td>\n",
              "      <td>-0.59556</td>\n",
              "      <td>-0.472760</td>\n",
              "      <td>-0.17509</td>\n",
              "      <td>0.32066</td>\n",
              "      <td>0.433220</td>\n",
              "      <td>0.138350</td>\n",
              "      <td>0.609270</td>\n",
              "      <td>0.54161</td>\n",
              "      <td>-0.199780</td>\n",
              "      <td>0.051260</td>\n",
              "      <td>-0.43582</td>\n",
              "      <td>0.215970</td>\n",
              "      <td>0.372850</td>\n",
              "      <td>-0.136990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>family</th>\n",
              "      <td>-0.204030</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>-0.20134</td>\n",
              "      <td>-0.497320</td>\n",
              "      <td>0.34645</td>\n",
              "      <td>-0.224730</td>\n",
              "      <td>0.34272</td>\n",
              "      <td>0.138450</td>\n",
              "      <td>0.057026</td>\n",
              "      <td>-0.14241</td>\n",
              "      <td>-0.212760</td>\n",
              "      <td>-0.570470</td>\n",
              "      <td>-0.588380</td>\n",
              "      <td>0.383220</td>\n",
              "      <td>0.048775</td>\n",
              "      <td>0.47896</td>\n",
              "      <td>-0.426170</td>\n",
              "      <td>0.032514</td>\n",
              "      <td>-0.148620</td>\n",
              "      <td>-0.16056</td>\n",
              "      <td>0.404310</td>\n",
              "      <td>0.074329</td>\n",
              "      <td>-0.344730</td>\n",
              "      <td>0.36171</td>\n",
              "      <td>-0.166510</td>\n",
              "      <td>0.57312</td>\n",
              "      <td>-0.337200</td>\n",
              "      <td>-0.096376</td>\n",
              "      <td>-0.140830</td>\n",
              "      <td>-0.095923</td>\n",
              "      <td>-0.32455</td>\n",
              "      <td>-0.39781</td>\n",
              "      <td>-0.121600</td>\n",
              "      <td>0.384430</td>\n",
              "      <td>-0.031900</td>\n",
              "      <td>0.159670</td>\n",
              "      <td>0.45478</td>\n",
              "      <td>0.171590</td>\n",
              "      <td>-0.086675</td>\n",
              "      <td>-0.186340</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106780</td>\n",
              "      <td>0.21994</td>\n",
              "      <td>-0.78250</td>\n",
              "      <td>0.231830</td>\n",
              "      <td>-0.110130</td>\n",
              "      <td>-0.058964</td>\n",
              "      <td>-0.33186</td>\n",
              "      <td>0.059253</td>\n",
              "      <td>0.802410</td>\n",
              "      <td>0.15400</td>\n",
              "      <td>-0.214850</td>\n",
              "      <td>0.27228</td>\n",
              "      <td>0.160630</td>\n",
              "      <td>-0.22833</td>\n",
              "      <td>-0.012848</td>\n",
              "      <td>-0.64909</td>\n",
              "      <td>0.419530</td>\n",
              "      <td>-0.143580</td>\n",
              "      <td>0.635550</td>\n",
              "      <td>0.25316</td>\n",
              "      <td>-0.40667</td>\n",
              "      <td>-0.748790</td>\n",
              "      <td>-0.26739</td>\n",
              "      <td>-0.13139</td>\n",
              "      <td>0.131520</td>\n",
              "      <td>-0.436380</td>\n",
              "      <td>0.13688</td>\n",
              "      <td>-0.346500</td>\n",
              "      <td>-0.64363</td>\n",
              "      <td>0.72773</td>\n",
              "      <td>-0.604560</td>\n",
              "      <td>-0.023956</td>\n",
              "      <td>0.008431</td>\n",
              "      <td>-0.65743</td>\n",
              "      <td>-0.304520</td>\n",
              "      <td>0.295640</td>\n",
              "      <td>-0.20179</td>\n",
              "      <td>-0.224310</td>\n",
              "      <td>0.053594</td>\n",
              "      <td>0.071288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>practice</th>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.074821</td>\n",
              "      <td>0.52975</td>\n",
              "      <td>-0.317730</td>\n",
              "      <td>0.16082</td>\n",
              "      <td>0.094025</td>\n",
              "      <td>1.17600</td>\n",
              "      <td>0.533840</td>\n",
              "      <td>-0.234700</td>\n",
              "      <td>-0.62745</td>\n",
              "      <td>0.400680</td>\n",
              "      <td>-0.742210</td>\n",
              "      <td>-0.039586</td>\n",
              "      <td>-0.429890</td>\n",
              "      <td>0.375220</td>\n",
              "      <td>-0.18529</td>\n",
              "      <td>-0.737180</td>\n",
              "      <td>-0.244620</td>\n",
              "      <td>-0.236410</td>\n",
              "      <td>-0.10292</td>\n",
              "      <td>-0.288780</td>\n",
              "      <td>0.405080</td>\n",
              "      <td>0.636950</td>\n",
              "      <td>-0.34018</td>\n",
              "      <td>-0.116070</td>\n",
              "      <td>1.32700</td>\n",
              "      <td>0.148170</td>\n",
              "      <td>0.742790</td>\n",
              "      <td>-0.137800</td>\n",
              "      <td>-0.181000</td>\n",
              "      <td>-0.19293</td>\n",
              "      <td>-1.03020</td>\n",
              "      <td>0.142130</td>\n",
              "      <td>-0.023994</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.473890</td>\n",
              "      <td>0.20752</td>\n",
              "      <td>0.510160</td>\n",
              "      <td>0.729560</td>\n",
              "      <td>0.521660</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.164080</td>\n",
              "      <td>-0.27670</td>\n",
              "      <td>0.12648</td>\n",
              "      <td>0.411310</td>\n",
              "      <td>-0.012115</td>\n",
              "      <td>0.224080</td>\n",
              "      <td>-0.18910</td>\n",
              "      <td>-0.015046</td>\n",
              "      <td>0.406690</td>\n",
              "      <td>-0.41758</td>\n",
              "      <td>0.683880</td>\n",
              "      <td>0.67441</td>\n",
              "      <td>0.246050</td>\n",
              "      <td>-0.34382</td>\n",
              "      <td>0.273760</td>\n",
              "      <td>-0.35473</td>\n",
              "      <td>0.018784</td>\n",
              "      <td>-0.011239</td>\n",
              "      <td>-0.109000</td>\n",
              "      <td>-0.61412</td>\n",
              "      <td>0.32809</td>\n",
              "      <td>0.702070</td>\n",
              "      <td>0.21946</td>\n",
              "      <td>-0.32508</td>\n",
              "      <td>-0.298080</td>\n",
              "      <td>0.076631</td>\n",
              "      <td>0.13557</td>\n",
              "      <td>-0.544340</td>\n",
              "      <td>-0.12014</td>\n",
              "      <td>0.23252</td>\n",
              "      <td>0.084184</td>\n",
              "      <td>0.190540</td>\n",
              "      <td>0.752250</td>\n",
              "      <td>0.31251</td>\n",
              "      <td>-0.712900</td>\n",
              "      <td>-0.095691</td>\n",
              "      <td>0.22554</td>\n",
              "      <td>0.225350</td>\n",
              "      <td>-0.263050</td>\n",
              "      <td>0.397380</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0         1        2    ...       197       198       199\n",
              "see       0.381280  0.952290 -0.22360  ... -0.407160 -0.109480 -0.249810\n",
              "star     -0.082023  0.301080 -0.51508  ... -0.062974 -0.013045 -0.898590\n",
              "review   -0.106200  0.379150 -0.41743  ...  0.215970  0.372850 -0.136990\n",
              "family   -0.204030  0.000338 -0.20134  ... -0.224310  0.053594  0.071288\n",
              "practice  0.241160  0.074821  0.52975  ...  0.225350 -0.263050  0.397380\n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK3qhrXHbbBL"
      },
      "source": [
        "messages = [[word.replace(\"'\", '').strip() for word in message] for message in df['final'].tolist()]\n",
        "messages_clean = [[word for word in message if word in pre_ft_vectors.vocab] for message in messages]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6iMfGRubkMp"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# create a function that simply calculates an average of all word vectors within a document\n",
        "def document_vector(word2vec_model, doc):\n",
        "  return np.mean(word2vec_model[doc], axis =0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyCfzOIWbd8L",
        "outputId": "b5d769bd-c538-4e1b-86fe-4c3cd47af131"
      },
      "source": [
        "message_vectors = []\n",
        "for doc in messages_clean:  # append the vector for each document\n",
        "  if not doc:\n",
        "    message_vectors.append([])\n",
        "  else:\n",
        "    message_vectors.append(document_vector(pre_ft_vectors, doc))\n",
        "\n",
        "message_vectors_a = np.array(message_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psSEgQvmcRUO"
      },
      "source": [
        "df['Index'] = df.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "GXqWrJx8b-CK",
        "outputId": "eb1637c2-2d08-4696-c500-d8306f77cad3"
      },
      "source": [
        "labels = ['AWE' + str(i) for i in range (1, 201)]\n",
        "awe_df = pd.DataFrame(message_vectors, index=df['Index'].tolist(), columns=labels)\n",
        "awe_df = awe_df.reset_index()\n",
        "awe_df.rename(columns={'index':'Index'}, inplace=True)\n",
        "awe_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>AWE1</th>\n",
              "      <th>AWE2</th>\n",
              "      <th>AWE3</th>\n",
              "      <th>AWE4</th>\n",
              "      <th>AWE5</th>\n",
              "      <th>AWE6</th>\n",
              "      <th>AWE7</th>\n",
              "      <th>AWE8</th>\n",
              "      <th>AWE9</th>\n",
              "      <th>AWE10</th>\n",
              "      <th>AWE11</th>\n",
              "      <th>AWE12</th>\n",
              "      <th>AWE13</th>\n",
              "      <th>AWE14</th>\n",
              "      <th>AWE15</th>\n",
              "      <th>AWE16</th>\n",
              "      <th>AWE17</th>\n",
              "      <th>AWE18</th>\n",
              "      <th>AWE19</th>\n",
              "      <th>AWE20</th>\n",
              "      <th>AWE21</th>\n",
              "      <th>AWE22</th>\n",
              "      <th>AWE23</th>\n",
              "      <th>AWE24</th>\n",
              "      <th>AWE25</th>\n",
              "      <th>AWE26</th>\n",
              "      <th>AWE27</th>\n",
              "      <th>AWE28</th>\n",
              "      <th>AWE29</th>\n",
              "      <th>AWE30</th>\n",
              "      <th>AWE31</th>\n",
              "      <th>AWE32</th>\n",
              "      <th>AWE33</th>\n",
              "      <th>AWE34</th>\n",
              "      <th>AWE35</th>\n",
              "      <th>AWE36</th>\n",
              "      <th>AWE37</th>\n",
              "      <th>AWE38</th>\n",
              "      <th>AWE39</th>\n",
              "      <th>...</th>\n",
              "      <th>AWE161</th>\n",
              "      <th>AWE162</th>\n",
              "      <th>AWE163</th>\n",
              "      <th>AWE164</th>\n",
              "      <th>AWE165</th>\n",
              "      <th>AWE166</th>\n",
              "      <th>AWE167</th>\n",
              "      <th>AWE168</th>\n",
              "      <th>AWE169</th>\n",
              "      <th>AWE170</th>\n",
              "      <th>AWE171</th>\n",
              "      <th>AWE172</th>\n",
              "      <th>AWE173</th>\n",
              "      <th>AWE174</th>\n",
              "      <th>AWE175</th>\n",
              "      <th>AWE176</th>\n",
              "      <th>AWE177</th>\n",
              "      <th>AWE178</th>\n",
              "      <th>AWE179</th>\n",
              "      <th>AWE180</th>\n",
              "      <th>AWE181</th>\n",
              "      <th>AWE182</th>\n",
              "      <th>AWE183</th>\n",
              "      <th>AWE184</th>\n",
              "      <th>AWE185</th>\n",
              "      <th>AWE186</th>\n",
              "      <th>AWE187</th>\n",
              "      <th>AWE188</th>\n",
              "      <th>AWE189</th>\n",
              "      <th>AWE190</th>\n",
              "      <th>AWE191</th>\n",
              "      <th>AWE192</th>\n",
              "      <th>AWE193</th>\n",
              "      <th>AWE194</th>\n",
              "      <th>AWE195</th>\n",
              "      <th>AWE196</th>\n",
              "      <th>AWE197</th>\n",
              "      <th>AWE198</th>\n",
              "      <th>AWE199</th>\n",
              "      <th>AWE200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.091733</td>\n",
              "      <td>0.116644</td>\n",
              "      <td>-0.057185</td>\n",
              "      <td>-0.103456</td>\n",
              "      <td>0.104724</td>\n",
              "      <td>0.060924</td>\n",
              "      <td>0.542219</td>\n",
              "      <td>0.033925</td>\n",
              "      <td>-0.001545</td>\n",
              "      <td>-0.158022</td>\n",
              "      <td>-0.115832</td>\n",
              "      <td>-0.099747</td>\n",
              "      <td>-0.461253</td>\n",
              "      <td>-0.076329</td>\n",
              "      <td>0.042458</td>\n",
              "      <td>0.003450</td>\n",
              "      <td>0.018623</td>\n",
              "      <td>-0.012138</td>\n",
              "      <td>-0.159880</td>\n",
              "      <td>-0.212174</td>\n",
              "      <td>-0.074341</td>\n",
              "      <td>0.028398</td>\n",
              "      <td>-0.125563</td>\n",
              "      <td>-0.013380</td>\n",
              "      <td>-0.110495</td>\n",
              "      <td>0.663112</td>\n",
              "      <td>0.020879</td>\n",
              "      <td>0.218826</td>\n",
              "      <td>0.014026</td>\n",
              "      <td>-0.164476</td>\n",
              "      <td>0.061494</td>\n",
              "      <td>-0.080638</td>\n",
              "      <td>0.038203</td>\n",
              "      <td>-0.045309</td>\n",
              "      <td>-0.078851</td>\n",
              "      <td>0.072991</td>\n",
              "      <td>0.170164</td>\n",
              "      <td>0.080335</td>\n",
              "      <td>0.187150</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013001</td>\n",
              "      <td>-0.036041</td>\n",
              "      <td>-0.020788</td>\n",
              "      <td>0.164829</td>\n",
              "      <td>0.028368</td>\n",
              "      <td>-0.022816</td>\n",
              "      <td>-0.103664</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.140346</td>\n",
              "      <td>-0.026546</td>\n",
              "      <td>-0.054877</td>\n",
              "      <td>0.069513</td>\n",
              "      <td>0.113889</td>\n",
              "      <td>0.017234</td>\n",
              "      <td>0.007763</td>\n",
              "      <td>-0.107550</td>\n",
              "      <td>-0.034695</td>\n",
              "      <td>-0.110795</td>\n",
              "      <td>0.085910</td>\n",
              "      <td>-0.062500</td>\n",
              "      <td>-0.008499</td>\n",
              "      <td>0.032974</td>\n",
              "      <td>0.096622</td>\n",
              "      <td>-0.044835</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.041028</td>\n",
              "      <td>-0.007234</td>\n",
              "      <td>-0.113410</td>\n",
              "      <td>-0.076270</td>\n",
              "      <td>0.209669</td>\n",
              "      <td>-0.087744</td>\n",
              "      <td>0.015287</td>\n",
              "      <td>0.211982</td>\n",
              "      <td>-0.206838</td>\n",
              "      <td>-0.047367</td>\n",
              "      <td>-0.120737</td>\n",
              "      <td>0.008818</td>\n",
              "      <td>-0.054575</td>\n",
              "      <td>0.023107</td>\n",
              "      <td>-0.063070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.050948</td>\n",
              "      <td>0.081066</td>\n",
              "      <td>-0.100553</td>\n",
              "      <td>-0.060556</td>\n",
              "      <td>0.137765</td>\n",
              "      <td>-0.016419</td>\n",
              "      <td>0.645886</td>\n",
              "      <td>-0.037969</td>\n",
              "      <td>0.105021</td>\n",
              "      <td>-0.003371</td>\n",
              "      <td>0.010374</td>\n",
              "      <td>0.050843</td>\n",
              "      <td>-0.560533</td>\n",
              "      <td>-0.116482</td>\n",
              "      <td>0.111024</td>\n",
              "      <td>-0.076210</td>\n",
              "      <td>-0.042454</td>\n",
              "      <td>0.133326</td>\n",
              "      <td>-0.102722</td>\n",
              "      <td>-0.136448</td>\n",
              "      <td>0.054454</td>\n",
              "      <td>-0.025712</td>\n",
              "      <td>-0.053532</td>\n",
              "      <td>-0.001973</td>\n",
              "      <td>-0.155463</td>\n",
              "      <td>0.800988</td>\n",
              "      <td>0.215096</td>\n",
              "      <td>0.106702</td>\n",
              "      <td>0.073111</td>\n",
              "      <td>-0.131786</td>\n",
              "      <td>0.083826</td>\n",
              "      <td>-0.119177</td>\n",
              "      <td>-0.041970</td>\n",
              "      <td>0.030339</td>\n",
              "      <td>-0.188913</td>\n",
              "      <td>0.147784</td>\n",
              "      <td>0.030011</td>\n",
              "      <td>0.012389</td>\n",
              "      <td>0.103764</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055591</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.136734</td>\n",
              "      <td>0.134832</td>\n",
              "      <td>-0.006499</td>\n",
              "      <td>-0.041918</td>\n",
              "      <td>-0.047471</td>\n",
              "      <td>-0.058321</td>\n",
              "      <td>0.064452</td>\n",
              "      <td>0.042912</td>\n",
              "      <td>-0.054448</td>\n",
              "      <td>0.119691</td>\n",
              "      <td>0.059884</td>\n",
              "      <td>0.012066</td>\n",
              "      <td>-0.046926</td>\n",
              "      <td>-0.173230</td>\n",
              "      <td>-0.016169</td>\n",
              "      <td>-0.213488</td>\n",
              "      <td>0.149717</td>\n",
              "      <td>0.050852</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.129645</td>\n",
              "      <td>0.077635</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>-0.025267</td>\n",
              "      <td>0.132430</td>\n",
              "      <td>0.161996</td>\n",
              "      <td>-0.029400</td>\n",
              "      <td>0.045942</td>\n",
              "      <td>0.027476</td>\n",
              "      <td>-0.070606</td>\n",
              "      <td>-0.067446</td>\n",
              "      <td>0.095191</td>\n",
              "      <td>-0.094072</td>\n",
              "      <td>0.094781</td>\n",
              "      <td>-0.127568</td>\n",
              "      <td>0.104540</td>\n",
              "      <td>0.033650</td>\n",
              "      <td>0.059103</td>\n",
              "      <td>0.035592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.087739</td>\n",
              "      <td>0.135671</td>\n",
              "      <td>0.005935</td>\n",
              "      <td>-0.006324</td>\n",
              "      <td>0.083941</td>\n",
              "      <td>-0.002159</td>\n",
              "      <td>0.692737</td>\n",
              "      <td>-0.035426</td>\n",
              "      <td>-0.009401</td>\n",
              "      <td>-0.044613</td>\n",
              "      <td>-0.323754</td>\n",
              "      <td>0.010717</td>\n",
              "      <td>-0.568150</td>\n",
              "      <td>-0.025733</td>\n",
              "      <td>-0.070422</td>\n",
              "      <td>0.057669</td>\n",
              "      <td>-0.077065</td>\n",
              "      <td>0.127696</td>\n",
              "      <td>0.069941</td>\n",
              "      <td>-0.128421</td>\n",
              "      <td>0.116801</td>\n",
              "      <td>0.008347</td>\n",
              "      <td>-0.270478</td>\n",
              "      <td>0.025517</td>\n",
              "      <td>-0.023778</td>\n",
              "      <td>0.726969</td>\n",
              "      <td>0.030071</td>\n",
              "      <td>0.088601</td>\n",
              "      <td>0.044947</td>\n",
              "      <td>-0.077319</td>\n",
              "      <td>-0.037049</td>\n",
              "      <td>-0.219086</td>\n",
              "      <td>-0.087616</td>\n",
              "      <td>0.095496</td>\n",
              "      <td>-0.194937</td>\n",
              "      <td>0.181570</td>\n",
              "      <td>0.140418</td>\n",
              "      <td>0.036875</td>\n",
              "      <td>-0.001867</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052727</td>\n",
              "      <td>-0.022286</td>\n",
              "      <td>-0.249808</td>\n",
              "      <td>0.198791</td>\n",
              "      <td>0.071539</td>\n",
              "      <td>-0.102149</td>\n",
              "      <td>-0.114673</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.145479</td>\n",
              "      <td>-0.020602</td>\n",
              "      <td>0.020586</td>\n",
              "      <td>0.024660</td>\n",
              "      <td>0.062785</td>\n",
              "      <td>-0.078413</td>\n",
              "      <td>0.022599</td>\n",
              "      <td>-0.071183</td>\n",
              "      <td>0.097420</td>\n",
              "      <td>-0.218218</td>\n",
              "      <td>0.171905</td>\n",
              "      <td>-0.068029</td>\n",
              "      <td>0.027951</td>\n",
              "      <td>-0.018332</td>\n",
              "      <td>-0.041327</td>\n",
              "      <td>-0.034740</td>\n",
              "      <td>0.193908</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>0.078278</td>\n",
              "      <td>0.003362</td>\n",
              "      <td>-0.101686</td>\n",
              "      <td>0.144752</td>\n",
              "      <td>-0.043917</td>\n",
              "      <td>0.072726</td>\n",
              "      <td>0.020087</td>\n",
              "      <td>-0.107518</td>\n",
              "      <td>-0.033213</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.198642</td>\n",
              "      <td>0.030723</td>\n",
              "      <td>0.031848</td>\n",
              "      <td>-0.115006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.062678</td>\n",
              "      <td>0.121224</td>\n",
              "      <td>-0.064672</td>\n",
              "      <td>0.122644</td>\n",
              "      <td>0.149081</td>\n",
              "      <td>0.044785</td>\n",
              "      <td>0.561236</td>\n",
              "      <td>-0.000373</td>\n",
              "      <td>-0.027437</td>\n",
              "      <td>-0.029970</td>\n",
              "      <td>-0.239860</td>\n",
              "      <td>-0.050296</td>\n",
              "      <td>-0.605329</td>\n",
              "      <td>-0.065064</td>\n",
              "      <td>0.067156</td>\n",
              "      <td>0.012832</td>\n",
              "      <td>-0.107096</td>\n",
              "      <td>0.018661</td>\n",
              "      <td>0.117309</td>\n",
              "      <td>-0.244392</td>\n",
              "      <td>-0.098588</td>\n",
              "      <td>0.118640</td>\n",
              "      <td>-0.122904</td>\n",
              "      <td>-0.038477</td>\n",
              "      <td>-0.025436</td>\n",
              "      <td>0.905873</td>\n",
              "      <td>0.022805</td>\n",
              "      <td>0.205491</td>\n",
              "      <td>-0.088479</td>\n",
              "      <td>0.003271</td>\n",
              "      <td>0.179962</td>\n",
              "      <td>-0.365007</td>\n",
              "      <td>0.045531</td>\n",
              "      <td>0.144821</td>\n",
              "      <td>-0.148051</td>\n",
              "      <td>0.176618</td>\n",
              "      <td>0.100164</td>\n",
              "      <td>0.098545</td>\n",
              "      <td>0.103261</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.021603</td>\n",
              "      <td>-0.121020</td>\n",
              "      <td>-0.105667</td>\n",
              "      <td>-0.133198</td>\n",
              "      <td>-0.174478</td>\n",
              "      <td>-0.092067</td>\n",
              "      <td>-0.228660</td>\n",
              "      <td>0.218980</td>\n",
              "      <td>0.029529</td>\n",
              "      <td>-0.006477</td>\n",
              "      <td>0.115629</td>\n",
              "      <td>-0.184149</td>\n",
              "      <td>0.176944</td>\n",
              "      <td>-0.009503</td>\n",
              "      <td>-0.021329</td>\n",
              "      <td>-0.233324</td>\n",
              "      <td>0.050152</td>\n",
              "      <td>-0.053244</td>\n",
              "      <td>0.060841</td>\n",
              "      <td>-0.029917</td>\n",
              "      <td>0.189309</td>\n",
              "      <td>0.032937</td>\n",
              "      <td>0.063392</td>\n",
              "      <td>0.071988</td>\n",
              "      <td>0.036674</td>\n",
              "      <td>0.056777</td>\n",
              "      <td>-0.022634</td>\n",
              "      <td>-0.003002</td>\n",
              "      <td>0.114672</td>\n",
              "      <td>0.055739</td>\n",
              "      <td>0.028275</td>\n",
              "      <td>0.020430</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>-0.047284</td>\n",
              "      <td>-0.087282</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>0.089896</td>\n",
              "      <td>0.059271</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>-0.091125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.002764</td>\n",
              "      <td>0.210691</td>\n",
              "      <td>-0.012721</td>\n",
              "      <td>0.133616</td>\n",
              "      <td>-0.019703</td>\n",
              "      <td>0.107070</td>\n",
              "      <td>0.728814</td>\n",
              "      <td>0.057699</td>\n",
              "      <td>-0.056013</td>\n",
              "      <td>-0.025552</td>\n",
              "      <td>-0.162720</td>\n",
              "      <td>-0.121610</td>\n",
              "      <td>-0.634945</td>\n",
              "      <td>-0.198183</td>\n",
              "      <td>0.033477</td>\n",
              "      <td>0.042578</td>\n",
              "      <td>-0.158665</td>\n",
              "      <td>0.165981</td>\n",
              "      <td>0.009809</td>\n",
              "      <td>-0.015610</td>\n",
              "      <td>0.140301</td>\n",
              "      <td>0.086306</td>\n",
              "      <td>-0.160582</td>\n",
              "      <td>0.082321</td>\n",
              "      <td>-0.059185</td>\n",
              "      <td>0.953112</td>\n",
              "      <td>-0.057333</td>\n",
              "      <td>0.134610</td>\n",
              "      <td>0.068319</td>\n",
              "      <td>-0.046145</td>\n",
              "      <td>0.057800</td>\n",
              "      <td>-0.311709</td>\n",
              "      <td>0.004753</td>\n",
              "      <td>0.159354</td>\n",
              "      <td>-0.089108</td>\n",
              "      <td>0.141817</td>\n",
              "      <td>0.058732</td>\n",
              "      <td>-0.035845</td>\n",
              "      <td>0.165769</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066712</td>\n",
              "      <td>-0.182554</td>\n",
              "      <td>-0.305282</td>\n",
              "      <td>-0.005345</td>\n",
              "      <td>0.019138</td>\n",
              "      <td>-0.091160</td>\n",
              "      <td>0.022118</td>\n",
              "      <td>0.113561</td>\n",
              "      <td>-0.032497</td>\n",
              "      <td>-0.016087</td>\n",
              "      <td>0.018641</td>\n",
              "      <td>-0.073920</td>\n",
              "      <td>0.097788</td>\n",
              "      <td>-0.071804</td>\n",
              "      <td>-0.032864</td>\n",
              "      <td>-0.197328</td>\n",
              "      <td>0.012225</td>\n",
              "      <td>-0.188776</td>\n",
              "      <td>0.144087</td>\n",
              "      <td>-0.108260</td>\n",
              "      <td>0.059655</td>\n",
              "      <td>-0.114022</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>0.046420</td>\n",
              "      <td>0.147844</td>\n",
              "      <td>-0.030070</td>\n",
              "      <td>0.051887</td>\n",
              "      <td>0.145162</td>\n",
              "      <td>0.049020</td>\n",
              "      <td>0.114916</td>\n",
              "      <td>-0.115151</td>\n",
              "      <td>0.037986</td>\n",
              "      <td>-0.017044</td>\n",
              "      <td>-0.032323</td>\n",
              "      <td>-0.075915</td>\n",
              "      <td>-0.068870</td>\n",
              "      <td>0.005740</td>\n",
              "      <td>-0.012073</td>\n",
              "      <td>-0.006631</td>\n",
              "      <td>-0.067276</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 201 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Index      AWE1      AWE2      AWE3  ...    AWE197    AWE198    AWE199    AWE200\n",
              "0      0  0.091733  0.116644 -0.057185  ...  0.008818 -0.054575  0.023107 -0.063070\n",
              "1      1 -0.050948  0.081066 -0.100553  ...  0.104540  0.033650  0.059103  0.035592\n",
              "2      2 -0.087739  0.135671  0.005935  ...  0.198642  0.030723  0.031848 -0.115006\n",
              "3      3 -0.062678  0.121224 -0.064672  ...  0.089896  0.059271  0.000523 -0.091125\n",
              "4      4 -0.002764  0.210691 -0.012721  ...  0.005740 -0.012073 -0.006631 -0.067276\n",
              "\n",
              "[5 rows x 201 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkdVZOKuciQv"
      },
      "source": [
        "df_w_glove = pd.merge(left=df, right=awe_df, left_on='Index', right_on='Index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDLJxVOhckgL",
        "outputId": "cbc16378-509d-4b62-9596-7c91b57e5557"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df_w_glove.to_pickle('df_w_glove')\n",
        "df_w_glove.to_csv('df_w_glove.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "N6iIM0FOcp9V",
        "outputId": "db481158-bbb8-44f9-b2f4-d207225c0d61"
      },
      "source": [
        "df_w_glove.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>final</th>\n",
              "      <th>Index</th>\n",
              "      <th>Length</th>\n",
              "      <th>Lexical Diversity</th>\n",
              "      <th>AWE1</th>\n",
              "      <th>AWE2</th>\n",
              "      <th>AWE3</th>\n",
              "      <th>AWE4</th>\n",
              "      <th>AWE5</th>\n",
              "      <th>AWE6</th>\n",
              "      <th>AWE7</th>\n",
              "      <th>AWE8</th>\n",
              "      <th>AWE9</th>\n",
              "      <th>AWE10</th>\n",
              "      <th>AWE11</th>\n",
              "      <th>AWE12</th>\n",
              "      <th>AWE13</th>\n",
              "      <th>AWE14</th>\n",
              "      <th>AWE15</th>\n",
              "      <th>AWE16</th>\n",
              "      <th>AWE17</th>\n",
              "      <th>AWE18</th>\n",
              "      <th>AWE19</th>\n",
              "      <th>AWE20</th>\n",
              "      <th>AWE21</th>\n",
              "      <th>AWE22</th>\n",
              "      <th>AWE23</th>\n",
              "      <th>AWE24</th>\n",
              "      <th>AWE25</th>\n",
              "      <th>...</th>\n",
              "      <th>AWE161</th>\n",
              "      <th>AWE162</th>\n",
              "      <th>AWE163</th>\n",
              "      <th>AWE164</th>\n",
              "      <th>AWE165</th>\n",
              "      <th>AWE166</th>\n",
              "      <th>AWE167</th>\n",
              "      <th>AWE168</th>\n",
              "      <th>AWE169</th>\n",
              "      <th>AWE170</th>\n",
              "      <th>AWE171</th>\n",
              "      <th>AWE172</th>\n",
              "      <th>AWE173</th>\n",
              "      <th>AWE174</th>\n",
              "      <th>AWE175</th>\n",
              "      <th>AWE176</th>\n",
              "      <th>AWE177</th>\n",
              "      <th>AWE178</th>\n",
              "      <th>AWE179</th>\n",
              "      <th>AWE180</th>\n",
              "      <th>AWE181</th>\n",
              "      <th>AWE182</th>\n",
              "      <th>AWE183</th>\n",
              "      <th>AWE184</th>\n",
              "      <th>AWE185</th>\n",
              "      <th>AWE186</th>\n",
              "      <th>AWE187</th>\n",
              "      <th>AWE188</th>\n",
              "      <th>AWE189</th>\n",
              "      <th>AWE190</th>\n",
              "      <th>AWE191</th>\n",
              "      <th>AWE192</th>\n",
              "      <th>AWE193</th>\n",
              "      <th>AWE194</th>\n",
              "      <th>AWE195</th>\n",
              "      <th>AWE196</th>\n",
              "      <th>AWE197</th>\n",
              "      <th>AWE198</th>\n",
              "      <th>AWE199</th>\n",
              "      <th>AWE200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>[After, seeing, 4, ., 5, stars, reviews, on, a...</td>\n",
              "      <td>[(After, IN), (seeing, VBG), (4, CD), (., .), ...</td>\n",
              "      <td>[After, see, 4, ., 5, star, review, on, a, fam...</td>\n",
              "      <td>[see, star, review, family, practice, rancho, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>1.142857</td>\n",
              "      <td>0.091733</td>\n",
              "      <td>0.116644</td>\n",
              "      <td>-0.057185</td>\n",
              "      <td>-0.103456</td>\n",
              "      <td>0.104724</td>\n",
              "      <td>0.060924</td>\n",
              "      <td>0.542219</td>\n",
              "      <td>0.033925</td>\n",
              "      <td>-0.001545</td>\n",
              "      <td>-0.158022</td>\n",
              "      <td>-0.115832</td>\n",
              "      <td>-0.099747</td>\n",
              "      <td>-0.461253</td>\n",
              "      <td>-0.076329</td>\n",
              "      <td>0.042458</td>\n",
              "      <td>0.003450</td>\n",
              "      <td>0.018623</td>\n",
              "      <td>-0.012138</td>\n",
              "      <td>-0.159880</td>\n",
              "      <td>-0.212174</td>\n",
              "      <td>-0.074341</td>\n",
              "      <td>0.028398</td>\n",
              "      <td>-0.125563</td>\n",
              "      <td>-0.013380</td>\n",
              "      <td>-0.110495</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013001</td>\n",
              "      <td>-0.036041</td>\n",
              "      <td>-0.020788</td>\n",
              "      <td>0.164829</td>\n",
              "      <td>0.028368</td>\n",
              "      <td>-0.022816</td>\n",
              "      <td>-0.103664</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.140346</td>\n",
              "      <td>-0.026546</td>\n",
              "      <td>-0.054877</td>\n",
              "      <td>0.069513</td>\n",
              "      <td>0.113889</td>\n",
              "      <td>0.017234</td>\n",
              "      <td>0.007763</td>\n",
              "      <td>-0.107550</td>\n",
              "      <td>-0.034695</td>\n",
              "      <td>-0.110795</td>\n",
              "      <td>0.085910</td>\n",
              "      <td>-0.062500</td>\n",
              "      <td>-0.008499</td>\n",
              "      <td>0.032974</td>\n",
              "      <td>0.096622</td>\n",
              "      <td>-0.044835</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.041028</td>\n",
              "      <td>-0.007234</td>\n",
              "      <td>-0.113410</td>\n",
              "      <td>-0.076270</td>\n",
              "      <td>0.209669</td>\n",
              "      <td>-0.087744</td>\n",
              "      <td>0.015287</td>\n",
              "      <td>0.211982</td>\n",
              "      <td>-0.206838</td>\n",
              "      <td>-0.047367</td>\n",
              "      <td>-0.120737</td>\n",
              "      <td>0.008818</td>\n",
              "      <td>-0.054575</td>\n",
              "      <td>0.023107</td>\n",
              "      <td>-0.063070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>[I, just, switched, this, month, to, Rancho, W...</td>\n",
              "      <td>[(I, PRP), (just, RB), (switched, VBD), (this,...</td>\n",
              "      <td>[I, just, switch, this, month, to, Rancho, Wel...</td>\n",
              "      <td>[switch, month, rancho, wellness, base, referr...</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>1.229167</td>\n",
              "      <td>-0.050948</td>\n",
              "      <td>0.081066</td>\n",
              "      <td>-0.100553</td>\n",
              "      <td>-0.060556</td>\n",
              "      <td>0.137765</td>\n",
              "      <td>-0.016419</td>\n",
              "      <td>0.645886</td>\n",
              "      <td>-0.037969</td>\n",
              "      <td>0.105021</td>\n",
              "      <td>-0.003371</td>\n",
              "      <td>0.010374</td>\n",
              "      <td>0.050843</td>\n",
              "      <td>-0.560533</td>\n",
              "      <td>-0.116482</td>\n",
              "      <td>0.111024</td>\n",
              "      <td>-0.076210</td>\n",
              "      <td>-0.042454</td>\n",
              "      <td>0.133326</td>\n",
              "      <td>-0.102722</td>\n",
              "      <td>-0.136448</td>\n",
              "      <td>0.054454</td>\n",
              "      <td>-0.025712</td>\n",
              "      <td>-0.053532</td>\n",
              "      <td>-0.001973</td>\n",
              "      <td>-0.155463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055591</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.136734</td>\n",
              "      <td>0.134832</td>\n",
              "      <td>-0.006499</td>\n",
              "      <td>-0.041918</td>\n",
              "      <td>-0.047471</td>\n",
              "      <td>-0.058321</td>\n",
              "      <td>0.064452</td>\n",
              "      <td>0.042912</td>\n",
              "      <td>-0.054448</td>\n",
              "      <td>0.119691</td>\n",
              "      <td>0.059884</td>\n",
              "      <td>0.012066</td>\n",
              "      <td>-0.046926</td>\n",
              "      <td>-0.173230</td>\n",
              "      <td>-0.016169</td>\n",
              "      <td>-0.213488</td>\n",
              "      <td>0.149717</td>\n",
              "      <td>0.050852</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.129645</td>\n",
              "      <td>0.077635</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>-0.025267</td>\n",
              "      <td>0.132430</td>\n",
              "      <td>0.161996</td>\n",
              "      <td>-0.029400</td>\n",
              "      <td>0.045942</td>\n",
              "      <td>0.027476</td>\n",
              "      <td>-0.070606</td>\n",
              "      <td>-0.067446</td>\n",
              "      <td>0.095191</td>\n",
              "      <td>-0.094072</td>\n",
              "      <td>0.094781</td>\n",
              "      <td>-0.127568</td>\n",
              "      <td>0.104540</td>\n",
              "      <td>0.033650</td>\n",
              "      <td>0.059103</td>\n",
              "      <td>0.035592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, are, very...</td>\n",
              "      <td>[(I, PRP), (love, VBP), (Rancho, NNP), (wellne...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, be, very,...</td>\n",
              "      <td>[love, rancho, wellness, organize, professiona...</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "      <td>1.181818</td>\n",
              "      <td>-0.087739</td>\n",
              "      <td>0.135671</td>\n",
              "      <td>0.005935</td>\n",
              "      <td>-0.006324</td>\n",
              "      <td>0.083941</td>\n",
              "      <td>-0.002159</td>\n",
              "      <td>0.692737</td>\n",
              "      <td>-0.035426</td>\n",
              "      <td>-0.009401</td>\n",
              "      <td>-0.044613</td>\n",
              "      <td>-0.323754</td>\n",
              "      <td>0.010717</td>\n",
              "      <td>-0.568150</td>\n",
              "      <td>-0.025733</td>\n",
              "      <td>-0.070422</td>\n",
              "      <td>0.057669</td>\n",
              "      <td>-0.077065</td>\n",
              "      <td>0.127696</td>\n",
              "      <td>0.069941</td>\n",
              "      <td>-0.128421</td>\n",
              "      <td>0.116801</td>\n",
              "      <td>0.008347</td>\n",
              "      <td>-0.270478</td>\n",
              "      <td>0.025517</td>\n",
              "      <td>-0.023778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052727</td>\n",
              "      <td>-0.022286</td>\n",
              "      <td>-0.249808</td>\n",
              "      <td>0.198791</td>\n",
              "      <td>0.071539</td>\n",
              "      <td>-0.102149</td>\n",
              "      <td>-0.114673</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.145479</td>\n",
              "      <td>-0.020602</td>\n",
              "      <td>0.020586</td>\n",
              "      <td>0.024660</td>\n",
              "      <td>0.062785</td>\n",
              "      <td>-0.078413</td>\n",
              "      <td>0.022599</td>\n",
              "      <td>-0.071183</td>\n",
              "      <td>0.097420</td>\n",
              "      <td>-0.218218</td>\n",
              "      <td>0.171905</td>\n",
              "      <td>-0.068029</td>\n",
              "      <td>0.027951</td>\n",
              "      <td>-0.018332</td>\n",
              "      <td>-0.041327</td>\n",
              "      <td>-0.034740</td>\n",
              "      <td>0.193908</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>0.078278</td>\n",
              "      <td>0.003362</td>\n",
              "      <td>-0.101686</td>\n",
              "      <td>0.144752</td>\n",
              "      <td>-0.043917</td>\n",
              "      <td>0.072726</td>\n",
              "      <td>0.020087</td>\n",
              "      <td>-0.107518</td>\n",
              "      <td>-0.033213</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.198642</td>\n",
              "      <td>0.030723</td>\n",
              "      <td>0.031848</td>\n",
              "      <td>-0.115006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 215 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  doctorID    username  ...    AWE198    AWE199    AWE200\n",
              "0           0         0  Whitney W.  ... -0.054575  0.023107 -0.063070\n",
              "1           1         0  Kristin R.  ...  0.033650  0.059103  0.035592\n",
              "2           2         0  Allyson F.  ...  0.030723  0.031848 -0.115006\n",
              "\n",
              "[3 rows x 215 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUydn7sscuWM",
        "outputId": "ad4b0ff3-8771-44f7-97db-84695a10c898"
      },
      "source": [
        "df_w_glove.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0           0\n",
              "doctorID             0\n",
              "username          9348\n",
              "rating               0\n",
              "date_of_review    3144\n",
              "                  ... \n",
              "AWE196               7\n",
              "AWE197               7\n",
              "AWE198               7\n",
              "AWE199               7\n",
              "AWE200               7\n",
              "Length: 213, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "O-ocSufzc-fJ",
        "outputId": "b855bd91-0216-48e7-e4df-2c7f850b73bf"
      },
      "source": [
        "df_w_glove[df_w_glove.iloc[:,7:207].isna().any(axis = 1)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>final</th>\n",
              "      <th>Index</th>\n",
              "      <th>AWE1</th>\n",
              "      <th>AWE2</th>\n",
              "      <th>AWE3</th>\n",
              "      <th>AWE4</th>\n",
              "      <th>AWE5</th>\n",
              "      <th>AWE6</th>\n",
              "      <th>AWE7</th>\n",
              "      <th>AWE8</th>\n",
              "      <th>AWE9</th>\n",
              "      <th>AWE10</th>\n",
              "      <th>AWE11</th>\n",
              "      <th>AWE12</th>\n",
              "      <th>AWE13</th>\n",
              "      <th>AWE14</th>\n",
              "      <th>AWE15</th>\n",
              "      <th>AWE16</th>\n",
              "      <th>AWE17</th>\n",
              "      <th>AWE18</th>\n",
              "      <th>AWE19</th>\n",
              "      <th>AWE20</th>\n",
              "      <th>AWE21</th>\n",
              "      <th>AWE22</th>\n",
              "      <th>AWE23</th>\n",
              "      <th>AWE24</th>\n",
              "      <th>AWE25</th>\n",
              "      <th>AWE26</th>\n",
              "      <th>AWE27</th>\n",
              "      <th>...</th>\n",
              "      <th>AWE161</th>\n",
              "      <th>AWE162</th>\n",
              "      <th>AWE163</th>\n",
              "      <th>AWE164</th>\n",
              "      <th>AWE165</th>\n",
              "      <th>AWE166</th>\n",
              "      <th>AWE167</th>\n",
              "      <th>AWE168</th>\n",
              "      <th>AWE169</th>\n",
              "      <th>AWE170</th>\n",
              "      <th>AWE171</th>\n",
              "      <th>AWE172</th>\n",
              "      <th>AWE173</th>\n",
              "      <th>AWE174</th>\n",
              "      <th>AWE175</th>\n",
              "      <th>AWE176</th>\n",
              "      <th>AWE177</th>\n",
              "      <th>AWE178</th>\n",
              "      <th>AWE179</th>\n",
              "      <th>AWE180</th>\n",
              "      <th>AWE181</th>\n",
              "      <th>AWE182</th>\n",
              "      <th>AWE183</th>\n",
              "      <th>AWE184</th>\n",
              "      <th>AWE185</th>\n",
              "      <th>AWE186</th>\n",
              "      <th>AWE187</th>\n",
              "      <th>AWE188</th>\n",
              "      <th>AWE189</th>\n",
              "      <th>AWE190</th>\n",
              "      <th>AWE191</th>\n",
              "      <th>AWE192</th>\n",
              "      <th>AWE193</th>\n",
              "      <th>AWE194</th>\n",
              "      <th>AWE195</th>\n",
              "      <th>AWE196</th>\n",
              "      <th>AWE197</th>\n",
              "      <th>AWE198</th>\n",
              "      <th>AWE199</th>\n",
              "      <th>AWE200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1536</th>\n",
              "      <td>1536</td>\n",
              "      <td>40</td>\n",
              "      <td>Andrew E.</td>\n",
              "      <td>3</td>\n",
              "      <td>10/3/2014</td>\n",
              "      <td>.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>.</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[(., .)]</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1536</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11970</th>\n",
              "      <td>11970</td>\n",
              "      <td>550</td>\n",
              "      <td>Heather M.</td>\n",
              "      <td>5</td>\n",
              "      <td>3/29/2018</td>\n",
              "      <td>.</td>\n",
              "      <td>5.0</td>\n",
              "      <td>.</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[(., .)]</td>\n",
              "      <td>[.]</td>\n",
              "      <td>[]</td>\n",
              "      <td>11970</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22847</th>\n",
              "      <td>22847</td>\n",
              "      <td>1229</td>\n",
              "      <td>Tamara U.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/8/2017</td>\n",
              "      <td>I have an ������������������������������������...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>I have an ������������������������������������...</td>\n",
              "      <td>[I, have, an, ��������������������������������...</td>\n",
              "      <td>[(I, PRP), (have, VBP), (an, DT), (�����������...</td>\n",
              "      <td>[I, have, an, ��������������������������������...</td>\n",
              "      <td>[���������������������������������������������...</td>\n",
              "      <td>22847</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23071</th>\n",
              "      <td>23071</td>\n",
              "      <td>1240</td>\n",
              "      <td>Adela H.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/20/2018</td>\n",
              "      <td>he ��������������������������������������������</td>\n",
              "      <td>3.5</td>\n",
              "      <td>he ��������������������������������������������</td>\n",
              "      <td>[he, �����������������������������������������...</td>\n",
              "      <td>[(he, PRP), (���������������������������������...</td>\n",
              "      <td>[he, �����������������������������������������...</td>\n",
              "      <td>[��������������������������������������������]</td>\n",
              "      <td>23071</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23921</th>\n",
              "      <td>23921</td>\n",
              "      <td>1276</td>\n",
              "      <td>Mandy M.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/15/2019</td>\n",
              "      <td>Dr. ������������������������������������������...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Dr. ������������������������������������������...</td>\n",
              "      <td>[Dr, ., ��������������������������������������...</td>\n",
              "      <td>[(Dr, NNP), (., .), (�������������������������...</td>\n",
              "      <td>[Dr, ., ��������������������������������������...</td>\n",
              "      <td>[���������������������������������������������...</td>\n",
              "      <td>23921</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24347</th>\n",
              "      <td>24347</td>\n",
              "      <td>1298</td>\n",
              "      <td>Alan S.</td>\n",
              "      <td>1</td>\n",
              "      <td>10/14/2019</td>\n",
              "      <td>Do not go here. ������������������������������...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Do not go here. ������������������������������...</td>\n",
              "      <td>[Do, not, go, here, ., �����������������������...</td>\n",
              "      <td>[(Do, VB), (not, RB), (go, VB), (here, RB), (....</td>\n",
              "      <td>[Do, not, go, here, ., �����������������������...</td>\n",
              "      <td>[���������������������������������������������...</td>\n",
              "      <td>24347</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24639</th>\n",
              "      <td>24639</td>\n",
              "      <td>1311</td>\n",
              "      <td>Jean G.</td>\n",
              "      <td>5</td>\n",
              "      <td>4/15/2017</td>\n",
              "      <td>Dr. ������������������������������������������...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Dr. ������������������������������������������...</td>\n",
              "      <td>[Dr, ., ��������������������������������������...</td>\n",
              "      <td>[(Dr, NNP), (., .), (�������������������������...</td>\n",
              "      <td>[Dr, ., ��������������������������������������...</td>\n",
              "      <td>[���������������������������������������������...</td>\n",
              "      <td>24639</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7 rows × 213 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  doctorID    username  rating  ... AWE197 AWE198  AWE199 AWE200\n",
              "1536         1536        40   Andrew E.       3  ...    NaN    NaN     NaN    NaN\n",
              "11970       11970       550  Heather M.       5  ...    NaN    NaN     NaN    NaN\n",
              "22847       22847      1229   Tamara U.       5  ...    NaN    NaN     NaN    NaN\n",
              "23071       23071      1240    Adela H.       5  ...    NaN    NaN     NaN    NaN\n",
              "23921       23921      1276    Mandy M.       5  ...    NaN    NaN     NaN    NaN\n",
              "24347       24347      1298     Alan S.       1  ...    NaN    NaN     NaN    NaN\n",
              "24639       24639      1311     Jean G.       5  ...    NaN    NaN     NaN    NaN\n",
              "\n",
              "[7 rows x 213 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPboo4ISYgf1",
        "outputId": "843b14c6-0171-4596-90b8-eff1e03d4538"
      },
      "source": [
        "df_w_glove.drop(df_w_glove.index[[1536,11970,22847,23071,23921,24347,24639]]).reset_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.reset_index of        Unnamed: 0  doctorID     username  ...    AWE198    AWE199    AWE200\n",
              "0               0         0   Whitney W.  ... -0.054575  0.023107 -0.063070\n",
              "1               1         0   Kristin R.  ...  0.033650  0.059103  0.035592\n",
              "2               2         0   Allyson F.  ...  0.030723  0.031848 -0.115006\n",
              "3               3         0     Brian J.  ...  0.059271  0.000523 -0.091125\n",
              "4               4         0  Stephani P.  ... -0.012073 -0.006631 -0.067276\n",
              "...           ...       ...          ...  ...       ...       ...       ...\n",
              "52686       52686      2606          NaN  ...  0.111247  0.023115  0.024175\n",
              "52687       52687      2606          NaN  ...  0.012114  0.038613  0.005946\n",
              "52688       52688      2606          NaN  ...  0.194254  0.078377 -0.069170\n",
              "52689       52689      2606          NaN  ...  0.149688  0.065580  0.044845\n",
              "52690       52690      2606          NaN  ...  0.195052  0.008436 -0.144907\n",
              "\n",
              "[52684 rows x 215 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IARdrQmuZhzy",
        "outputId": "87b96de2-8a3a-417f-ae7a-3cd2eeb7eade"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df_w_glove.to_pickle('df_w_glove')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBNU1Fj8rCQd"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "may46uV8766-",
        "outputId": "f9ddb4fb-c0fe-4ed3-e7d7-dc058ba393fb"
      },
      "source": [
        "# create a corpus (a list of strings) of all cleaned reviews (lemmas)\n",
        "review_list = df['final'].tolist()\n",
        "\n",
        "print(\"The number of reviews:\", len(review_list))\n",
        "\n",
        "print(\"The data type of reviews:\", type(review_list))\n",
        "print(\"The data type of the first review:\", type(review_list[0]))\n",
        "\n",
        "review_list[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of reviews: 52691\n",
            "The data type of reviews: <class 'list'>\n",
            "The data type of the first review: <class 'list'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['see',\n",
              " 'star',\n",
              " 'review',\n",
              " 'family',\n",
              " 'practice',\n",
              " 'rancho',\n",
              " 'call',\n",
              " 'make',\n",
              " 'day',\n",
              " 'appointment',\n",
              " 'send',\n",
              " 'upland',\n",
              " 'office',\n",
              " 'see',\n",
              " 'super',\n",
              " 'nice',\n",
              " 'name',\n",
              " 'heidi',\n",
              " 'throughout',\n",
              " 'checkup',\n",
              " 'also',\n",
              " 'explain',\n",
              " 'condition',\n",
              " 'patiently',\n",
              " 'happy',\n",
              " 'find',\n",
              " 'nice',\n",
              " 'family',\n",
              " 'practice',\n",
              " 'office',\n",
              " 'near',\n",
              " 'home']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njG_5Ml120OP",
        "outputId": "58ae3a79-ec73-42c3-9196-adf11f9a8648"
      },
      "source": [
        "# count the number of unique tokens\n",
        "from nltk import FreqDist\n",
        "\n",
        "# use FreqDis() to find the frequency disctibution for each token\n",
        "%time fdist_reviews= FreqDist([word for sublist in review_list for word in sublist])\n",
        "print('The total number of tokens:',fdist_reviews.N())\n",
        "print('The number of unique tokens:', len(fdist_reviews))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.34 s, sys: 44.5 ms, total: 2.38 s\n",
            "Wall time: 2.4 s\n",
            "The total number of tokens: 2832965\n",
            "The number of unique tokens: 41542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMKoNWvC4nv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "76a730ef-73c1-4288-e5ae-b0ca29df5f68"
      },
      "source": [
        "#currently we have a list of lists of tokens\n",
        "#we need each list item to be a string and not a list itself for this\n",
        "\n",
        "reviews = []\n",
        "\n",
        "for review in review_list:\n",
        "  review_string = ' '.join(map(str, review))\n",
        "  #print(review_string)\n",
        "  reviews.append(review_string)\n",
        "\n",
        "print(\"The number of reviews:\", len(reviews))\n",
        "\n",
        "print(\"The data type of reviews:\", type(reviews))\n",
        "print(\"The data type of the first review:\", type(reviews[0]))\n",
        "\n",
        "reviews[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of reviews: 52691\n",
            "The data type of reviews: <class 'list'>\n",
            "The data type of the first review: <class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'see star review family practice rancho call make day appointment send upland office see super nice name heidi throughout checkup also explain condition patiently happy find nice family practice office near home'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8pRDmYO3W17"
      },
      "source": [
        "trying to find tf-idf of the entire corpus was not practical as the corpus was rather large and doing so took too long to run; therefore we are doing choosing to include tokens with a document frequency of 1% and higher.\n",
        "\n",
        "our corpus has 52691 reviews, more than 1% of that means the document frequency is more than 526, or 527 above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmubRtL-1Vj4",
        "outputId": "9622759e-00b7-4634-8276-8965a26196bf"
      },
      "source": [
        "len(reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52691"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxU0D-DZ2w21",
        "outputId": "0bf6ab36-143e-4006-d9cc-cac5f66b2c6e"
      },
      "source": [
        "# create a token list with a minimal document frequency count (number of documents which a token occurs)\n",
        "token_list =[]\n",
        "for token_id, count in dictionary.dfs.items(): # .dfs returns doctument frequecy based on token id (int)\n",
        "  if count>526:\n",
        "    token_list.append(dictionary.get(token_id)) #.get() retrieve the actual tokens based on id\n",
        "\n",
        "print(len(token_list))\n",
        "print(token_list[:3]) # see the list of tokens "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "718\n",
            "['see', 'star', 'review']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4oC1rtp65Ln"
      },
      "source": [
        "# create a tfidf with from a term list (i.e., these high frequent terms) \n",
        "\n",
        "def tf_idf_nltk_wordlist(corpus, term_list):\n",
        "# tokenization\n",
        "  tokenized_corpus = [doc.split() for doc in corpus]\n",
        "\n",
        "  texts = TextCollection(tokenized_corpus)\n",
        "  tf_idf = dict()\n",
        "\n",
        "  for i, doc in enumerate(tokenized_corpus):\n",
        "    tf_idf_score = dict()\n",
        "\n",
        "    doc_num = 'doc_' + str(i+1)\n",
        "    \n",
        "    for term in doc:\n",
        "      if term in term_list:\n",
        "        tf_idf_score[term] = texts.tf_idf(term, doc)\n",
        "    \n",
        "    tf_idf[doc_num] = tf_idf_score\n",
        "  \n",
        "  return tf_idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx8a1grY7lvI",
        "outputId": "7465dacc-0a1b-426e-f085-9e88307dcdb1"
      },
      "source": [
        "%time tf_idf = tf_idf_nltk_wordlist(reviews, token_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 3s, sys: 766 ms, total: 1min 4s\n",
            "Wall time: 1min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W97g3a8YsxiQ",
        "outputId": "b9d17c42-6ad4-43d0-e67a-325201fc3dc1"
      },
      "source": [
        "list(tf_idf.items())[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('doc_1',\n",
              " {'also': 0.06326277740974275,\n",
              "  'appointment': 0.048520202656472465,\n",
              "  'call': 0.045945248946230866,\n",
              "  'condition': 0.11681581939886725,\n",
              "  'day': 0.05527427663520351,\n",
              "  'explain': 0.08140649237576478,\n",
              "  'family': 0.1596063513407675,\n",
              "  'find': 0.06376981446361109,\n",
              "  'happy': 0.08493710147007467,\n",
              "  'home': 0.09297735267774228,\n",
              "  'make': 0.04064671002402658,\n",
              "  'name': 0.09214030024960564,\n",
              "  'nice': 0.13461646496911486,\n",
              "  'office': 0.08374506305316023,\n",
              "  'practice': 0.20480079144391533,\n",
              "  'rancho': 0.13300421115074376,\n",
              "  'review': 0.07157908535487548,\n",
              "  'see': 0.07449099629591872,\n",
              "  'send': 0.08245927505383199,\n",
              "  'star': 0.07546778411603322,\n",
              "  'super': 0.10092736899136612})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "IHdPoE2ZKbtA",
        "outputId": "f12fa331-bf80-4f06-e311-66ca58889cac"
      },
      "source": [
        "tf_idf_df = pd.DataFrame.from_dict(tf_idf).T.reset_index().reset_index()\n",
        "\n",
        "tf_idf_df = tf_idf_df.rename (columns={'level_0': 'review_id'}).drop(['index'], axis =1)\n",
        "\n",
        "tf_idf_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>see</th>\n",
              "      <th>star</th>\n",
              "      <th>review</th>\n",
              "      <th>family</th>\n",
              "      <th>practice</th>\n",
              "      <th>rancho</th>\n",
              "      <th>call</th>\n",
              "      <th>make</th>\n",
              "      <th>day</th>\n",
              "      <th>appointment</th>\n",
              "      <th>send</th>\n",
              "      <th>office</th>\n",
              "      <th>super</th>\n",
              "      <th>nice</th>\n",
              "      <th>name</th>\n",
              "      <th>also</th>\n",
              "      <th>explain</th>\n",
              "      <th>condition</th>\n",
              "      <th>happy</th>\n",
              "      <th>find</th>\n",
              "      <th>home</th>\n",
              "      <th>switch</th>\n",
              "      <th>month</th>\n",
              "      <th>base</th>\n",
              "      <th>referral</th>\n",
              "      <th>previous</th>\n",
              "      <th>get</th>\n",
              "      <th>insurance</th>\n",
              "      <th>shot</th>\n",
              "      <th>tell</th>\n",
              "      <th>long</th>\n",
              "      <th>free</th>\n",
              "      <th>birth</th>\n",
              "      <th>control</th>\n",
              "      <th>still</th>\n",
              "      <th>want</th>\n",
              "      <th>would</th>\n",
              "      <th>cost</th>\n",
              "      <th>decide</th>\n",
              "      <th>...</th>\n",
              "      <th>mistake</th>\n",
              "      <th>dirty</th>\n",
              "      <th>realize</th>\n",
              "      <th>miss</th>\n",
              "      <th>moment</th>\n",
              "      <th>card</th>\n",
              "      <th>honest</th>\n",
              "      <th>ridiculous</th>\n",
              "      <th>total</th>\n",
              "      <th>machine</th>\n",
              "      <th>dad</th>\n",
              "      <th>mention</th>\n",
              "      <th>skill</th>\n",
              "      <th>add</th>\n",
              "      <th>voice</th>\n",
              "      <th>came</th>\n",
              "      <th>zero</th>\n",
              "      <th>elsewhere</th>\n",
              "      <th>note</th>\n",
              "      <th>ultrasound</th>\n",
              "      <th>.....</th>\n",
              "      <th>pregnancy</th>\n",
              "      <th>apparently</th>\n",
              "      <th>building</th>\n",
              "      <th>obviously</th>\n",
              "      <th>glass</th>\n",
              "      <th>scan</th>\n",
              "      <th>cancer</th>\n",
              "      <th>stuff</th>\n",
              "      <th>improve</th>\n",
              "      <th>basically</th>\n",
              "      <th>drug</th>\n",
              "      <th>riverside</th>\n",
              "      <th>surgeon</th>\n",
              "      <th>loma</th>\n",
              "      <th>therapy</th>\n",
              "      <th>tech</th>\n",
              "      <th>temecula</th>\n",
              "      <th>������������������������������������</th>\n",
              "      <th>������������</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.074491</td>\n",
              "      <td>0.075468</td>\n",
              "      <td>0.071579</td>\n",
              "      <td>0.159606</td>\n",
              "      <td>0.204801</td>\n",
              "      <td>0.133004</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.040647</td>\n",
              "      <td>0.055274</td>\n",
              "      <td>0.048520</td>\n",
              "      <td>0.082459</td>\n",
              "      <td>0.083745</td>\n",
              "      <td>0.100927</td>\n",
              "      <td>0.134616</td>\n",
              "      <td>0.09214</td>\n",
              "      <td>0.063263</td>\n",
              "      <td>0.081406</td>\n",
              "      <td>0.116816</td>\n",
              "      <td>0.084937</td>\n",
              "      <td>0.06377</td>\n",
              "      <td>0.092977</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.040402</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.144276</td>\n",
              "      <td>0.024919</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.029979</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.022711</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.036506</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.066031</td>\n",
              "      <td>0.040151</td>\n",
              "      <td>0.072621</td>\n",
              "      <td>0.061037</td>\n",
              "      <td>0.070455</td>\n",
              "      <td>0.0983</td>\n",
              "      <td>0.039251</td>\n",
              "      <td>0.132253</td>\n",
              "      <td>0.025837</td>\n",
              "      <td>0.075544</td>\n",
              "      <td>0.069322</td>\n",
              "      <td>0.070724</td>\n",
              "      <td>0.075324</td>\n",
              "      <td>0.038624</td>\n",
              "      <td>0.032944</td>\n",
              "      <td>0.020188</td>\n",
              "      <td>0.06745</td>\n",
              "      <td>0.056226</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.030560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.065480</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.109132</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.150999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.077385</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.040604</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.067532</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 719 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   review_id       see  ...  ������������������������������������  ������������\n",
              "0          0  0.074491  ...                                   NaN           NaN\n",
              "1          1  0.040402  ...                                   NaN           NaN\n",
              "2          2  0.030560  ...                                   NaN           NaN\n",
              "3          3       NaN  ...                                   NaN           NaN\n",
              "4          4       NaN  ...                                   NaN           NaN\n",
              "\n",
              "[5 rows x 719 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "owkCkujPvQ4f",
        "outputId": "33f47b2a-ab90-4e50-ed69-aaf9eaa305fe"
      },
      "source": [
        "# replace NA with 0\n",
        "tf_idf_df = tf_idf_df.fillna(0)\n",
        "\n",
        "tf_idf_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>see</th>\n",
              "      <th>star</th>\n",
              "      <th>review</th>\n",
              "      <th>family</th>\n",
              "      <th>practice</th>\n",
              "      <th>rancho</th>\n",
              "      <th>call</th>\n",
              "      <th>make</th>\n",
              "      <th>day</th>\n",
              "      <th>appointment</th>\n",
              "      <th>send</th>\n",
              "      <th>office</th>\n",
              "      <th>super</th>\n",
              "      <th>nice</th>\n",
              "      <th>name</th>\n",
              "      <th>also</th>\n",
              "      <th>explain</th>\n",
              "      <th>condition</th>\n",
              "      <th>happy</th>\n",
              "      <th>find</th>\n",
              "      <th>home</th>\n",
              "      <th>switch</th>\n",
              "      <th>month</th>\n",
              "      <th>base</th>\n",
              "      <th>referral</th>\n",
              "      <th>previous</th>\n",
              "      <th>get</th>\n",
              "      <th>insurance</th>\n",
              "      <th>shot</th>\n",
              "      <th>tell</th>\n",
              "      <th>long</th>\n",
              "      <th>free</th>\n",
              "      <th>birth</th>\n",
              "      <th>control</th>\n",
              "      <th>still</th>\n",
              "      <th>want</th>\n",
              "      <th>would</th>\n",
              "      <th>cost</th>\n",
              "      <th>decide</th>\n",
              "      <th>...</th>\n",
              "      <th>mistake</th>\n",
              "      <th>dirty</th>\n",
              "      <th>realize</th>\n",
              "      <th>miss</th>\n",
              "      <th>moment</th>\n",
              "      <th>card</th>\n",
              "      <th>honest</th>\n",
              "      <th>ridiculous</th>\n",
              "      <th>total</th>\n",
              "      <th>machine</th>\n",
              "      <th>dad</th>\n",
              "      <th>mention</th>\n",
              "      <th>skill</th>\n",
              "      <th>add</th>\n",
              "      <th>voice</th>\n",
              "      <th>came</th>\n",
              "      <th>zero</th>\n",
              "      <th>elsewhere</th>\n",
              "      <th>note</th>\n",
              "      <th>ultrasound</th>\n",
              "      <th>.....</th>\n",
              "      <th>pregnancy</th>\n",
              "      <th>apparently</th>\n",
              "      <th>building</th>\n",
              "      <th>obviously</th>\n",
              "      <th>glass</th>\n",
              "      <th>scan</th>\n",
              "      <th>cancer</th>\n",
              "      <th>stuff</th>\n",
              "      <th>improve</th>\n",
              "      <th>basically</th>\n",
              "      <th>drug</th>\n",
              "      <th>riverside</th>\n",
              "      <th>surgeon</th>\n",
              "      <th>loma</th>\n",
              "      <th>therapy</th>\n",
              "      <th>tech</th>\n",
              "      <th>temecula</th>\n",
              "      <th>������������������������������������</th>\n",
              "      <th>������������</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.074491</td>\n",
              "      <td>0.075468</td>\n",
              "      <td>0.071579</td>\n",
              "      <td>0.159606</td>\n",
              "      <td>0.204801</td>\n",
              "      <td>0.133004</td>\n",
              "      <td>0.045945</td>\n",
              "      <td>0.040647</td>\n",
              "      <td>0.055274</td>\n",
              "      <td>0.048520</td>\n",
              "      <td>0.082459</td>\n",
              "      <td>0.083745</td>\n",
              "      <td>0.100927</td>\n",
              "      <td>0.134616</td>\n",
              "      <td>0.09214</td>\n",
              "      <td>0.063263</td>\n",
              "      <td>0.081406</td>\n",
              "      <td>0.116816</td>\n",
              "      <td>0.084937</td>\n",
              "      <td>0.06377</td>\n",
              "      <td>0.092977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.040402</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.144276</td>\n",
              "      <td>0.024919</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029979</td>\n",
              "      <td>0.026316</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036506</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066031</td>\n",
              "      <td>0.040151</td>\n",
              "      <td>0.072621</td>\n",
              "      <td>0.061037</td>\n",
              "      <td>0.070455</td>\n",
              "      <td>0.0983</td>\n",
              "      <td>0.039251</td>\n",
              "      <td>0.132253</td>\n",
              "      <td>0.025837</td>\n",
              "      <td>0.075544</td>\n",
              "      <td>0.069322</td>\n",
              "      <td>0.070724</td>\n",
              "      <td>0.075324</td>\n",
              "      <td>0.038624</td>\n",
              "      <td>0.032944</td>\n",
              "      <td>0.020188</td>\n",
              "      <td>0.06745</td>\n",
              "      <td>0.056226</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.030560</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109132</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.150999</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077385</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040604</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067532</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 719 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   review_id       see  ...  ������������������������������������  ������������\n",
              "0          0  0.074491  ...                                   0.0           0.0\n",
              "1          1  0.040402  ...                                   0.0           0.0\n",
              "2          2  0.030560  ...                                   0.0           0.0\n",
              "3          3  0.000000  ...                                   0.0           0.0\n",
              "4          4  0.000000  ...                                   0.0           0.0\n",
              "\n",
              "[5 rows x 719 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_iLKHv7EV7q"
      },
      "source": [
        "tf_idf_df.to_csv('tf_idf.csv')\n",
        "tf_idf_df.to_pickle('tf_idf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNhOoVWSdb1s"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QolUdFjrdd9f"
      },
      "source": [
        "# pickle file cannot be read on Colab.\n",
        "# This is a dataset only having NER, but the order is the same as prepared_data.\n",
        "df_ner_spacy = pd.read_pickle('/content/drive/MyDrive/Colab_Notebooks/df_ner_spacy.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4aAMzZfVd12i",
        "outputId": "e484c19d-990e-4ee3-a57f-bee55c4ccc9b"
      },
      "source": [
        "df_ner_spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>dict_ner</th>\n",
              "      <th>ORG</th>\n",
              "      <th>GPE</th>\n",
              "      <th>PERSON</th>\n",
              "      <th>MONEY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>{'ORG': 0, 'GPE': 1, 'PERSON': 1, 'MONEY': 0}</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>{'ORG': 2, 'GPE': 1, 'PERSON': 0, 'MONEY': 1}</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "      <td>{'ORG': 0, 'GPE': 0, 'PERSON': 1, 'MONEY': 0}</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Brian J.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>Large waiting room and welcoming  staff. Dr. S...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Large waiting room and welcoming staff. Dr. Si...</td>\n",
              "      <td>{'ORG': 0, 'GPE': 0, 'PERSON': 1, 'MONEY': 0}</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Stephani P.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/2/2015</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>{'ORG': 0, 'GPE': 0, 'PERSON': 1, 'MONEY': 0}</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52686</th>\n",
              "      <td>52686</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>10/21/2015</td>\n",
              "      <td>I live in LA but was on a trip in Palm Springs...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I live in LA but was on a trip in Palm Springs...</td>\n",
              "      <td>{'ORG': 2, 'GPE': 3, 'PERSON': 1, 'MONEY': 1}</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52687</th>\n",
              "      <td>52687</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>9/9/2015</td>\n",
              "      <td>These people are crooks!!!So to get started th...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>These people are crooks!!!So to get started th...</td>\n",
              "      <td>{'ORG': 1, 'GPE': 0, 'PERSON': 0, 'MONEY': 0}</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52688</th>\n",
              "      <td>52688</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>11/13/2014</td>\n",
              "      <td>Sadly, I think the previous comments and revie...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>Sadly, I think the previous comments and revie...</td>\n",
              "      <td>{'ORG': 0, 'GPE': 0, 'PERSON': 0, 'MONEY': 0}</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52689</th>\n",
              "      <td>52689</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>7/23/2014</td>\n",
              "      <td>I've been waiting for just over an hour now an...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I have been waiting for just over an hour now ...</td>\n",
              "      <td>{'ORG': 0, 'GPE': 0, 'PERSON': 1, 'MONEY': 0}</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52690</th>\n",
              "      <td>52690</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>12/6/2011</td>\n",
              "      <td>I went here for a broken foot, because they ad...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I went here for a broken foot, because they ad...</td>\n",
              "      <td>{'ORG': 4, 'GPE': 1, 'PERSON': 1, 'MONEY': 0}</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52691 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  doctorID     username  rating  ... ORG GPE  PERSON MONEY\n",
              "0               0         0   Whitney W.       5  ...   0   1       1     0\n",
              "1               1         0   Kristin R.       5  ...   2   1       0     1\n",
              "2               2         0   Allyson F.       5  ...   0   0       1     0\n",
              "3               3         0     Brian J.       5  ...   0   0       1     0\n",
              "4               4         0  Stephani P.       5  ...   0   0       1     0\n",
              "...           ...       ...          ...     ...  ...  ..  ..     ...   ...\n",
              "52686       52686      2606          NaN       1  ...   2   3       1     1\n",
              "52687       52687      2606          NaN       1  ...   1   0       0     0\n",
              "52688       52688      2606          NaN       1  ...   0   0       0     0\n",
              "52689       52689      2606          NaN       1  ...   0   0       1     0\n",
              "52690       52690      2606          NaN       1  ...   4   1       1     0\n",
              "\n",
              "[52691 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrzXWxKdvrYG"
      },
      "source": [
        "df_ner_spacy.to_csv('/content/drive/MyDrive/Colab_Notebooks/df_ner_spacy.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2hzx3HqafhC"
      },
      "source": [
        "## Aggregated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp69rSX_ckdF"
      },
      "source": [
        "def agg_cols(dat, col):\n",
        "  dat2 = dat.groupby(['doctorID']).agg({col : 'mean'}).reset_index()\n",
        "  return dat2[col]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaQEbwfUdPE2"
      },
      "source": [
        "df_glove_agg = df_w_glove.groupby([\t'doctorID',\t]).agg({'rating' : 'mean'}).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlmuWdUvco1F"
      },
      "source": [
        "for label in labels:\n",
        "  df_glove_agg[label] = agg_cols(df_w_glove,label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WUa67OT6oFYe",
        "outputId": "6d173243-b02f-4921-8d64-4a11ad7af350"
      },
      "source": [
        "df_w_glove"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>final</th>\n",
              "      <th>Index</th>\n",
              "      <th>Length</th>\n",
              "      <th>Lexical Diversity</th>\n",
              "      <th>AWE1</th>\n",
              "      <th>AWE2</th>\n",
              "      <th>AWE3</th>\n",
              "      <th>AWE4</th>\n",
              "      <th>AWE5</th>\n",
              "      <th>AWE6</th>\n",
              "      <th>AWE7</th>\n",
              "      <th>AWE8</th>\n",
              "      <th>AWE9</th>\n",
              "      <th>AWE10</th>\n",
              "      <th>AWE11</th>\n",
              "      <th>AWE12</th>\n",
              "      <th>AWE13</th>\n",
              "      <th>AWE14</th>\n",
              "      <th>AWE15</th>\n",
              "      <th>AWE16</th>\n",
              "      <th>AWE17</th>\n",
              "      <th>AWE18</th>\n",
              "      <th>AWE19</th>\n",
              "      <th>AWE20</th>\n",
              "      <th>AWE21</th>\n",
              "      <th>AWE22</th>\n",
              "      <th>AWE23</th>\n",
              "      <th>AWE24</th>\n",
              "      <th>AWE25</th>\n",
              "      <th>...</th>\n",
              "      <th>AWE161</th>\n",
              "      <th>AWE162</th>\n",
              "      <th>AWE163</th>\n",
              "      <th>AWE164</th>\n",
              "      <th>AWE165</th>\n",
              "      <th>AWE166</th>\n",
              "      <th>AWE167</th>\n",
              "      <th>AWE168</th>\n",
              "      <th>AWE169</th>\n",
              "      <th>AWE170</th>\n",
              "      <th>AWE171</th>\n",
              "      <th>AWE172</th>\n",
              "      <th>AWE173</th>\n",
              "      <th>AWE174</th>\n",
              "      <th>AWE175</th>\n",
              "      <th>AWE176</th>\n",
              "      <th>AWE177</th>\n",
              "      <th>AWE178</th>\n",
              "      <th>AWE179</th>\n",
              "      <th>AWE180</th>\n",
              "      <th>AWE181</th>\n",
              "      <th>AWE182</th>\n",
              "      <th>AWE183</th>\n",
              "      <th>AWE184</th>\n",
              "      <th>AWE185</th>\n",
              "      <th>AWE186</th>\n",
              "      <th>AWE187</th>\n",
              "      <th>AWE188</th>\n",
              "      <th>AWE189</th>\n",
              "      <th>AWE190</th>\n",
              "      <th>AWE191</th>\n",
              "      <th>AWE192</th>\n",
              "      <th>AWE193</th>\n",
              "      <th>AWE194</th>\n",
              "      <th>AWE195</th>\n",
              "      <th>AWE196</th>\n",
              "      <th>AWE197</th>\n",
              "      <th>AWE198</th>\n",
              "      <th>AWE199</th>\n",
              "      <th>AWE200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>[After, seeing, 4, ., 5, stars, reviews, on, a...</td>\n",
              "      <td>[(After, IN), (seeing, VBG), (4, CD), (., .), ...</td>\n",
              "      <td>[After, see, 4, ., 5, star, review, on, a, fam...</td>\n",
              "      <td>[see, star, review, family, practice, rancho, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>1.142857</td>\n",
              "      <td>0.091733</td>\n",
              "      <td>0.116644</td>\n",
              "      <td>-0.057185</td>\n",
              "      <td>-0.103456</td>\n",
              "      <td>0.104724</td>\n",
              "      <td>0.060924</td>\n",
              "      <td>0.542219</td>\n",
              "      <td>0.033925</td>\n",
              "      <td>-0.001545</td>\n",
              "      <td>-0.158022</td>\n",
              "      <td>-0.115832</td>\n",
              "      <td>-0.099747</td>\n",
              "      <td>-0.461253</td>\n",
              "      <td>-0.076329</td>\n",
              "      <td>0.042458</td>\n",
              "      <td>0.003450</td>\n",
              "      <td>0.018623</td>\n",
              "      <td>-0.012138</td>\n",
              "      <td>-0.159880</td>\n",
              "      <td>-0.212174</td>\n",
              "      <td>-0.074341</td>\n",
              "      <td>0.028398</td>\n",
              "      <td>-0.125563</td>\n",
              "      <td>-0.013380</td>\n",
              "      <td>-0.110495</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013001</td>\n",
              "      <td>-0.036041</td>\n",
              "      <td>-0.020788</td>\n",
              "      <td>0.164829</td>\n",
              "      <td>0.028368</td>\n",
              "      <td>-0.022816</td>\n",
              "      <td>-0.103664</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.140346</td>\n",
              "      <td>-0.026546</td>\n",
              "      <td>-0.054877</td>\n",
              "      <td>0.069513</td>\n",
              "      <td>0.113889</td>\n",
              "      <td>0.017234</td>\n",
              "      <td>0.007763</td>\n",
              "      <td>-0.107550</td>\n",
              "      <td>-0.034695</td>\n",
              "      <td>-0.110795</td>\n",
              "      <td>0.085910</td>\n",
              "      <td>-0.062500</td>\n",
              "      <td>-0.008499</td>\n",
              "      <td>0.032974</td>\n",
              "      <td>0.096622</td>\n",
              "      <td>-0.044835</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.041028</td>\n",
              "      <td>-0.007234</td>\n",
              "      <td>-0.113410</td>\n",
              "      <td>-0.076270</td>\n",
              "      <td>0.209669</td>\n",
              "      <td>-0.087744</td>\n",
              "      <td>0.015287</td>\n",
              "      <td>0.211982</td>\n",
              "      <td>-0.206838</td>\n",
              "      <td>-0.047367</td>\n",
              "      <td>-0.120737</td>\n",
              "      <td>0.008818</td>\n",
              "      <td>-0.054575</td>\n",
              "      <td>0.023107</td>\n",
              "      <td>-0.063070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>[I, just, switched, this, month, to, Rancho, W...</td>\n",
              "      <td>[(I, PRP), (just, RB), (switched, VBD), (this,...</td>\n",
              "      <td>[I, just, switch, this, month, to, Rancho, Wel...</td>\n",
              "      <td>[switch, month, rancho, wellness, base, referr...</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>1.229167</td>\n",
              "      <td>-0.050948</td>\n",
              "      <td>0.081066</td>\n",
              "      <td>-0.100553</td>\n",
              "      <td>-0.060556</td>\n",
              "      <td>0.137765</td>\n",
              "      <td>-0.016419</td>\n",
              "      <td>0.645886</td>\n",
              "      <td>-0.037969</td>\n",
              "      <td>0.105021</td>\n",
              "      <td>-0.003371</td>\n",
              "      <td>0.010374</td>\n",
              "      <td>0.050843</td>\n",
              "      <td>-0.560533</td>\n",
              "      <td>-0.116482</td>\n",
              "      <td>0.111024</td>\n",
              "      <td>-0.076210</td>\n",
              "      <td>-0.042454</td>\n",
              "      <td>0.133326</td>\n",
              "      <td>-0.102722</td>\n",
              "      <td>-0.136448</td>\n",
              "      <td>0.054454</td>\n",
              "      <td>-0.025712</td>\n",
              "      <td>-0.053532</td>\n",
              "      <td>-0.001973</td>\n",
              "      <td>-0.155463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055591</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.136734</td>\n",
              "      <td>0.134832</td>\n",
              "      <td>-0.006499</td>\n",
              "      <td>-0.041918</td>\n",
              "      <td>-0.047471</td>\n",
              "      <td>-0.058321</td>\n",
              "      <td>0.064452</td>\n",
              "      <td>0.042912</td>\n",
              "      <td>-0.054448</td>\n",
              "      <td>0.119691</td>\n",
              "      <td>0.059884</td>\n",
              "      <td>0.012066</td>\n",
              "      <td>-0.046926</td>\n",
              "      <td>-0.173230</td>\n",
              "      <td>-0.016169</td>\n",
              "      <td>-0.213488</td>\n",
              "      <td>0.149717</td>\n",
              "      <td>0.050852</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.129645</td>\n",
              "      <td>0.077635</td>\n",
              "      <td>-0.038020</td>\n",
              "      <td>-0.025267</td>\n",
              "      <td>0.132430</td>\n",
              "      <td>0.161996</td>\n",
              "      <td>-0.029400</td>\n",
              "      <td>0.045942</td>\n",
              "      <td>0.027476</td>\n",
              "      <td>-0.070606</td>\n",
              "      <td>-0.067446</td>\n",
              "      <td>0.095191</td>\n",
              "      <td>-0.094072</td>\n",
              "      <td>0.094781</td>\n",
              "      <td>-0.127568</td>\n",
              "      <td>0.104540</td>\n",
              "      <td>0.033650</td>\n",
              "      <td>0.059103</td>\n",
              "      <td>0.035592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, are, very...</td>\n",
              "      <td>[(I, PRP), (love, VBP), (Rancho, NNP), (wellne...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, be, very,...</td>\n",
              "      <td>[love, rancho, wellness, organize, professiona...</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "      <td>1.181818</td>\n",
              "      <td>-0.087739</td>\n",
              "      <td>0.135671</td>\n",
              "      <td>0.005935</td>\n",
              "      <td>-0.006324</td>\n",
              "      <td>0.083941</td>\n",
              "      <td>-0.002159</td>\n",
              "      <td>0.692737</td>\n",
              "      <td>-0.035426</td>\n",
              "      <td>-0.009401</td>\n",
              "      <td>-0.044613</td>\n",
              "      <td>-0.323754</td>\n",
              "      <td>0.010717</td>\n",
              "      <td>-0.568150</td>\n",
              "      <td>-0.025733</td>\n",
              "      <td>-0.070422</td>\n",
              "      <td>0.057669</td>\n",
              "      <td>-0.077065</td>\n",
              "      <td>0.127696</td>\n",
              "      <td>0.069941</td>\n",
              "      <td>-0.128421</td>\n",
              "      <td>0.116801</td>\n",
              "      <td>0.008347</td>\n",
              "      <td>-0.270478</td>\n",
              "      <td>0.025517</td>\n",
              "      <td>-0.023778</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052727</td>\n",
              "      <td>-0.022286</td>\n",
              "      <td>-0.249808</td>\n",
              "      <td>0.198791</td>\n",
              "      <td>0.071539</td>\n",
              "      <td>-0.102149</td>\n",
              "      <td>-0.114673</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>0.145479</td>\n",
              "      <td>-0.020602</td>\n",
              "      <td>0.020586</td>\n",
              "      <td>0.024660</td>\n",
              "      <td>0.062785</td>\n",
              "      <td>-0.078413</td>\n",
              "      <td>0.022599</td>\n",
              "      <td>-0.071183</td>\n",
              "      <td>0.097420</td>\n",
              "      <td>-0.218218</td>\n",
              "      <td>0.171905</td>\n",
              "      <td>-0.068029</td>\n",
              "      <td>0.027951</td>\n",
              "      <td>-0.018332</td>\n",
              "      <td>-0.041327</td>\n",
              "      <td>-0.034740</td>\n",
              "      <td>0.193908</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>0.078278</td>\n",
              "      <td>0.003362</td>\n",
              "      <td>-0.101686</td>\n",
              "      <td>0.144752</td>\n",
              "      <td>-0.043917</td>\n",
              "      <td>0.072726</td>\n",
              "      <td>0.020087</td>\n",
              "      <td>-0.107518</td>\n",
              "      <td>-0.033213</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.198642</td>\n",
              "      <td>0.030723</td>\n",
              "      <td>0.031848</td>\n",
              "      <td>-0.115006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Brian J.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>Large waiting room and welcoming  staff. Dr. S...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Large waiting room and welcoming staff. Dr. Si...</td>\n",
              "      <td>[Large, waiting, room, and, welcoming, staff, ...</td>\n",
              "      <td>[(Large, JJ), (waiting, VBG), (room, NN), (and...</td>\n",
              "      <td>[Large, wait, room, and, welcoming, staff, ., ...</td>\n",
              "      <td>[large, wait, room, welcoming, staff, singh, p...</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.062678</td>\n",
              "      <td>0.121224</td>\n",
              "      <td>-0.064672</td>\n",
              "      <td>0.122644</td>\n",
              "      <td>0.149081</td>\n",
              "      <td>0.044785</td>\n",
              "      <td>0.561236</td>\n",
              "      <td>-0.000373</td>\n",
              "      <td>-0.027437</td>\n",
              "      <td>-0.029970</td>\n",
              "      <td>-0.239860</td>\n",
              "      <td>-0.050296</td>\n",
              "      <td>-0.605329</td>\n",
              "      <td>-0.065064</td>\n",
              "      <td>0.067156</td>\n",
              "      <td>0.012832</td>\n",
              "      <td>-0.107096</td>\n",
              "      <td>0.018661</td>\n",
              "      <td>0.117309</td>\n",
              "      <td>-0.244392</td>\n",
              "      <td>-0.098588</td>\n",
              "      <td>0.118640</td>\n",
              "      <td>-0.122904</td>\n",
              "      <td>-0.038477</td>\n",
              "      <td>-0.025436</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.021603</td>\n",
              "      <td>-0.121020</td>\n",
              "      <td>-0.105667</td>\n",
              "      <td>-0.133198</td>\n",
              "      <td>-0.174478</td>\n",
              "      <td>-0.092067</td>\n",
              "      <td>-0.228660</td>\n",
              "      <td>0.218980</td>\n",
              "      <td>0.029529</td>\n",
              "      <td>-0.006477</td>\n",
              "      <td>0.115629</td>\n",
              "      <td>-0.184149</td>\n",
              "      <td>0.176944</td>\n",
              "      <td>-0.009503</td>\n",
              "      <td>-0.021329</td>\n",
              "      <td>-0.233324</td>\n",
              "      <td>0.050152</td>\n",
              "      <td>-0.053244</td>\n",
              "      <td>0.060841</td>\n",
              "      <td>-0.029917</td>\n",
              "      <td>0.189309</td>\n",
              "      <td>0.032937</td>\n",
              "      <td>0.063392</td>\n",
              "      <td>0.071988</td>\n",
              "      <td>0.036674</td>\n",
              "      <td>0.056777</td>\n",
              "      <td>-0.022634</td>\n",
              "      <td>-0.003002</td>\n",
              "      <td>0.114672</td>\n",
              "      <td>0.055739</td>\n",
              "      <td>0.028275</td>\n",
              "      <td>0.020430</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>-0.047284</td>\n",
              "      <td>-0.087282</td>\n",
              "      <td>-0.054300</td>\n",
              "      <td>0.089896</td>\n",
              "      <td>0.059271</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>-0.091125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Stephani P.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/2/2015</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>[My, family, and, I, have, been, going, to, Dr...</td>\n",
              "      <td>[(My, PRP$), (family, NN), (and, CC), (I, PRP)...</td>\n",
              "      <td>[My, family, and, I, have, be, go, to, Dr, ., ...</td>\n",
              "      <td>[family, singh, year, actually, follow, anothe...</td>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>1.137931</td>\n",
              "      <td>-0.002764</td>\n",
              "      <td>0.210691</td>\n",
              "      <td>-0.012721</td>\n",
              "      <td>0.133616</td>\n",
              "      <td>-0.019703</td>\n",
              "      <td>0.107070</td>\n",
              "      <td>0.728814</td>\n",
              "      <td>0.057699</td>\n",
              "      <td>-0.056013</td>\n",
              "      <td>-0.025552</td>\n",
              "      <td>-0.162720</td>\n",
              "      <td>-0.121610</td>\n",
              "      <td>-0.634945</td>\n",
              "      <td>-0.198183</td>\n",
              "      <td>0.033477</td>\n",
              "      <td>0.042578</td>\n",
              "      <td>-0.158665</td>\n",
              "      <td>0.165981</td>\n",
              "      <td>0.009809</td>\n",
              "      <td>-0.015610</td>\n",
              "      <td>0.140301</td>\n",
              "      <td>0.086306</td>\n",
              "      <td>-0.160582</td>\n",
              "      <td>0.082321</td>\n",
              "      <td>-0.059185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066712</td>\n",
              "      <td>-0.182554</td>\n",
              "      <td>-0.305282</td>\n",
              "      <td>-0.005345</td>\n",
              "      <td>0.019138</td>\n",
              "      <td>-0.091160</td>\n",
              "      <td>0.022118</td>\n",
              "      <td>0.113561</td>\n",
              "      <td>-0.032497</td>\n",
              "      <td>-0.016087</td>\n",
              "      <td>0.018641</td>\n",
              "      <td>-0.073920</td>\n",
              "      <td>0.097788</td>\n",
              "      <td>-0.071804</td>\n",
              "      <td>-0.032864</td>\n",
              "      <td>-0.197328</td>\n",
              "      <td>0.012225</td>\n",
              "      <td>-0.188776</td>\n",
              "      <td>0.144087</td>\n",
              "      <td>-0.108260</td>\n",
              "      <td>0.059655</td>\n",
              "      <td>-0.114022</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>0.046420</td>\n",
              "      <td>0.147844</td>\n",
              "      <td>-0.030070</td>\n",
              "      <td>0.051887</td>\n",
              "      <td>0.145162</td>\n",
              "      <td>0.049020</td>\n",
              "      <td>0.114916</td>\n",
              "      <td>-0.115151</td>\n",
              "      <td>0.037986</td>\n",
              "      <td>-0.017044</td>\n",
              "      <td>-0.032323</td>\n",
              "      <td>-0.075915</td>\n",
              "      <td>-0.068870</td>\n",
              "      <td>0.005740</td>\n",
              "      <td>-0.012073</td>\n",
              "      <td>-0.006631</td>\n",
              "      <td>-0.067276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52686</th>\n",
              "      <td>52686</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>10/21/2015</td>\n",
              "      <td>I live in LA but was on a trip in Palm Springs...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I live in LA but was on a trip in Palm Springs...</td>\n",
              "      <td>[I, live, in, LA, but, was, on, a, trip, in, P...</td>\n",
              "      <td>[(I, PRP), (live, VBP), (in, IN), (LA, NNP), (...</td>\n",
              "      <td>[I, live, in, LA, but, be, on, a, trip, in, Pa...</td>\n",
              "      <td>[live, trip, palm, springs, urgent, care, cold...</td>\n",
              "      <td>52686</td>\n",
              "      <td>144</td>\n",
              "      <td>1.484536</td>\n",
              "      <td>0.023729</td>\n",
              "      <td>0.138514</td>\n",
              "      <td>-0.074463</td>\n",
              "      <td>-0.062699</td>\n",
              "      <td>0.034909</td>\n",
              "      <td>0.047998</td>\n",
              "      <td>0.712380</td>\n",
              "      <td>-0.012606</td>\n",
              "      <td>0.127002</td>\n",
              "      <td>0.056427</td>\n",
              "      <td>-0.027318</td>\n",
              "      <td>-0.015074</td>\n",
              "      <td>-0.596675</td>\n",
              "      <td>-0.088595</td>\n",
              "      <td>0.034287</td>\n",
              "      <td>-0.066228</td>\n",
              "      <td>-0.135914</td>\n",
              "      <td>0.060232</td>\n",
              "      <td>-0.084892</td>\n",
              "      <td>-0.130465</td>\n",
              "      <td>0.007860</td>\n",
              "      <td>-0.080887</td>\n",
              "      <td>-0.106875</td>\n",
              "      <td>-0.054794</td>\n",
              "      <td>-0.080162</td>\n",
              "      <td>...</td>\n",
              "      <td>0.069530</td>\n",
              "      <td>-0.147700</td>\n",
              "      <td>-0.032083</td>\n",
              "      <td>0.039009</td>\n",
              "      <td>0.102948</td>\n",
              "      <td>-0.100405</td>\n",
              "      <td>-0.010568</td>\n",
              "      <td>-0.031198</td>\n",
              "      <td>-0.004697</td>\n",
              "      <td>-0.030350</td>\n",
              "      <td>-0.070669</td>\n",
              "      <td>0.010229</td>\n",
              "      <td>-0.054536</td>\n",
              "      <td>0.007701</td>\n",
              "      <td>-0.165343</td>\n",
              "      <td>-0.145016</td>\n",
              "      <td>-0.043866</td>\n",
              "      <td>-0.120956</td>\n",
              "      <td>0.133217</td>\n",
              "      <td>-0.009052</td>\n",
              "      <td>-0.012586</td>\n",
              "      <td>0.057069</td>\n",
              "      <td>0.080517</td>\n",
              "      <td>0.027150</td>\n",
              "      <td>0.086458</td>\n",
              "      <td>0.102036</td>\n",
              "      <td>0.118104</td>\n",
              "      <td>0.008524</td>\n",
              "      <td>0.033872</td>\n",
              "      <td>0.065347</td>\n",
              "      <td>-0.013843</td>\n",
              "      <td>-0.013543</td>\n",
              "      <td>-0.002664</td>\n",
              "      <td>-0.174586</td>\n",
              "      <td>0.082061</td>\n",
              "      <td>-0.153287</td>\n",
              "      <td>0.037887</td>\n",
              "      <td>0.111247</td>\n",
              "      <td>0.023115</td>\n",
              "      <td>0.024175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52687</th>\n",
              "      <td>52687</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>9/9/2015</td>\n",
              "      <td>These people are crooks!!!So to get started th...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>These people are crooks!!!So to get started th...</td>\n",
              "      <td>[These, people, are, crooks, !!!, So, to, get,...</td>\n",
              "      <td>[(These, DT), (people, NNS), (are, VBP), (croo...</td>\n",
              "      <td>[These, people, be, crooks, !!!, So, to, get, ...</td>\n",
              "      <td>[people, crooks, !!!, get, start, charge, copa...</td>\n",
              "      <td>52687</td>\n",
              "      <td>48</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>0.041777</td>\n",
              "      <td>0.125878</td>\n",
              "      <td>-0.133935</td>\n",
              "      <td>-0.114618</td>\n",
              "      <td>0.018108</td>\n",
              "      <td>0.022393</td>\n",
              "      <td>0.618870</td>\n",
              "      <td>0.011319</td>\n",
              "      <td>0.106012</td>\n",
              "      <td>0.067943</td>\n",
              "      <td>0.016461</td>\n",
              "      <td>0.059102</td>\n",
              "      <td>-0.672835</td>\n",
              "      <td>-0.143455</td>\n",
              "      <td>0.046456</td>\n",
              "      <td>-0.111785</td>\n",
              "      <td>-0.117134</td>\n",
              "      <td>0.038612</td>\n",
              "      <td>-0.049746</td>\n",
              "      <td>-0.245287</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>-0.112108</td>\n",
              "      <td>-0.096140</td>\n",
              "      <td>0.058422</td>\n",
              "      <td>-0.141347</td>\n",
              "      <td>...</td>\n",
              "      <td>0.057500</td>\n",
              "      <td>-0.073968</td>\n",
              "      <td>0.088515</td>\n",
              "      <td>-0.026553</td>\n",
              "      <td>0.011277</td>\n",
              "      <td>-0.040911</td>\n",
              "      <td>-0.021939</td>\n",
              "      <td>-0.012726</td>\n",
              "      <td>-0.030984</td>\n",
              "      <td>0.074820</td>\n",
              "      <td>-0.087374</td>\n",
              "      <td>0.025143</td>\n",
              "      <td>0.026019</td>\n",
              "      <td>0.023392</td>\n",
              "      <td>-0.132444</td>\n",
              "      <td>-0.157595</td>\n",
              "      <td>0.019487</td>\n",
              "      <td>-0.123140</td>\n",
              "      <td>0.120859</td>\n",
              "      <td>-0.014530</td>\n",
              "      <td>-0.001739</td>\n",
              "      <td>0.081947</td>\n",
              "      <td>0.115640</td>\n",
              "      <td>0.059875</td>\n",
              "      <td>-0.010019</td>\n",
              "      <td>0.051305</td>\n",
              "      <td>0.138860</td>\n",
              "      <td>-0.131707</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>0.047830</td>\n",
              "      <td>-0.044521</td>\n",
              "      <td>0.052964</td>\n",
              "      <td>0.102050</td>\n",
              "      <td>-0.143703</td>\n",
              "      <td>0.028437</td>\n",
              "      <td>-0.046058</td>\n",
              "      <td>-0.032246</td>\n",
              "      <td>0.012114</td>\n",
              "      <td>0.038613</td>\n",
              "      <td>0.005946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52688</th>\n",
              "      <td>52688</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>11/13/2014</td>\n",
              "      <td>Sadly, I think the previous comments and revie...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>Sadly, I think the previous comments and revie...</td>\n",
              "      <td>[Sadly, ,, I, think, the, previous, comments, ...</td>\n",
              "      <td>[(Sadly, RB), (,, ,), (I, PRP), (think, VBP), ...</td>\n",
              "      <td>[Sadly, ,, I, think, the, previous, comment, a...</td>\n",
              "      <td>[sadly, think, previous, comment, review, true...</td>\n",
              "      <td>52688</td>\n",
              "      <td>33</td>\n",
              "      <td>1.064516</td>\n",
              "      <td>0.041730</td>\n",
              "      <td>0.193020</td>\n",
              "      <td>-0.120608</td>\n",
              "      <td>-0.000751</td>\n",
              "      <td>0.019138</td>\n",
              "      <td>0.072478</td>\n",
              "      <td>0.668670</td>\n",
              "      <td>0.077718</td>\n",
              "      <td>-0.091382</td>\n",
              "      <td>0.110822</td>\n",
              "      <td>0.051939</td>\n",
              "      <td>-0.065672</td>\n",
              "      <td>-0.643903</td>\n",
              "      <td>-0.075540</td>\n",
              "      <td>-0.076924</td>\n",
              "      <td>0.037176</td>\n",
              "      <td>0.052543</td>\n",
              "      <td>0.113376</td>\n",
              "      <td>-0.068476</td>\n",
              "      <td>0.005174</td>\n",
              "      <td>0.061591</td>\n",
              "      <td>-0.005886</td>\n",
              "      <td>-0.046418</td>\n",
              "      <td>-0.060214</td>\n",
              "      <td>-0.003904</td>\n",
              "      <td>...</td>\n",
              "      <td>0.095151</td>\n",
              "      <td>-0.145236</td>\n",
              "      <td>-0.025406</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>0.041487</td>\n",
              "      <td>0.083200</td>\n",
              "      <td>0.028628</td>\n",
              "      <td>-0.025669</td>\n",
              "      <td>-0.097289</td>\n",
              "      <td>-0.032053</td>\n",
              "      <td>0.096781</td>\n",
              "      <td>0.107051</td>\n",
              "      <td>0.011641</td>\n",
              "      <td>0.009951</td>\n",
              "      <td>-0.007506</td>\n",
              "      <td>-0.078128</td>\n",
              "      <td>0.007042</td>\n",
              "      <td>-0.219322</td>\n",
              "      <td>0.042379</td>\n",
              "      <td>-0.010758</td>\n",
              "      <td>-0.055176</td>\n",
              "      <td>0.060126</td>\n",
              "      <td>-0.015249</td>\n",
              "      <td>0.026437</td>\n",
              "      <td>-0.028388</td>\n",
              "      <td>-0.007482</td>\n",
              "      <td>0.053510</td>\n",
              "      <td>-0.054477</td>\n",
              "      <td>0.031958</td>\n",
              "      <td>0.021976</td>\n",
              "      <td>0.039913</td>\n",
              "      <td>-0.072352</td>\n",
              "      <td>0.120617</td>\n",
              "      <td>0.055196</td>\n",
              "      <td>0.007354</td>\n",
              "      <td>-0.100615</td>\n",
              "      <td>-0.015888</td>\n",
              "      <td>0.194254</td>\n",
              "      <td>0.078377</td>\n",
              "      <td>-0.069170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52689</th>\n",
              "      <td>52689</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>7/23/2014</td>\n",
              "      <td>I've been waiting for just over an hour now an...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I have been waiting for just over an hour now ...</td>\n",
              "      <td>[I, have, been, waiting, for, just, over, an, ...</td>\n",
              "      <td>[(I, PRP), (have, VBP), (been, VBN), (waiting,...</td>\n",
              "      <td>[I, have, be, wait, for, just, over, an, hour,...</td>\n",
              "      <td>[wait, hour, call, back, two, people, since, f...</td>\n",
              "      <td>52689</td>\n",
              "      <td>56</td>\n",
              "      <td>1.217391</td>\n",
              "      <td>0.094579</td>\n",
              "      <td>0.127270</td>\n",
              "      <td>-0.092072</td>\n",
              "      <td>0.034931</td>\n",
              "      <td>-0.079529</td>\n",
              "      <td>0.103664</td>\n",
              "      <td>0.777946</td>\n",
              "      <td>-0.052366</td>\n",
              "      <td>0.002138</td>\n",
              "      <td>0.122032</td>\n",
              "      <td>-0.013814</td>\n",
              "      <td>-0.010283</td>\n",
              "      <td>-0.647982</td>\n",
              "      <td>-0.076123</td>\n",
              "      <td>-0.010960</td>\n",
              "      <td>-0.121900</td>\n",
              "      <td>-0.066994</td>\n",
              "      <td>0.077313</td>\n",
              "      <td>-0.082001</td>\n",
              "      <td>-0.133710</td>\n",
              "      <td>0.122970</td>\n",
              "      <td>-0.040677</td>\n",
              "      <td>-0.018423</td>\n",
              "      <td>-0.104634</td>\n",
              "      <td>-0.110420</td>\n",
              "      <td>...</td>\n",
              "      <td>0.035732</td>\n",
              "      <td>-0.096487</td>\n",
              "      <td>-0.057043</td>\n",
              "      <td>0.078847</td>\n",
              "      <td>0.038207</td>\n",
              "      <td>0.000482</td>\n",
              "      <td>-0.043672</td>\n",
              "      <td>0.036418</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0.022237</td>\n",
              "      <td>-0.123684</td>\n",
              "      <td>0.047744</td>\n",
              "      <td>-0.001815</td>\n",
              "      <td>-0.019893</td>\n",
              "      <td>-0.091787</td>\n",
              "      <td>-0.120571</td>\n",
              "      <td>-0.071389</td>\n",
              "      <td>-0.160508</td>\n",
              "      <td>0.068817</td>\n",
              "      <td>0.106090</td>\n",
              "      <td>0.005153</td>\n",
              "      <td>0.091534</td>\n",
              "      <td>0.002748</td>\n",
              "      <td>-0.037344</td>\n",
              "      <td>-0.026253</td>\n",
              "      <td>0.106332</td>\n",
              "      <td>0.158728</td>\n",
              "      <td>0.019205</td>\n",
              "      <td>0.090897</td>\n",
              "      <td>0.011066</td>\n",
              "      <td>0.016467</td>\n",
              "      <td>-0.000180</td>\n",
              "      <td>0.020191</td>\n",
              "      <td>-0.116421</td>\n",
              "      <td>-0.035089</td>\n",
              "      <td>-0.173575</td>\n",
              "      <td>0.005820</td>\n",
              "      <td>0.149688</td>\n",
              "      <td>0.065580</td>\n",
              "      <td>0.044845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52690</th>\n",
              "      <td>52690</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>12/6/2011</td>\n",
              "      <td>I went here for a broken foot, because they ad...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I went here for a broken foot, because they ad...</td>\n",
              "      <td>[I, went, here, for, a, broken, foot, ,, becau...</td>\n",
              "      <td>[(I, PRP), (went, VBD), (here, RB), (for, IN),...</td>\n",
              "      <td>[I, go, here, for, a, broken, foot, ,, because...</td>\n",
              "      <td>[broken, foot, advertise, state, art, digital,...</td>\n",
              "      <td>52690</td>\n",
              "      <td>91</td>\n",
              "      <td>1.229730</td>\n",
              "      <td>-0.037366</td>\n",
              "      <td>0.139304</td>\n",
              "      <td>-0.103502</td>\n",
              "      <td>0.079925</td>\n",
              "      <td>-0.003509</td>\n",
              "      <td>-0.087001</td>\n",
              "      <td>0.484827</td>\n",
              "      <td>0.023435</td>\n",
              "      <td>0.033458</td>\n",
              "      <td>-0.047293</td>\n",
              "      <td>0.124581</td>\n",
              "      <td>-0.037843</td>\n",
              "      <td>-0.595495</td>\n",
              "      <td>-0.101596</td>\n",
              "      <td>0.044016</td>\n",
              "      <td>-0.032648</td>\n",
              "      <td>0.025410</td>\n",
              "      <td>0.030127</td>\n",
              "      <td>0.009939</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.041510</td>\n",
              "      <td>0.012977</td>\n",
              "      <td>0.046908</td>\n",
              "      <td>-0.016794</td>\n",
              "      <td>-0.109465</td>\n",
              "      <td>...</td>\n",
              "      <td>0.119328</td>\n",
              "      <td>-0.017102</td>\n",
              "      <td>0.029120</td>\n",
              "      <td>-0.004939</td>\n",
              "      <td>0.078739</td>\n",
              "      <td>0.022344</td>\n",
              "      <td>-0.092942</td>\n",
              "      <td>0.059865</td>\n",
              "      <td>-0.027833</td>\n",
              "      <td>-0.030380</td>\n",
              "      <td>0.009022</td>\n",
              "      <td>0.082397</td>\n",
              "      <td>0.080859</td>\n",
              "      <td>-0.051215</td>\n",
              "      <td>-0.165144</td>\n",
              "      <td>-0.045177</td>\n",
              "      <td>-0.029091</td>\n",
              "      <td>-0.235319</td>\n",
              "      <td>0.022513</td>\n",
              "      <td>-0.215592</td>\n",
              "      <td>0.050035</td>\n",
              "      <td>0.036722</td>\n",
              "      <td>0.116860</td>\n",
              "      <td>-0.075418</td>\n",
              "      <td>0.007744</td>\n",
              "      <td>0.060474</td>\n",
              "      <td>0.048681</td>\n",
              "      <td>-0.107399</td>\n",
              "      <td>0.135261</td>\n",
              "      <td>0.168725</td>\n",
              "      <td>-0.088098</td>\n",
              "      <td>0.102053</td>\n",
              "      <td>0.056022</td>\n",
              "      <td>-0.025295</td>\n",
              "      <td>0.095138</td>\n",
              "      <td>-0.108198</td>\n",
              "      <td>0.254071</td>\n",
              "      <td>0.195052</td>\n",
              "      <td>0.008436</td>\n",
              "      <td>-0.144907</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52691 rows × 215 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  doctorID     username  ...    AWE198    AWE199    AWE200\n",
              "0               0         0   Whitney W.  ... -0.054575  0.023107 -0.063070\n",
              "1               1         0   Kristin R.  ...  0.033650  0.059103  0.035592\n",
              "2               2         0   Allyson F.  ...  0.030723  0.031848 -0.115006\n",
              "3               3         0     Brian J.  ...  0.059271  0.000523 -0.091125\n",
              "4               4         0  Stephani P.  ... -0.012073 -0.006631 -0.067276\n",
              "...           ...       ...          ...  ...       ...       ...       ...\n",
              "52686       52686      2606          NaN  ...  0.111247  0.023115  0.024175\n",
              "52687       52687      2606          NaN  ...  0.012114  0.038613  0.005946\n",
              "52688       52688      2606          NaN  ...  0.194254  0.078377 -0.069170\n",
              "52689       52689      2606          NaN  ...  0.149688  0.065580  0.044845\n",
              "52690       52690      2606          NaN  ...  0.195052  0.008436 -0.144907\n",
              "\n",
              "[52691 rows x 215 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMKSBmskd5L2"
      },
      "source": [
        "df_glove_agg['Length'] = agg_cols(df_w_glove, 'Length')\n",
        "df_glove_agg['Lexical Diversity'] = agg_cols(df_w_glove, 'Lexical Diversity')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "UhxfKRWflA8J",
        "outputId": "2f1b676a-9eb6-4e11-8ed2-4c43d894370d"
      },
      "source": [
        "df_glove_agg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rating</th>\n",
              "      <th>AWE1</th>\n",
              "      <th>AWE2</th>\n",
              "      <th>AWE3</th>\n",
              "      <th>AWE4</th>\n",
              "      <th>AWE5</th>\n",
              "      <th>AWE6</th>\n",
              "      <th>AWE7</th>\n",
              "      <th>AWE8</th>\n",
              "      <th>AWE9</th>\n",
              "      <th>AWE10</th>\n",
              "      <th>AWE11</th>\n",
              "      <th>AWE12</th>\n",
              "      <th>AWE13</th>\n",
              "      <th>AWE14</th>\n",
              "      <th>AWE15</th>\n",
              "      <th>AWE16</th>\n",
              "      <th>AWE17</th>\n",
              "      <th>AWE18</th>\n",
              "      <th>AWE19</th>\n",
              "      <th>AWE20</th>\n",
              "      <th>AWE21</th>\n",
              "      <th>AWE22</th>\n",
              "      <th>AWE23</th>\n",
              "      <th>AWE24</th>\n",
              "      <th>AWE25</th>\n",
              "      <th>AWE26</th>\n",
              "      <th>AWE27</th>\n",
              "      <th>AWE28</th>\n",
              "      <th>AWE29</th>\n",
              "      <th>AWE30</th>\n",
              "      <th>AWE31</th>\n",
              "      <th>AWE32</th>\n",
              "      <th>AWE33</th>\n",
              "      <th>AWE34</th>\n",
              "      <th>AWE35</th>\n",
              "      <th>AWE36</th>\n",
              "      <th>AWE37</th>\n",
              "      <th>AWE38</th>\n",
              "      <th>...</th>\n",
              "      <th>AWE163</th>\n",
              "      <th>AWE164</th>\n",
              "      <th>AWE165</th>\n",
              "      <th>AWE166</th>\n",
              "      <th>AWE167</th>\n",
              "      <th>AWE168</th>\n",
              "      <th>AWE169</th>\n",
              "      <th>AWE170</th>\n",
              "      <th>AWE171</th>\n",
              "      <th>AWE172</th>\n",
              "      <th>AWE173</th>\n",
              "      <th>AWE174</th>\n",
              "      <th>AWE175</th>\n",
              "      <th>AWE176</th>\n",
              "      <th>AWE177</th>\n",
              "      <th>AWE178</th>\n",
              "      <th>AWE179</th>\n",
              "      <th>AWE180</th>\n",
              "      <th>AWE181</th>\n",
              "      <th>AWE182</th>\n",
              "      <th>AWE183</th>\n",
              "      <th>AWE184</th>\n",
              "      <th>AWE185</th>\n",
              "      <th>AWE186</th>\n",
              "      <th>AWE187</th>\n",
              "      <th>AWE188</th>\n",
              "      <th>AWE189</th>\n",
              "      <th>AWE190</th>\n",
              "      <th>AWE191</th>\n",
              "      <th>AWE192</th>\n",
              "      <th>AWE193</th>\n",
              "      <th>AWE194</th>\n",
              "      <th>AWE195</th>\n",
              "      <th>AWE196</th>\n",
              "      <th>AWE197</th>\n",
              "      <th>AWE198</th>\n",
              "      <th>AWE199</th>\n",
              "      <th>AWE200</th>\n",
              "      <th>Length</th>\n",
              "      <th>Lexical Diversity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4.450617</td>\n",
              "      <td>-0.004739</td>\n",
              "      <td>0.109580</td>\n",
              "      <td>-0.055467</td>\n",
              "      <td>0.032624</td>\n",
              "      <td>0.037863</td>\n",
              "      <td>0.068353</td>\n",
              "      <td>0.612281</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>-0.014816</td>\n",
              "      <td>-0.041297</td>\n",
              "      <td>-0.101164</td>\n",
              "      <td>-0.085488</td>\n",
              "      <td>-0.601401</td>\n",
              "      <td>-0.117590</td>\n",
              "      <td>0.053162</td>\n",
              "      <td>0.003640</td>\n",
              "      <td>-0.067281</td>\n",
              "      <td>0.082147</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>-0.107096</td>\n",
              "      <td>0.044002</td>\n",
              "      <td>0.049359</td>\n",
              "      <td>-0.116865</td>\n",
              "      <td>0.018457</td>\n",
              "      <td>-0.112021</td>\n",
              "      <td>0.879065</td>\n",
              "      <td>0.039489</td>\n",
              "      <td>0.179910</td>\n",
              "      <td>0.006141</td>\n",
              "      <td>-0.072227</td>\n",
              "      <td>0.076318</td>\n",
              "      <td>-0.186713</td>\n",
              "      <td>-0.030613</td>\n",
              "      <td>0.016760</td>\n",
              "      <td>-0.072900</td>\n",
              "      <td>0.061665</td>\n",
              "      <td>0.051384</td>\n",
              "      <td>0.016398</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071170</td>\n",
              "      <td>0.031078</td>\n",
              "      <td>0.008759</td>\n",
              "      <td>-0.084882</td>\n",
              "      <td>-0.113550</td>\n",
              "      <td>0.069989</td>\n",
              "      <td>0.046964</td>\n",
              "      <td>-0.036594</td>\n",
              "      <td>0.045713</td>\n",
              "      <td>-0.038768</td>\n",
              "      <td>0.079776</td>\n",
              "      <td>-0.029784</td>\n",
              "      <td>-0.094229</td>\n",
              "      <td>-0.133694</td>\n",
              "      <td>-0.020224</td>\n",
              "      <td>-0.172105</td>\n",
              "      <td>0.097750</td>\n",
              "      <td>-0.083468</td>\n",
              "      <td>0.055543</td>\n",
              "      <td>0.024740</td>\n",
              "      <td>0.052234</td>\n",
              "      <td>-0.016325</td>\n",
              "      <td>0.072179</td>\n",
              "      <td>0.100411</td>\n",
              "      <td>0.040954</td>\n",
              "      <td>-0.030364</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.094056</td>\n",
              "      <td>-0.054656</td>\n",
              "      <td>0.015817</td>\n",
              "      <td>0.100769</td>\n",
              "      <td>-0.119765</td>\n",
              "      <td>0.007846</td>\n",
              "      <td>-0.067108</td>\n",
              "      <td>0.077146</td>\n",
              "      <td>0.048946</td>\n",
              "      <td>0.017984</td>\n",
              "      <td>-0.055928</td>\n",
              "      <td>43.753086</td>\n",
              "      <td>1.105869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.810811</td>\n",
              "      <td>0.017838</td>\n",
              "      <td>0.090133</td>\n",
              "      <td>-0.060930</td>\n",
              "      <td>0.026661</td>\n",
              "      <td>0.045295</td>\n",
              "      <td>0.027326</td>\n",
              "      <td>0.603496</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>0.009490</td>\n",
              "      <td>-0.014742</td>\n",
              "      <td>-0.067378</td>\n",
              "      <td>-0.084041</td>\n",
              "      <td>-0.580594</td>\n",
              "      <td>-0.093487</td>\n",
              "      <td>0.070746</td>\n",
              "      <td>-0.043194</td>\n",
              "      <td>-0.051184</td>\n",
              "      <td>0.096345</td>\n",
              "      <td>-0.026170</td>\n",
              "      <td>-0.138979</td>\n",
              "      <td>0.037236</td>\n",
              "      <td>0.048987</td>\n",
              "      <td>-0.073811</td>\n",
              "      <td>-0.022814</td>\n",
              "      <td>-0.104906</td>\n",
              "      <td>0.885185</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>0.171312</td>\n",
              "      <td>0.001509</td>\n",
              "      <td>-0.073263</td>\n",
              "      <td>0.063365</td>\n",
              "      <td>-0.157461</td>\n",
              "      <td>-0.018407</td>\n",
              "      <td>0.014403</td>\n",
              "      <td>-0.072707</td>\n",
              "      <td>0.078101</td>\n",
              "      <td>0.015396</td>\n",
              "      <td>0.041139</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074104</td>\n",
              "      <td>0.011204</td>\n",
              "      <td>0.055701</td>\n",
              "      <td>-0.092004</td>\n",
              "      <td>-0.145391</td>\n",
              "      <td>0.040438</td>\n",
              "      <td>0.061408</td>\n",
              "      <td>-0.012917</td>\n",
              "      <td>0.034453</td>\n",
              "      <td>-0.046430</td>\n",
              "      <td>0.042516</td>\n",
              "      <td>-0.029370</td>\n",
              "      <td>-0.103526</td>\n",
              "      <td>-0.126716</td>\n",
              "      <td>-0.010893</td>\n",
              "      <td>-0.185730</td>\n",
              "      <td>0.065239</td>\n",
              "      <td>-0.097023</td>\n",
              "      <td>0.022581</td>\n",
              "      <td>0.024610</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.025163</td>\n",
              "      <td>0.052349</td>\n",
              "      <td>0.119964</td>\n",
              "      <td>0.056785</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.060572</td>\n",
              "      <td>0.080504</td>\n",
              "      <td>-0.033145</td>\n",
              "      <td>0.028659</td>\n",
              "      <td>0.078784</td>\n",
              "      <td>-0.104174</td>\n",
              "      <td>0.013709</td>\n",
              "      <td>-0.071503</td>\n",
              "      <td>0.064676</td>\n",
              "      <td>0.080182</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.058888</td>\n",
              "      <td>48.432432</td>\n",
              "      <td>1.105801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4.904762</td>\n",
              "      <td>-0.025036</td>\n",
              "      <td>0.057492</td>\n",
              "      <td>-0.032071</td>\n",
              "      <td>0.002374</td>\n",
              "      <td>-0.020498</td>\n",
              "      <td>0.049577</td>\n",
              "      <td>0.595520</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>0.010198</td>\n",
              "      <td>-0.019621</td>\n",
              "      <td>-0.100948</td>\n",
              "      <td>-0.005968</td>\n",
              "      <td>-0.577615</td>\n",
              "      <td>-0.062437</td>\n",
              "      <td>-0.002557</td>\n",
              "      <td>0.098014</td>\n",
              "      <td>-0.010020</td>\n",
              "      <td>0.054560</td>\n",
              "      <td>-0.052749</td>\n",
              "      <td>-0.064203</td>\n",
              "      <td>0.037048</td>\n",
              "      <td>0.028531</td>\n",
              "      <td>-0.062374</td>\n",
              "      <td>0.081435</td>\n",
              "      <td>-0.109872</td>\n",
              "      <td>0.939803</td>\n",
              "      <td>0.014482</td>\n",
              "      <td>0.209217</td>\n",
              "      <td>0.079155</td>\n",
              "      <td>-0.053990</td>\n",
              "      <td>0.061940</td>\n",
              "      <td>-0.113278</td>\n",
              "      <td>-0.063614</td>\n",
              "      <td>0.041954</td>\n",
              "      <td>-0.102660</td>\n",
              "      <td>0.031450</td>\n",
              "      <td>0.066701</td>\n",
              "      <td>0.024994</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051389</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.023759</td>\n",
              "      <td>-0.112784</td>\n",
              "      <td>-0.042657</td>\n",
              "      <td>0.032377</td>\n",
              "      <td>0.002203</td>\n",
              "      <td>0.029569</td>\n",
              "      <td>0.152568</td>\n",
              "      <td>0.015837</td>\n",
              "      <td>0.012060</td>\n",
              "      <td>-0.097442</td>\n",
              "      <td>-0.116197</td>\n",
              "      <td>-0.121550</td>\n",
              "      <td>-0.015339</td>\n",
              "      <td>-0.175676</td>\n",
              "      <td>0.036518</td>\n",
              "      <td>-0.135913</td>\n",
              "      <td>0.009221</td>\n",
              "      <td>0.021252</td>\n",
              "      <td>-0.038877</td>\n",
              "      <td>-0.019115</td>\n",
              "      <td>0.111799</td>\n",
              "      <td>-0.028155</td>\n",
              "      <td>0.045435</td>\n",
              "      <td>0.063098</td>\n",
              "      <td>0.003909</td>\n",
              "      <td>0.066870</td>\n",
              "      <td>-0.106063</td>\n",
              "      <td>0.050678</td>\n",
              "      <td>0.084019</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>0.045542</td>\n",
              "      <td>-0.073244</td>\n",
              "      <td>0.129461</td>\n",
              "      <td>0.045889</td>\n",
              "      <td>0.025196</td>\n",
              "      <td>-0.079230</td>\n",
              "      <td>77.571429</td>\n",
              "      <td>1.171556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2.416667</td>\n",
              "      <td>0.038018</td>\n",
              "      <td>0.105665</td>\n",
              "      <td>-0.064177</td>\n",
              "      <td>0.024043</td>\n",
              "      <td>0.050930</td>\n",
              "      <td>0.014426</td>\n",
              "      <td>0.615250</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>0.020949</td>\n",
              "      <td>-0.056484</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>-0.049697</td>\n",
              "      <td>-0.619148</td>\n",
              "      <td>-0.102900</td>\n",
              "      <td>0.090590</td>\n",
              "      <td>-0.014152</td>\n",
              "      <td>-0.010258</td>\n",
              "      <td>0.075215</td>\n",
              "      <td>-0.044463</td>\n",
              "      <td>-0.141619</td>\n",
              "      <td>0.025299</td>\n",
              "      <td>0.041922</td>\n",
              "      <td>-0.042957</td>\n",
              "      <td>-0.008504</td>\n",
              "      <td>-0.136420</td>\n",
              "      <td>0.915835</td>\n",
              "      <td>0.063691</td>\n",
              "      <td>0.184653</td>\n",
              "      <td>-0.032327</td>\n",
              "      <td>-0.092390</td>\n",
              "      <td>0.063586</td>\n",
              "      <td>-0.156963</td>\n",
              "      <td>-0.029596</td>\n",
              "      <td>-0.011433</td>\n",
              "      <td>-0.048102</td>\n",
              "      <td>0.042451</td>\n",
              "      <td>-0.005934</td>\n",
              "      <td>0.045870</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.054205</td>\n",
              "      <td>0.028498</td>\n",
              "      <td>0.074691</td>\n",
              "      <td>-0.043320</td>\n",
              "      <td>-0.088574</td>\n",
              "      <td>0.034603</td>\n",
              "      <td>0.029922</td>\n",
              "      <td>-0.081509</td>\n",
              "      <td>0.042854</td>\n",
              "      <td>0.011205</td>\n",
              "      <td>0.029060</td>\n",
              "      <td>-0.063012</td>\n",
              "      <td>-0.101559</td>\n",
              "      <td>-0.107197</td>\n",
              "      <td>-0.023220</td>\n",
              "      <td>-0.147769</td>\n",
              "      <td>0.075356</td>\n",
              "      <td>-0.094736</td>\n",
              "      <td>0.024044</td>\n",
              "      <td>0.039865</td>\n",
              "      <td>0.064397</td>\n",
              "      <td>-0.008018</td>\n",
              "      <td>0.072907</td>\n",
              "      <td>0.087641</td>\n",
              "      <td>0.072423</td>\n",
              "      <td>-0.020360</td>\n",
              "      <td>0.055657</td>\n",
              "      <td>0.109679</td>\n",
              "      <td>-0.057609</td>\n",
              "      <td>0.006164</td>\n",
              "      <td>0.044117</td>\n",
              "      <td>-0.111159</td>\n",
              "      <td>0.013871</td>\n",
              "      <td>-0.073210</td>\n",
              "      <td>0.050130</td>\n",
              "      <td>0.073727</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>-0.046022</td>\n",
              "      <td>52.833333</td>\n",
              "      <td>1.160025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.059764</td>\n",
              "      <td>0.082836</td>\n",
              "      <td>-0.142219</td>\n",
              "      <td>0.091863</td>\n",
              "      <td>0.008776</td>\n",
              "      <td>0.022993</td>\n",
              "      <td>0.496680</td>\n",
              "      <td>0.012337</td>\n",
              "      <td>-0.051832</td>\n",
              "      <td>-0.047512</td>\n",
              "      <td>-0.153940</td>\n",
              "      <td>-0.058628</td>\n",
              "      <td>-0.631568</td>\n",
              "      <td>-0.112419</td>\n",
              "      <td>0.047878</td>\n",
              "      <td>-0.067804</td>\n",
              "      <td>-0.005677</td>\n",
              "      <td>0.102753</td>\n",
              "      <td>-0.063472</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>-0.096702</td>\n",
              "      <td>-0.053903</td>\n",
              "      <td>-0.121419</td>\n",
              "      <td>0.699968</td>\n",
              "      <td>0.095285</td>\n",
              "      <td>0.201253</td>\n",
              "      <td>0.020723</td>\n",
              "      <td>-0.041219</td>\n",
              "      <td>0.081264</td>\n",
              "      <td>-0.047501</td>\n",
              "      <td>-0.117670</td>\n",
              "      <td>0.023999</td>\n",
              "      <td>-0.120962</td>\n",
              "      <td>0.101529</td>\n",
              "      <td>0.109891</td>\n",
              "      <td>0.076047</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040244</td>\n",
              "      <td>0.052940</td>\n",
              "      <td>0.053133</td>\n",
              "      <td>-0.053241</td>\n",
              "      <td>-0.086791</td>\n",
              "      <td>0.024374</td>\n",
              "      <td>-0.005235</td>\n",
              "      <td>-0.038448</td>\n",
              "      <td>0.073409</td>\n",
              "      <td>-0.001047</td>\n",
              "      <td>-0.019316</td>\n",
              "      <td>0.021735</td>\n",
              "      <td>-0.098803</td>\n",
              "      <td>0.020090</td>\n",
              "      <td>-0.048372</td>\n",
              "      <td>-0.122186</td>\n",
              "      <td>0.046514</td>\n",
              "      <td>-0.057995</td>\n",
              "      <td>-0.045879</td>\n",
              "      <td>0.009031</td>\n",
              "      <td>0.035915</td>\n",
              "      <td>0.011018</td>\n",
              "      <td>0.094855</td>\n",
              "      <td>0.079106</td>\n",
              "      <td>0.024286</td>\n",
              "      <td>-0.030704</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.043099</td>\n",
              "      <td>-0.000574</td>\n",
              "      <td>0.019067</td>\n",
              "      <td>0.055732</td>\n",
              "      <td>-0.122686</td>\n",
              "      <td>-0.018306</td>\n",
              "      <td>-0.127239</td>\n",
              "      <td>0.117071</td>\n",
              "      <td>0.093071</td>\n",
              "      <td>0.058105</td>\n",
              "      <td>-0.142413</td>\n",
              "      <td>38.500000</td>\n",
              "      <td>1.047309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2490</th>\n",
              "      <td>2602</td>\n",
              "      <td>3.627907</td>\n",
              "      <td>0.003801</td>\n",
              "      <td>0.056678</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>0.027493</td>\n",
              "      <td>0.052407</td>\n",
              "      <td>0.068345</td>\n",
              "      <td>0.578416</td>\n",
              "      <td>-0.019550</td>\n",
              "      <td>-0.020487</td>\n",
              "      <td>-0.001910</td>\n",
              "      <td>-0.043694</td>\n",
              "      <td>-0.046127</td>\n",
              "      <td>-0.616708</td>\n",
              "      <td>-0.103197</td>\n",
              "      <td>0.031103</td>\n",
              "      <td>0.018686</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>0.050676</td>\n",
              "      <td>-0.057047</td>\n",
              "      <td>-0.133830</td>\n",
              "      <td>0.024876</td>\n",
              "      <td>0.004445</td>\n",
              "      <td>-0.072272</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>-0.094578</td>\n",
              "      <td>0.957469</td>\n",
              "      <td>0.067008</td>\n",
              "      <td>0.197532</td>\n",
              "      <td>-0.003215</td>\n",
              "      <td>-0.077643</td>\n",
              "      <td>0.069441</td>\n",
              "      <td>-0.122064</td>\n",
              "      <td>-0.030265</td>\n",
              "      <td>-0.005515</td>\n",
              "      <td>-0.063978</td>\n",
              "      <td>0.054702</td>\n",
              "      <td>0.010844</td>\n",
              "      <td>0.026273</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056610</td>\n",
              "      <td>0.075562</td>\n",
              "      <td>0.064769</td>\n",
              "      <td>-0.059805</td>\n",
              "      <td>-0.117945</td>\n",
              "      <td>-0.012230</td>\n",
              "      <td>0.054759</td>\n",
              "      <td>-0.029953</td>\n",
              "      <td>0.055020</td>\n",
              "      <td>-0.010160</td>\n",
              "      <td>0.011240</td>\n",
              "      <td>-0.028164</td>\n",
              "      <td>-0.062938</td>\n",
              "      <td>-0.116394</td>\n",
              "      <td>-0.005935</td>\n",
              "      <td>-0.158633</td>\n",
              "      <td>0.076857</td>\n",
              "      <td>-0.080711</td>\n",
              "      <td>0.032272</td>\n",
              "      <td>0.044044</td>\n",
              "      <td>0.085706</td>\n",
              "      <td>0.001675</td>\n",
              "      <td>0.058613</td>\n",
              "      <td>0.095436</td>\n",
              "      <td>0.080023</td>\n",
              "      <td>-0.002176</td>\n",
              "      <td>0.087351</td>\n",
              "      <td>0.075854</td>\n",
              "      <td>-0.071634</td>\n",
              "      <td>0.016003</td>\n",
              "      <td>0.080881</td>\n",
              "      <td>-0.121475</td>\n",
              "      <td>0.041629</td>\n",
              "      <td>-0.096464</td>\n",
              "      <td>0.050183</td>\n",
              "      <td>0.050052</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>-0.044870</td>\n",
              "      <td>45.174419</td>\n",
              "      <td>1.091651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2491</th>\n",
              "      <td>2603</td>\n",
              "      <td>3.568182</td>\n",
              "      <td>0.008686</td>\n",
              "      <td>0.064240</td>\n",
              "      <td>-0.112123</td>\n",
              "      <td>0.066053</td>\n",
              "      <td>0.056164</td>\n",
              "      <td>0.060306</td>\n",
              "      <td>0.577812</td>\n",
              "      <td>-0.016179</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.021948</td>\n",
              "      <td>-0.024541</td>\n",
              "      <td>-0.066753</td>\n",
              "      <td>-0.615053</td>\n",
              "      <td>-0.100780</td>\n",
              "      <td>0.040845</td>\n",
              "      <td>0.020933</td>\n",
              "      <td>-0.057925</td>\n",
              "      <td>0.070935</td>\n",
              "      <td>-0.026832</td>\n",
              "      <td>-0.139642</td>\n",
              "      <td>0.042404</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>-0.058934</td>\n",
              "      <td>-0.007043</td>\n",
              "      <td>-0.099441</td>\n",
              "      <td>0.960317</td>\n",
              "      <td>0.111114</td>\n",
              "      <td>0.214441</td>\n",
              "      <td>-0.002811</td>\n",
              "      <td>-0.048085</td>\n",
              "      <td>0.058234</td>\n",
              "      <td>-0.147535</td>\n",
              "      <td>-0.019297</td>\n",
              "      <td>0.013371</td>\n",
              "      <td>-0.053802</td>\n",
              "      <td>0.068896</td>\n",
              "      <td>-0.002958</td>\n",
              "      <td>0.072276</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.065796</td>\n",
              "      <td>0.064095</td>\n",
              "      <td>0.058642</td>\n",
              "      <td>-0.062993</td>\n",
              "      <td>-0.134788</td>\n",
              "      <td>-0.014483</td>\n",
              "      <td>0.039297</td>\n",
              "      <td>-0.031786</td>\n",
              "      <td>0.073128</td>\n",
              "      <td>0.006764</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>-0.036176</td>\n",
              "      <td>-0.077153</td>\n",
              "      <td>-0.089316</td>\n",
              "      <td>0.005797</td>\n",
              "      <td>-0.152313</td>\n",
              "      <td>0.087047</td>\n",
              "      <td>-0.106796</td>\n",
              "      <td>0.026582</td>\n",
              "      <td>0.043631</td>\n",
              "      <td>0.072984</td>\n",
              "      <td>-0.001705</td>\n",
              "      <td>0.070524</td>\n",
              "      <td>0.102812</td>\n",
              "      <td>0.101850</td>\n",
              "      <td>0.022718</td>\n",
              "      <td>0.111631</td>\n",
              "      <td>0.079243</td>\n",
              "      <td>-0.074038</td>\n",
              "      <td>0.003718</td>\n",
              "      <td>0.083287</td>\n",
              "      <td>-0.114277</td>\n",
              "      <td>0.015116</td>\n",
              "      <td>-0.095072</td>\n",
              "      <td>0.048425</td>\n",
              "      <td>0.070964</td>\n",
              "      <td>0.012766</td>\n",
              "      <td>-0.067173</td>\n",
              "      <td>53.454545</td>\n",
              "      <td>1.147635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>2604</td>\n",
              "      <td>2.441176</td>\n",
              "      <td>0.017475</td>\n",
              "      <td>0.087743</td>\n",
              "      <td>-0.074101</td>\n",
              "      <td>0.031143</td>\n",
              "      <td>0.014160</td>\n",
              "      <td>0.044622</td>\n",
              "      <td>0.658050</td>\n",
              "      <td>0.023040</td>\n",
              "      <td>-0.023856</td>\n",
              "      <td>-0.000529</td>\n",
              "      <td>0.007301</td>\n",
              "      <td>-0.056878</td>\n",
              "      <td>-0.608873</td>\n",
              "      <td>-0.088152</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>-0.018370</td>\n",
              "      <td>-0.046578</td>\n",
              "      <td>0.075851</td>\n",
              "      <td>-0.059118</td>\n",
              "      <td>-0.114944</td>\n",
              "      <td>0.052351</td>\n",
              "      <td>0.010889</td>\n",
              "      <td>-0.057088</td>\n",
              "      <td>-0.028194</td>\n",
              "      <td>-0.063588</td>\n",
              "      <td>0.935718</td>\n",
              "      <td>0.056559</td>\n",
              "      <td>0.145997</td>\n",
              "      <td>0.016430</td>\n",
              "      <td>-0.077094</td>\n",
              "      <td>0.029316</td>\n",
              "      <td>-0.060867</td>\n",
              "      <td>-0.034266</td>\n",
              "      <td>0.025471</td>\n",
              "      <td>-0.112250</td>\n",
              "      <td>0.096995</td>\n",
              "      <td>-0.013451</td>\n",
              "      <td>0.016222</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042635</td>\n",
              "      <td>0.040778</td>\n",
              "      <td>0.069572</td>\n",
              "      <td>-0.057148</td>\n",
              "      <td>-0.095314</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>0.014888</td>\n",
              "      <td>-0.061426</td>\n",
              "      <td>0.038052</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.005368</td>\n",
              "      <td>-0.045913</td>\n",
              "      <td>-0.098609</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>-0.026113</td>\n",
              "      <td>-0.163115</td>\n",
              "      <td>0.047714</td>\n",
              "      <td>-0.059092</td>\n",
              "      <td>-0.000953</td>\n",
              "      <td>0.066118</td>\n",
              "      <td>0.071290</td>\n",
              "      <td>0.021352</td>\n",
              "      <td>0.038844</td>\n",
              "      <td>0.069540</td>\n",
              "      <td>0.061056</td>\n",
              "      <td>-0.009095</td>\n",
              "      <td>0.086287</td>\n",
              "      <td>0.071161</td>\n",
              "      <td>-0.041962</td>\n",
              "      <td>0.025281</td>\n",
              "      <td>0.079294</td>\n",
              "      <td>-0.082179</td>\n",
              "      <td>0.017093</td>\n",
              "      <td>-0.093254</td>\n",
              "      <td>0.031321</td>\n",
              "      <td>0.093794</td>\n",
              "      <td>0.016454</td>\n",
              "      <td>-0.053786</td>\n",
              "      <td>50.529412</td>\n",
              "      <td>1.124702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>2605</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>0.016577</td>\n",
              "      <td>0.060966</td>\n",
              "      <td>-0.087064</td>\n",
              "      <td>0.046408</td>\n",
              "      <td>0.029821</td>\n",
              "      <td>0.040159</td>\n",
              "      <td>0.627986</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>-0.015949</td>\n",
              "      <td>0.013601</td>\n",
              "      <td>-0.004045</td>\n",
              "      <td>-0.050633</td>\n",
              "      <td>-0.638278</td>\n",
              "      <td>-0.092950</td>\n",
              "      <td>0.059763</td>\n",
              "      <td>-0.007455</td>\n",
              "      <td>-0.027335</td>\n",
              "      <td>0.050195</td>\n",
              "      <td>-0.072600</td>\n",
              "      <td>-0.118941</td>\n",
              "      <td>0.043963</td>\n",
              "      <td>0.028052</td>\n",
              "      <td>-0.020707</td>\n",
              "      <td>0.017013</td>\n",
              "      <td>-0.100314</td>\n",
              "      <td>0.924332</td>\n",
              "      <td>0.066779</td>\n",
              "      <td>0.184031</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>-0.087042</td>\n",
              "      <td>0.082218</td>\n",
              "      <td>-0.084034</td>\n",
              "      <td>-0.053479</td>\n",
              "      <td>0.020944</td>\n",
              "      <td>-0.107375</td>\n",
              "      <td>0.062292</td>\n",
              "      <td>0.004020</td>\n",
              "      <td>0.050163</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040871</td>\n",
              "      <td>0.023813</td>\n",
              "      <td>0.060147</td>\n",
              "      <td>-0.026382</td>\n",
              "      <td>-0.114827</td>\n",
              "      <td>-0.027009</td>\n",
              "      <td>0.027002</td>\n",
              "      <td>-0.049506</td>\n",
              "      <td>0.053767</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.020552</td>\n",
              "      <td>-0.024567</td>\n",
              "      <td>-0.095950</td>\n",
              "      <td>-0.102344</td>\n",
              "      <td>-0.014801</td>\n",
              "      <td>-0.154797</td>\n",
              "      <td>0.043244</td>\n",
              "      <td>-0.085830</td>\n",
              "      <td>0.016672</td>\n",
              "      <td>0.063306</td>\n",
              "      <td>0.065970</td>\n",
              "      <td>-0.007496</td>\n",
              "      <td>0.042143</td>\n",
              "      <td>0.109708</td>\n",
              "      <td>0.071727</td>\n",
              "      <td>-0.017870</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.060924</td>\n",
              "      <td>-0.063255</td>\n",
              "      <td>-0.002328</td>\n",
              "      <td>0.090009</td>\n",
              "      <td>-0.110579</td>\n",
              "      <td>0.055245</td>\n",
              "      <td>-0.057162</td>\n",
              "      <td>0.068616</td>\n",
              "      <td>0.101482</td>\n",
              "      <td>-0.015338</td>\n",
              "      <td>-0.062622</td>\n",
              "      <td>49.250000</td>\n",
              "      <td>1.165649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>2606</td>\n",
              "      <td>1.571429</td>\n",
              "      <td>0.027545</td>\n",
              "      <td>0.102103</td>\n",
              "      <td>-0.087128</td>\n",
              "      <td>0.025585</td>\n",
              "      <td>0.010081</td>\n",
              "      <td>0.062728</td>\n",
              "      <td>0.650946</td>\n",
              "      <td>0.008557</td>\n",
              "      <td>-0.010532</td>\n",
              "      <td>0.013319</td>\n",
              "      <td>-0.033369</td>\n",
              "      <td>-0.055501</td>\n",
              "      <td>-0.609024</td>\n",
              "      <td>-0.084756</td>\n",
              "      <td>0.033530</td>\n",
              "      <td>-0.018587</td>\n",
              "      <td>-0.018317</td>\n",
              "      <td>0.088617</td>\n",
              "      <td>-0.088441</td>\n",
              "      <td>-0.114973</td>\n",
              "      <td>0.047151</td>\n",
              "      <td>0.022768</td>\n",
              "      <td>-0.068239</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>-0.096126</td>\n",
              "      <td>0.889699</td>\n",
              "      <td>0.070293</td>\n",
              "      <td>0.139533</td>\n",
              "      <td>0.015974</td>\n",
              "      <td>-0.090256</td>\n",
              "      <td>0.050364</td>\n",
              "      <td>-0.099643</td>\n",
              "      <td>-0.038802</td>\n",
              "      <td>0.019065</td>\n",
              "      <td>-0.100004</td>\n",
              "      <td>0.070795</td>\n",
              "      <td>0.012292</td>\n",
              "      <td>0.015073</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063595</td>\n",
              "      <td>0.036156</td>\n",
              "      <td>0.075096</td>\n",
              "      <td>-0.039093</td>\n",
              "      <td>-0.078704</td>\n",
              "      <td>0.006561</td>\n",
              "      <td>0.020117</td>\n",
              "      <td>-0.029605</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>-0.004767</td>\n",
              "      <td>0.030869</td>\n",
              "      <td>-0.015064</td>\n",
              "      <td>-0.095884</td>\n",
              "      <td>-0.097213</td>\n",
              "      <td>-0.040490</td>\n",
              "      <td>-0.167914</td>\n",
              "      <td>0.057458</td>\n",
              "      <td>-0.062123</td>\n",
              "      <td>-0.006414</td>\n",
              "      <td>0.057779</td>\n",
              "      <td>0.056900</td>\n",
              "      <td>0.026271</td>\n",
              "      <td>0.029138</td>\n",
              "      <td>0.068665</td>\n",
              "      <td>0.062784</td>\n",
              "      <td>-0.039613</td>\n",
              "      <td>0.094178</td>\n",
              "      <td>0.077215</td>\n",
              "      <td>-0.044480</td>\n",
              "      <td>0.013093</td>\n",
              "      <td>0.072874</td>\n",
              "      <td>-0.092804</td>\n",
              "      <td>0.031574</td>\n",
              "      <td>-0.079673</td>\n",
              "      <td>0.076916</td>\n",
              "      <td>0.104879</td>\n",
              "      <td>0.033944</td>\n",
              "      <td>-0.061523</td>\n",
              "      <td>86.885714</td>\n",
              "      <td>1.219102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2495 rows × 204 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      doctorID    rating      AWE1  ...    AWE200     Length  Lexical Diversity\n",
              "0            0  4.450617 -0.004739  ... -0.055928  43.753086           1.105869\n",
              "1            1  2.810811  0.017838  ... -0.058888  48.432432           1.105801\n",
              "2            2  4.904762 -0.025036  ... -0.079230  77.571429           1.171556\n",
              "3            3  2.416667  0.038018  ... -0.046022  52.833333           1.160025\n",
              "4            4  3.000000  0.059764  ... -0.142413  38.500000           1.047309\n",
              "...        ...       ...       ...  ...       ...        ...                ...\n",
              "2490      2602  3.627907  0.003801  ... -0.044870  45.174419           1.091651\n",
              "2491      2603  3.568182  0.008686  ... -0.067173  53.454545           1.147635\n",
              "2492      2604  2.441176  0.017475  ... -0.053786  50.529412           1.124702\n",
              "2493      2605  1.750000  0.016577  ... -0.062622  49.250000           1.165649\n",
              "2494      2606  1.571429  0.027545  ... -0.061523  86.885714           1.219102\n",
              "\n",
              "[2495 rows x 204 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsizz6mFd-1y",
        "outputId": "ef9ce5f3-e975-4a80-81f8-785eba61b53c"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df_glove_agg.to_pickle('df_glove_agg')\n",
        "df_glove_agg.to_csv('df_glove_agg.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HsDzBeWUgvE"
      },
      "source": [
        "# Task 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlVE8jn6A7zO"
      },
      "source": [
        "##Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ml2FCYx-ycH",
        "outputId": "104014d9-689a-4ff7-92db-5e585ba91d05"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-payw963_wHV",
        "outputId": "42f9fd7d-cea3-41d2-dfaa-4483d8859a97"
      },
      "source": [
        "# read a local file into Pandas DataFrame\n",
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df_sa = pd.read_pickle('prepared_data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HYJGu9_GsIe"
      },
      "source": [
        "###for individual reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "ZCtA2VDbWBD4",
        "outputId": "a1063204-4639-4e8b-ad3a-e81e74b1588f"
      },
      "source": [
        "%time df_sa['polarity'] = df_sa['expanded'].apply(lambda row: TextBlob (row).sentiment[0])\n",
        "\n",
        "df_sa.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 50.5 s, sys: 167 ms, total: 50.6 s\n",
            "Wall time: 50.8 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>final</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>[After, seeing, 4, ., 5, stars, reviews, on, a...</td>\n",
              "      <td>[(After, IN), (seeing, VBG), (4, CD), (., .), ...</td>\n",
              "      <td>[After, see, 4, ., 5, star, review, on, a, fam...</td>\n",
              "      <td>[see, star, review, family, practice, rancho, ...</td>\n",
              "      <td>0.404762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>[I, just, switched, this, month, to, Rancho, W...</td>\n",
              "      <td>[(I, PRP), (just, RB), (switched, VBD), (this,...</td>\n",
              "      <td>[I, just, switch, this, month, to, Rancho, Wel...</td>\n",
              "      <td>[switch, month, rancho, wellness, base, referr...</td>\n",
              "      <td>0.214470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, are, very...</td>\n",
              "      <td>[(I, PRP), (love, VBP), (Rancho, NNP), (wellne...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, be, very,...</td>\n",
              "      <td>[love, rancho, wellness, organize, professiona...</td>\n",
              "      <td>0.361364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Brian J.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>Large waiting room and welcoming  staff. Dr. S...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Large waiting room and welcoming staff. Dr. Si...</td>\n",
              "      <td>[Large, waiting, room, and, welcoming, staff, ...</td>\n",
              "      <td>[(Large, JJ), (waiting, VBG), (room, NN), (and...</td>\n",
              "      <td>[Large, wait, room, and, welcoming, staff, ., ...</td>\n",
              "      <td>[large, wait, room, welcoming, staff, singh, p...</td>\n",
              "      <td>0.278571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Stephani P.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/2/2015</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>[My, family, and, I, have, been, going, to, Dr...</td>\n",
              "      <td>[(My, PRP$), (family, NN), (and, CC), (I, PRP)...</td>\n",
              "      <td>[My, family, and, I, have, be, go, to, Dr, ., ...</td>\n",
              "      <td>[family, singh, year, actually, follow, anothe...</td>\n",
              "      <td>0.337143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...  polarity\n",
              "0           0  ...  0.404762\n",
              "1           1  ...  0.214470\n",
              "2           2  ...  0.361364\n",
              "3           3  ...  0.278571\n",
              "4           4  ...  0.337143\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "tY4ISoSmUVmt",
        "outputId": "df41e3a8-9382-47ee-8264-773defa8e755"
      },
      "source": [
        "df_sa['subjectivity'] = df_sa['expanded'].apply(lambda row: TextBlob (row).sentiment[1])\n",
        "\n",
        "df_sa.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>final</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>[After, seeing, 4, ., 5, stars, reviews, on, a...</td>\n",
              "      <td>[(After, IN), (seeing, VBG), (4, CD), (., .), ...</td>\n",
              "      <td>[After, see, 4, ., 5, star, review, on, a, fam...</td>\n",
              "      <td>[see, star, review, family, practice, rancho, ...</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>0.641667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>[I, just, switched, this, month, to, Rancho, W...</td>\n",
              "      <td>[(I, PRP), (just, RB), (switched, VBD), (this,...</td>\n",
              "      <td>[I, just, switch, this, month, to, Rancho, Wel...</td>\n",
              "      <td>[switch, month, rancho, wellness, base, referr...</td>\n",
              "      <td>0.214470</td>\n",
              "      <td>0.467121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, are, very...</td>\n",
              "      <td>[(I, PRP), (love, VBP), (Rancho, NNP), (wellne...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, be, very,...</td>\n",
              "      <td>[love, rancho, wellness, organize, professiona...</td>\n",
              "      <td>0.361364</td>\n",
              "      <td>0.484091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Brian J.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>Large waiting room and welcoming  staff. Dr. S...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Large waiting room and welcoming staff. Dr. Si...</td>\n",
              "      <td>[Large, waiting, room, and, welcoming, staff, ...</td>\n",
              "      <td>[(Large, JJ), (waiting, VBG), (room, NN), (and...</td>\n",
              "      <td>[Large, wait, room, and, welcoming, staff, ., ...</td>\n",
              "      <td>[large, wait, room, welcoming, staff, singh, p...</td>\n",
              "      <td>0.278571</td>\n",
              "      <td>0.407143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Stephani P.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/2/2015</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>[My, family, and, I, have, been, going, to, Dr...</td>\n",
              "      <td>[(My, PRP$), (family, NN), (and, CC), (I, PRP)...</td>\n",
              "      <td>[My, family, and, I, have, be, go, to, Dr, ., ...</td>\n",
              "      <td>[family, singh, year, actually, follow, anothe...</td>\n",
              "      <td>0.337143</td>\n",
              "      <td>0.598571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  doctorID  ...  polarity  subjectivity\n",
              "0           0         0  ...  0.404762      0.641667\n",
              "1           1         0  ...  0.214470      0.467121\n",
              "2           2         0  ...  0.361364      0.484091\n",
              "3           3         0  ...  0.278571      0.407143\n",
              "4           4         0  ...  0.337143      0.598571\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1h5Zwegu2-T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "1d0be993-7605-48cd-8205-072b781d6648"
      },
      "source": [
        "# let's add the compound score to the dataframe\n",
        "%time df_sa['NLTK_Compound'] = df_sa['expanded'].apply(lambda row: sia.polarity_scores(row)['compound'])\n",
        "\n",
        "df_sa.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 19s, sys: 285 ms, total: 1min 19s\n",
            "Wall time: 1min 20s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>final</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>NLTK_Compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>[After, seeing, 4, ., 5, stars, reviews, on, a...</td>\n",
              "      <td>[(After, IN), (seeing, VBG), (4, CD), (., .), ...</td>\n",
              "      <td>[After, see, 4, ., 5, star, review, on, a, fam...</td>\n",
              "      <td>[see, star, review, family, practice, rancho, ...</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.9259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>[I, just, switched, this, month, to, Rancho, W...</td>\n",
              "      <td>[(I, PRP), (just, RB), (switched, VBD), (this,...</td>\n",
              "      <td>[I, just, switch, this, month, to, Rancho, Wel...</td>\n",
              "      <td>[switch, month, rancho, wellness, base, referr...</td>\n",
              "      <td>0.214470</td>\n",
              "      <td>0.467121</td>\n",
              "      <td>0.9240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, are, very...</td>\n",
              "      <td>[(I, PRP), (love, VBP), (Rancho, NNP), (wellne...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, be, very,...</td>\n",
              "      <td>[love, rancho, wellness, organize, professiona...</td>\n",
              "      <td>0.361364</td>\n",
              "      <td>0.484091</td>\n",
              "      <td>0.9712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Brian J.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>Large waiting room and welcoming  staff. Dr. S...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Large waiting room and welcoming staff. Dr. Si...</td>\n",
              "      <td>[Large, waiting, room, and, welcoming, staff, ...</td>\n",
              "      <td>[(Large, JJ), (waiting, VBG), (room, NN), (and...</td>\n",
              "      <td>[Large, wait, room, and, welcoming, staff, ., ...</td>\n",
              "      <td>[large, wait, room, welcoming, staff, singh, p...</td>\n",
              "      <td>0.278571</td>\n",
              "      <td>0.407143</td>\n",
              "      <td>0.9186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Stephani P.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/2/2015</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>[My, family, and, I, have, been, going, to, Dr...</td>\n",
              "      <td>[(My, PRP$), (family, NN), (and, CC), (I, PRP)...</td>\n",
              "      <td>[My, family, and, I, have, be, go, to, Dr, ., ...</td>\n",
              "      <td>[family, singh, year, actually, follow, anothe...</td>\n",
              "      <td>0.337143</td>\n",
              "      <td>0.598571</td>\n",
              "      <td>0.9838</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  doctorID     username  ...  polarity subjectivity NLTK_Compound\n",
              "0           0         0   Whitney W.  ...  0.404762     0.641667        0.9259\n",
              "1           1         0   Kristin R.  ...  0.214470     0.467121        0.9240\n",
              "2           2         0   Allyson F.  ...  0.361364     0.484091        0.9712\n",
              "3           3         0     Brian J.  ...  0.278571     0.407143        0.9186\n",
              "4           4         0  Stephani P.  ...  0.337143     0.598571        0.9838\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "s2Cm6jxAVicZ",
        "outputId": "6f395d99-46c2-47a6-827f-766dd66ad5f6"
      },
      "source": [
        "import matplotlib.pyplot as plot\n",
        "\n",
        "#plotting each individual review's polarity against its rating\n",
        "df_sa.plot.scatter(x='polarity', y='rating', title= \"Polarity vs. Rating\");\n",
        "\n",
        "plot.show(block=True);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "1NVDCCXyVnbX",
        "outputId": "6d82c57a-05b1-44cf-9b54-9a41ec4bf632"
      },
      "source": [
        "#each individual review's subjectivity vs. its rating\n",
        "df_sa.plot.scatter(x='subjectivity', y='rating', c='green', \n",
        "                   title= \"Subjectivity vs. Rating\");\n",
        "\n",
        "plot.show(block=True);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ay5Kx_KjVuk5",
        "outputId": "2eaf523a-feed-4a40-fbb8-c36165ea3f55"
      },
      "source": [
        "#each individual review's compound score vs. its rating\n",
        "df_sa.plot.scatter(x='NLTK_Compound', y='rating', c='purple', \n",
        "                   title= 'NLTK Compound vs. Rating');\n",
        "\n",
        "plot.show(block=True);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYV7yb-SDaar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcf9692-5c3b-4405-fd75-7d59404afa56"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df_sa.to_pickle('SA_individual')\n",
        "df_sa.to_csv('SA_individual.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFPrK2sRGyfW"
      },
      "source": [
        "###by doctor's office"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "UTk84V5Sf2mi",
        "outputId": "d24fdaa2-449e-4d4e-b17a-689041800246"
      },
      "source": [
        "# Create a dataframe that includes the individual cleaned reviews plus attributes for each doctor's office\n",
        "df_sa_by_office = df_sa[['doctorID','rounded_rating','expanded']]\n",
        "\n",
        "df_sa_by_office.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Large waiting room and welcoming staff. Dr. Si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   doctorID  rounded_rating                                           expanded\n",
              "0         0             4.5  After seeing 4.5 stars reviews on a family pra...\n",
              "1         0             4.5  I just switched this month to Rancho Wellness ...\n",
              "2         0             4.5  I love Rancho wellness, they are very organize...\n",
              "3         0             4.5  Large waiting room and welcoming staff. Dr. Si...\n",
              "4         0             4.5  My family and I have been going to Dr. Singh f..."
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "wfMdpUjpocDp",
        "outputId": "245bbe98-ee7d-4cbf-df2b-d145fd4bfecc"
      },
      "source": [
        "# use groupby plus a custom aggregate function to merge all review texts\n",
        "# set as_index=False so that grouping columns are not added to the index.\n",
        "\n",
        "%time df_sa_by_office = df_sa_by_office.groupby(['doctorID','rounded_rating'], as_index=False)['expanded'].apply (lambda x: ' '.join(x))\n",
        "\n",
        "df_sa_by_office.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 148 ms, sys: 5.97 ms, total: 154 ms\n",
            "Wall time: 164 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>I have been going here for several years now. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>The baby center at St Bernardines is awesome. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>I feel the need to post a review about Doctor ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Love Dr. Ali! She is very knowledgeable and is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   doctorID  rounded_rating                                           expanded\n",
              "0         0             4.5  After seeing 4.5 stars reviews on a family pra...\n",
              "1         1             3.0  I have been going here for several years now. ...\n",
              "2         2             5.0  The baby center at St Bernardines is awesome. ...\n",
              "3         3             2.5  I feel the need to post a review about Doctor ...\n",
              "4         4             3.0  Love Dr. Ali! She is very knowledgeable and is..."
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzn48yZ5DIUc",
        "outputId": "833c7feb-de76-4063-8f1c-0b0c6bcc7268"
      },
      "source": [
        "len(df_sa_by_office)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2552"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "6h93y0Rwp4NX",
        "outputId": "e7f16fde-aaa5-4e67-872b-a19feae5c1c4"
      },
      "source": [
        "# check the first aggregrated reviews\n",
        "(df_sa_by_office.loc[0:0,['expanded']]).iloc[0,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'After seeing 4.5 stars reviews on a family practice in Rancho, I called an made a same day appointment with them. I was sent to their upland office and was seen by a super nice PA name Heidi. She did very throughout checkups on me and also explained my conditions to me patiently. I was very happy that I found a nice family practice office near home. I just switched this month to Rancho Wellness based on a referral from my sister in law. I had a previous appointment with Planned Parenthood before getting this insurance and when I went to go get my depo shot, they told me I was no longer qualified for the free birth control and that if I still wanted to get it, it would cost me over $100. I decided to call and see if Rancho Wellness could get me in, since I had a dead line to get the shot. They were able to get me in the same day. They were extremely nice and friendly. It took me longer to fill out the paperwork than it did for them to take me back and see the doctor. Love my new doctor\\'s office and staff. I love Rancho wellness, they are very organized, professional and very knowledgeable ... the staff is friendly and always helpful... Dr. Santa Cruz is by far the best at what she does... my entire family including my mother, father, my step mother, aunt, Husbends and daughter come there.. and many of my friends do to...most of all, I love that they are always on time. I never wait more then 10 minutes to be seen... Large waiting room and welcoming staff. Dr. Singh is professional, knowledgeable and above all caring. The doctor listens and takes his time to provide you with feedback. I am happy to be under Dr. Singh care. My family and I have been going to Dr. Singh for years. We actually followed him from another physician\\'s office because we were always so pleased with the care and attention he gave to us. I highly recommend him and I love his staff. They are always so sweet and helpful. In all the years (10+) I have never had to wait too long and never felt rushed out. Dr. Singh is an incredible caring doctor and we are grateful for his care. I have been going here since Dr. Beseth retired and I just have to say I love Rancho Wellness! They take their time with me and do not rush me out of my appointment. They manage my thyroid problem and I never have to wait too long once I get there to get in to see the Doctor. Dr. Singh has been a God send. My grandmother is 87 and switched to him earlier this year. He has adjusted her medications to where she is functioning at a much better level. This doctor listens to her and is very patient and kind. We are very happy with him. He is now seeing almost every member of our family and provides excellent and personalized care for each person. He even removed my husband\\'s cyst in office for a very nominal price. Their pricing structure for a private pay patient is fair and they offer options for even less expensive care options. In response to Jeremy M.\\'s review: Yes, the doctor\\'s office takes payments for services provided. This is not unique to this office. Medical care is a business and a doctor is not going to work for free when running a practice. If it is an emergency go to the E.R. where they will not turn you away for care due to inability to pay. Or seek a county clinic that provides care on a sliding scale payment. In my opinion $150 is a fair deposit. I have never had any problems with the staff here. In fact they have always been very friendly and efficient. We are usually in quickly and treated with respect and dignity. I am very impressed with this office and especially with Dr. Singh himself. I would highly recommend Rancho Wellness. I have been going here for yrs the staff is very professional and helpful. The doctor I see every couple of months is very informative and caring I never feel rushed. Its a very comfortable environment I appreciate everything Rancho Wellness does for me. Excellent office. I drive all he way from Whittier, to see Barbara love her This is a fantastic office. Dr. Santa Cruz is very compassionate she always answers my questions she is very informational and takes her time in there I have never felt I was rushed out. The staff is very professional and very helpful. Highly recommended. Dr. Singh and his staff provide the best medical care I have ever experienced.Professional, caring, best bedside manners of any doctor I know.Thanks and kudos for having such a wonderful office and staff. I have to say I have the best doctor and medical staff that anyone could possibly ask for.Both of my parents were in the medical field and I was taught at an early age what to look for and expect in a good doctor and I found that with Dr. Singh. He is very knowledgeable and caring. He makes all of his patients feel important and does not make us wait long like you have to in so many other offices.Me and my son love him and are very blessed to have found him over 10 years ago when I went to San Antonio Hospital and asked about the besr doctor they had on staff and another medical staff introduced us.Over these past years his practice has grown so much but he is still the same and has brought in staff as caring as he is. I have not yet established care with this office yet but totally plan to! I called to obtain information after finding out in the most inconvenient way that my Primary Dr has quit. I spoke to Misty. She was VERY friendly! And very helpful! Due to her customer service just over the phone I feel confident that this office could be what I am looking for! My husband and I have been seen in Dr. Singh\\'s office for a few years now and they are an excellent doctor\\'s office. Before coming here we had a very hard time finding a good doctor since losing our other doctor (Jamie Cruiz) when he moved up to Northern Ca. My husband and I are an older couple and my husband has had quite a few major issues and each time they were handled very well. we have been seeing mostly Dr. Singh\\'s PA Jessica and she is very caring and an excellent, but whenever we had seen Dr. Singh he also has been very caring, takes the time to listen to us and takes care of our needs. I can not say enough good things about this office and their staff.I have a prior medical back round and was in medical for over 28 years and I highly recommend this Medical Group. I definitely like the Drs and staff...however, when there is phone problems, there is most likely internet issues as well. Please, and I stress please communicate this to your Drs, for an unhappy client makes for unhappy Drs. If there is internet issues a hand written prescription as well as e-script should be written. Staff was great but Dr. Singh has to be the best Dr. I have ever encountered. Very friendly and down to earth. Very easy person to talk to. The staff are all great, experienced professionals. I recommended Dr. Singh to all my friends and family. I wanted to update my review, I still love this doctors office. Dr. Singh and his team are the most knowledgeable in the area. I hate that I am moving back out of state and cannot continue going here. Wanted to wait a few days prior to giving a review just to make sure his assessment was correct. The doctor was very nice and knowledgeable. My husband booked me an appointment for the wrong doc initially but was happy to transfer over to Dr. Singh. The front office staff was very friendly, so glad to find a great doc in the area. I loved Dr. Singh. I have to decide whether or not to leave my 20 year relationship with my doctor in Los Angeles. I am leaning towards Dr. Singh. I went to see Dr Singh yesterday with terrible asthma and struggling to breathe. Dr Singh is so kind and took time to treat me as well as listen to my concerns. Today, on a Saturday he called me to find out how I was doing. I loved that!!! Any Dr who takes this kind of time to really care about a first time visit has a new patient. Thank you Rancho Wellness, and Dr. Singh, for such good service. I am a patient of Dr. Ravinder Singh ever since he started his private practice. He is excellent and his staff are very kind and courteous. Dr. Singh has a well designed and clean clinic. He is a doctor with no protocol as he has great concern for his patients. He always tells me to walk in, if I need to see him urgently. Many doctors wanted to go to urgent care rather pay attention to their clients. Referrals and prescriptions are done by his staff on top priority. It is great pleasure to assosiate with them. Put in a nutshell \" I am in safe hands\". I am so satisfied with their great service, I always love to carry a bouquet for them from my rose garden. I strongly recommend Dr. Singh to anyone. Dr Singh and the team there are awesome! It was my first time going and they made me feel very comfortable and welcome. Dr Singh was friendly and took the time to answer all my questions and concerns. I would definitely recommend anyone to come here. New to the area and wanted to establish a new physician relationship. Quick appointment availability. Barbara was very thorough. She addressed all of my needs. My husband and adult daughter also have visited the clinic and they love it as well. Highly recommend. Dr Singh is a one of a kind doctor. He is definitely the best doctor I have ever had for me and my family. He truly cares about his patients and takes the necessary time to help them. Office is clean and staff is friendly. His assistant Kara is awesome. Dr. Singh is by far the best doctor I have ever had. He is very attentive and never rushes through his appointments. When my daughter was diagnosed with a tumor he stayed up all night researching what could have caused the problem and trying to find the best facility to get her transferred to. I was lucky and it turned out to be a hospital error but he gave up a lot of personal time to be there for us thru a scary time. He takes time to listen to medical concerns and takes time in making sure that he understand so he can provide an accurate diagnosis and treatment. He is well educated in new medical procedures and treatments. I have needed special requests of medicines and appointments for all of my family members at some point and he has gone above and beyond what any other doctor would do. I could have my family covered at no cost under another medical plan but refuse to give up Dr. Singh as my primary care doctor. Although it is not always possible to get in to see him the other doctors that work with him are just as knowledgeable. Excellent medical care. They listen and work with you. They respond back and get referral\\'s right away. They have enough staff that you can get in with someone right away. The wait time is very reasonable and staff is very friendly and helpful. My family has been with Rancho Wellness for over a year. We love their office, staff & ease of scheduling /communication. Our appointment times are always honored (not stuck in a waiting room for an extra hour). I am so very happy I found Rancho Wellness! My medical facility no longer accepts Aetna insurance, so I had to find a new place. After a couple of months of tears and being super upset (because I have to re-establish myself), I could not be happier. They made me feel so very comfortable. it is a very clean and friendly environment. Thank you all Rancho Wellness is one of the Best Medical Practice Facility under care and Supervision of Dr Ravinder Singh ,MDThough I must have recommended more then 50 Patients, for the last couple years , Now I m 51st , made Dr Ravinder Singh as my primary Physician. Each member of his Team at Rancho Wellness is very very polite , Courteous and caring .My rating is 10/10. This is the friendliest office I have been to in a while. And I do have a regular schedule of doctors and doctors offices that I attend to on a regular basis due to my personal issues so it is not like I see doctors and doctors offices few and far between. it is quite the opposite. On top of them being time efficient they called me back to be seen before I could even finish the new patient packet. that is unheard of . everything that I was looking for was handled. I would genuinely recommend recommend this place on a very high-level to anyone I love Dr. Singh and his staff. Dr. Heidi Larson is wonderful as well as nurse practitioner Barbara Santa Cruz. The staff really cares and you do not ever feel like you are rushed out of their office. I have been coming here for years and recommend everyone to come see Dr. Singh. I know they really care and that is great when you are not feeling well! Thank you so much for all that you do at Rancho Wellness!!! When our insurance changed, I was upset. To have to start over looking for a doctor, because I am so picky, was going to be hard work. I found Rancho Wellness Center here on Yelp. Gave them a try and was impressed. We go to the Rancho office and it is big, clean and are up to date. All of the office staff are friendly. I have the female doctor, Dr. Santa Cruz and my husband and boys have the male doctor, Dr. Singh...how convenient is that, all in one office. We have been patients for about 1yr and are happy. Dr Singh is an amazing provider. He is kind, considerate, thorough, knowledgeable and current. His practice is clean, his staff is helpful, timely appointments are available and wait times are reasonable. I was recommended to his practice site by my family and now I am recommending his practice to my friends. Glad we found him! I have been a patient here for a little over a year now and could not be happier. I have been able to get appointments when I needed them and some day appointments when urgent matters have occurred. I have very little wait time when I do arrive at the office. Their office is very clean and neat and modern which is a reflection of the staff who are pleasant, professional and efficient. I would recommend Rancho Wellness to anyone looking for quality, efficient medical care. Awesome establishment as well as friendly customer service. Dr. Barbara is a true professional and is always on point with her analysis. Great team of peeps!!! I have been going to Dr. Singh for the last 12 years and his medical practice is one of the best around. Easy to get in, short wait times, and extremely competent doctor and staff. Very happy with Rancho wellness! Since my long term family physician had retired, I had had a difficult time finding a doctor that I trusted. Luckily, Dr. Singh was referred to me and I have been happily visiting his practice for the past two years. Dr. Singh is very knowledgeable and a great listener. He offers great advice and takes the time to explain his plan. With his new web sessions, I have been able to still see him despite my hectic schedule and relocating. The sessions are still thorough and very convenient. Although we cannot do everything we would be able to in an in person visit, it is a great way to get my concerns addressed until I have the chance to come in. I would definitely recommend him as a physician because he is competent, reliable, and his practice is always up to date. I been a Dr.Singh patient for 12 years and always get great service and today I show up with no appointment and he was not there but I am glad they were able to help me from Angela in the front desk to Kelsey and off course Barbara santacruz she was amazing for having a busy schedule and me not having an appointment they sure did a great job. Thanks to all the Rancho wellness for your attention. Gerardo Arciga. The staff is friendly and on point. The office keeps to your appointment time. I enjoy seeing Dr. Santa Cruz. She is knowledgeable and empathetic. I definitely would recommend Rancho Wellness to chose as your primary care medical facility. First time I can actually say I enjoyed a doctor\\'s office visit. Dr. Singh came in and talked to me and really listened. I have been to doctors that usually try and rush you out and usually only want to handle one issue at a time. Dr. Singh was open to listening to all of my issues and made sure that I get answers I needed. I was taken in rather quickly, the doctor came in soon after to see me. The visit was great. I highly recommend this place. I really hope this is how it is always going to be when I come here. ::Fingers crossed:: Love love love this Dr\\'s office. They always seem to know what is going on and prescribe the right medication to me. Also it is pretty easy to get an appt the same day. I see all their 5-star Yelps and think oh I want to go there but when calling the office and booking our first appointment I had some questions it is the first time I have used my insurance before I wanted to see what we are covered for how much things are what is my co-payment I do not know are not I allowed to ask those questions the lady direct me to call my insurance not a problem I still want to make sure that what they say and what the office says matches up I asked her to please give me a call the day before to confirm my appointment and also to confirm my co-payment and things like that I receive no phone call yesterday so I called this morning to confirm my husband has an appointment at 3:30 and they say yes and I say please cancel here is the thing if people are not going to do their job the way they are supposed to I am not going to start my doctor visits there I will find a doctor whose nurses and front staff will help me with the questions that I need answered I am really sad that I am not going to go into this place it looks like such a good shop on Yelp for the first time you have misled me if you are one of those people who have lots of questions and want to know what you are paying for before you go in and do it this may not be the place for you I still on hold for 15 minutes just to check if my husband had an appointment today because no one called me to confirm as they said they would do the day before I have been seeing Dr. Singh as my primary care doctor for a number of years now. Dr. Singh is very easy to talk with and always friendly. He is knowledgeable and patient as I am not the easiest person when it comes to medical. I travel a lot for business and often to foreign countries - Dr. Singh is easily accessible and even has a video call appointment service which allows me to have a solid medical discussion and review with him even when traveling. I love the patient portal which allows me to log in and send a confidential message to the Dr. or the Staff. I can also check my medical records and schedule appointments. The office itself is clean and efficient. Great staff and very competent all the way around. This year I have had a few hospital visits due to an accident on a bike and Dr Singh always kept up with my situation. He is the best doctor I have ever had the pleasure to see. I highly recommend Dr. Singh and his entire staff. If you are a busy executive, be sure to check out the video appointments! They are terrific :) Rancho Wellness has provided for my needs from the second i started going there. Since I have been seeing Dr. Singh I no longer dread going to the doctor because it is always quick and easy. Dr. Singh is very patient and I have recommended him to the rest of my family. The staff is always very friendly and helpful when it comes to making appointments and dealing with insurance. One of my favorite things about Rancho Wellness is their online patient portal which makes it so easy to refill perscriptions. The online site also has a way to email your doctor directly and in my experience they have always gotten back to me quickly and efficiently. I have finally found a doctors office that I feel comfortable going to. Excellent service, friendly staff and Dr. Singh is a godsend! He is always there to listen to your problems and has always taken a positive approach to everything. Would recommend him to everyone. Literally the best doctor we have ever been to! He is attentive, and genuinely cares about his patients. I have been to lots of doctors and have even worked at a health plan and I am continually impressed with Dr. Singh\\'s office and staff. So far, no negative comments about his staff either and I am so happy I finally found a doctor I love and can rely on! I have been a patient of Barbara Santa Cruz, NP for about 11 years now. I followed her and Dr. Ravinder Singh from another physicians office. Barbara, Grace, Dr. Singh and all the girls in the office are AMAZING. So caring. Exactly what healthcare should be about. You just do not get this kind of service anywhere these days. I almost was sad to see them so connected on fb, twitter, etc. I want them to stay small and unknown. Just be MY doctor\\'s office. LOL! Is that selfish? Barbara allows me to be proactive about my health by not treating me like an idiot. She explains everything fully. Such a wonderful Christian woman who shares personal stories to take away my fears. Absolutely love her! I had a condition go diagnosed for many years because of fat prejudice in the medical community. Previous doctors would look at me and say, \"Oh it is your weight\". Dr. Singh took the time to talk to me. He is open-minded, knowledgable, and keeps up on new technology & advances in medicine. Polycistic Ovarian Syndrome (PCOS) and its treatment was just being recognized by the medical communtiy at that time. Dr. Singh presribed the correct medication and diet for me. With his and Barbara Santa Cruz\\'s, his NP, help I was able to lose 90 lbs. I had been on Weight Watchers and exercising with a personal trainer for more than a year, but was not having any success. The combination of the low GI diet advice & metformin worked. I have too many other stories to share. Absolutely so blessed to have found Barbara & Dr. Singh.Dr. Singh and his staff remember each patient. I feel valued as a patient. My time means something to them and when I make an appointment there are no long waits. In and out of there so I can return to work or get on with my day. You will really find old fashioned healthcare there. Someone always answers the phone and you actually have time to talk to your doctor. it is pretty easy to get same day appointments. If they have a large patient load, you would never know it. Dr. Singh has been my doctor for awhile now and my families too. He is always checking up my family every time I come in and is very friendly. The staff is great and helps in anyway they can. I made an appointment and did not have to wait long and was seen and out in time. Thank you Rancho Wellness! 3 of my accounts from Labs etc....went to collections because the insurance staff did not provide Blue Shield with simple info that was needed...If they called Blue Shield and hit a hurdle as far as speaking with someone they would stop. It took me almost 1 and 1/2 years to get all resolved. Thanks to the collection companies working with me and Blue Shield. Blue Shield was the hero after my many hours of my contacting them. Also I had to call Blue Shield while sitting in front of the insurance gal at the dr. \\'s office to get results because staff said they could not get through to them!!!!! Also I never waited less than 30 minutes for an appt. Dr. Ravinder Singh is truly a special and unique doctor. He takes the time to listen to your needs rather than just rushing you in and out of his office. I first came here (years ago) because I felt that all of the other doctors I had visited lacked A sense of professionalism and worry more about the dollar rather than the care of the patient. My wife fortunately found this office online as we were looking for a medical doctor within the Rancho Cucamonga or surrounding area. I have suffered from migraine headaches for nearly 15 years of my life and I do feel that Dr. Singh and his staff have worked to provide both care and service at a high level. I had visited at least 5 doctors within the Rancho Cucamonga area and I finally found a home after visiting this office. One thing that I think we can all agree on is that no one likes taking time out of their day especially when you have to leave work in order to visit the doctors office. This organization respect your time and when you do check in you can usually count on seeing your doctor within a reasonable timeframe. On the other hand one of my biggest issues with other doctors within Rancho Cucamonga is that I would wait up to an hour after checking in at the front desk. If you are truly looking for a professional to take care of both you and your family then make sure and visit Rancho Wellness. you will be happy that you did. Dr. Singh is an amazing physician. Every appointment I have ever had at Rancho Wellness has been a positive experience. Dr. Singh is always a great listener, caring, and has given my family the best care possible. He has even gone out of his way by returning my call immediately when I was out of state on vacation and needed a prescription filled. I was absolutely astonished by that level of patient care. My whole family is seen by Dr. Singh and I recommend him often to friends and colleagues. I am a patient at Rancho Wellness, and thus far I have been very impressed with Dr. Singh. He is kind, understanding and really takes his time. I am a pretty tough critic, but this doctor made me feel like he cared about my health and well being. First let me say that as of yet I am not a patient of Dr S. However I f I had real insurance that Drs actually except instead of IEHP Id very much want to be his patient. Iv had the privilege of getting to know Dr and take it from me this man is a truly kind compassionate wonderful person and anyone would be lucky to have him. He truly goes the extra mile and truly cares about his patients and is a rare breed of physisn. I was referd to him by a mutual friend/dr ehom sadly passed away and sadly while I was not able to see Dr I have gotten to know him and he is a wonderful caring man. I pray i will have the privilege of having him a my gp one day. If your looking for a Dr that cares listens and understands look no further. I have been a patient at Rancho Wellness since 2009. The office was recommended to me by a specialist. This is by far the best general practice I have ever been a part of. The entire staff is professional and caring. Dr. Singh provides a comprehensive plan for preventive health as well as managing chronic conditions. He calls specialists to get more detailed reports on his patients and is very responsive to communication using the secure patient portal. He has 2 superb nurse practitioners who work with him. I do not hesitate to see either one of them. By far the best part of this staff is how well they communicate with me and with each other. I highly recommend them. We made this appointment online. Barbara Santa Cruz NP entered the office to see my son. She did not introduce herself. I am an RN and I found her condescending, accusatory, a poor communicator. After the lack of introduction she did little to evaluate my son. There was no exam...she did not lay a stethescope on him. I found this entire visit a horrible experience and can honestly say I have never experience anything like this. Caring......what a laugh. We were all terrified by what my son had experienced yet she showed no compassion or caring. How absolutely disheartening. We will never go back. She actually threw her perscription pad on the desk in front of us in a disgusting manner. I have been a patient of Rancho Wellness 8 years it is a very family friendly environment my family and I have a great relationship and bond with rancho wellness and Dr Singh. He is a great doctor I would recommend him to anyone looking for a doctor. There is alot of good points about Dr. Singh, he is extremely nice and takes his time, he even emails you and you can email him. The office staff needs work for sure. They can be unfriendly and lack personality the majority of the time. Sometimes the Dr will try to solve things on his own instead of sending you to a specialist, but over all he is a good doctor, just wish the office staff was friendlier. I chose to establish care with Dr. Singh because of his credentials. I have seen him three times so far. Each time I enter the office, the girls at the front are extremely professional and very friendly. The medical assistant who takes me back every time is just as nice. I have not experienced any billing issues, insurance issues, scheduling issues, or anything of that nature. I have never waited longer than 20 minutes in the waiting room. In fact, they take me back within 5 minutes of signing in (my appointments have been in the morning hours). I have had my blood drawn there and the lady who performed the procedure was so caring and so nice. I got a little light headed when she finished and she let me sit there until she made sure I was okay to leave. She talked to me and laughed with me like we knew each other for years. Dr. Singh is truly the best doctor I have ever met. He is kind, sincere, he does not rush to get out of the room and most importantly, he listens to you and he does not make you feel uncomfortable in any way. I typically try to pick female doctor\\'s as I feel more comfortable with them but I will not see anyone else but Dr. Singh now. He does not just throw you on medications like a lot of Doctors do. I saw a Doctor a few years ago (for what I did not know at the time was just anxiety) and he was cold and made me feel like he was annoyed with me. Not Dr. Singh! I believe he has the best interest of his patients at heart. I have never met a Doctor nicer than him. He knows what he is talking about, he is extremely professional and as far as I am concerned, he is one of the top in his field. Needless to say, he will be my Doctor for years to come. You cannot get better than Rancho Wellness! First I would like to say I do not go to the doctor\\'s very often. I wanted to get established with a primary doctor and this was perfect for me. Their staff was very friendly and there were no hassles. I went in, got to see the primary doctor, who was very considerate. They checked up on me and boom I was done. I feel better now and feel that I could keep coming here for future health care now that I feel comfortable. Thanks :) Awesome medical care. Great and caring staff! I enjoyed the Sycamore Inn event too. dr Singh is compassionate caring and loves his patients. As for the nurses they really care and return my calls. Thanks! Hands down....best doctor I have seen in the area! Dr. Singh was highly recommended to me by a friend and I am so glad. In my 2 visits with him he has shown compassion and a genuine care and concern in my well being. He has gone above and beyond to make me comfortable and trust his expertise. After being with Kaiser (which is a complete joke when it comes to healthcare) it is refreshing to find Dr. Singh who actually looks and listens to you as a patient. At Kaiser the doctors are too busy typing in the computer to even have any eye contact with a patient. I have been suffering with something no one could seem to figure out and Kaisers best answers were to put everyone on an anti-depressant. They only care about your payments, not your health. With Dr. Singh I feel that he is really trying to help me find out what my issues are and the best route to treat me. He is very knowledgeable and well connected with the specialists in the area. I am so glad to have found him and I look forward to a long relationship with him. The office staff is very helpful and very friendly (and I hear they enjoy a blizzard treat on occasion). I am a very busy hairstylist and I refer clients to many businesses when I have faith in their performance. Look out doc....loads of new patients will be on their way! Very thorough and they listen to any medical problems. Great experience! P.A. Barbara Santa Cruz & Senior Care Specialist Brandy Furgerson as well as all other staff at Rancho Wellness/Dr.Singh office in Rancho Cucamonga are excellent!! My mother has been going there for several years and this week was the first time I went in with her due to a recent medical event. I saw firsthand how all of them treat their senior patients. It is a huge relief to see that their patients are treated as family. We were not rushed in and out, they took their time and answered all of our question. Even when I called in to speak to someone there about my Mom there is a special senior line for quick accurate customer service. Very impressed with this office! From the friendly, helpful office staff, to the caring PA her took her time with me. This office is very well run, which seems to be almost obsolete these days. I am glad I found them and am hopeful they will be a great replacement for my dr of over 10 years who recently retired. Dr. Singh and staff are awesome! He has been my PCP for 5 yrs now. he is attentive, listens to your concerns before responding to them. Excellent bedside manner. Such a kind man. My husband goes to him also per my recommendation and likes him as well. Staff is great also!. I always have a good experience when I go. I highly recommend Rancho Wellness. I was looking for a new Doctor after moving here, saw the reviews thought I would try Dr. Singh. All I can say is amazing!! Went in for my first visit, staff was awesome, only waited 10-15 mins in the lobby, and 5 in the room. He then sat down with me for 25-30 min, to get know me and what my history was, and what was going on. He took me off almost all my meds, explained the reasons why, reordered new labs, and has a plan of attack depending on results of labs. All I know after 1 visit with this guy, I have found my Doctor!!! Rancho wellness is everything, that all these countless reviews say they are.. My husband has been so sick and Heidi Larson went over all the extensive labs, X rays, ct\\'s etc. and really listened to the symptoms my husband was experiencing. He was there in her presence for an hour. It was a breath of fresh air to FINALLY find someone who cares. My husband has more tests and things to do but we feel hopeful that she will come up with a solution. Also you have a question the staff actually calls you back !! Thank you Rancho Wellness Dr. Singh is the BEST Doctor I have come across yet. He is extremely caring, attentive, patient and responsive.His staff, however, is another story. It never fails that when I call them, I have to repeat information - not because they cannot hear me, but because they are not listening. The conversations with any of the assistants are those you would expect to have with someone at a fast food restaurant, not in a medical environment. You know, the kind where there is no common sense and they have no interest in getting answers for you. When you call in and say \"Hi, my name is Jane Doe\" and they reply unapologetically with \"Can I get your name?\". Yeah, that kind of call. Irritating!But, the Dr. himself is fantastic - and for that, I will continue to see him.The best part is they have an online patient portal where you can log in and send messages to the Dr/staff, get your medical information, etc. A step ahead! Dr. Singh is a great doctor , he is been our family doctor for years. He cares a great deal about his patients. The staff are always so helpful and friendly. We absolutely love everyone there. Dr. Singh and his staff truly deserve five stars!! Dr Singh is the most compassionate Doctor I have ever seen. He is like the old fashion Doctors that used to really care about the patient. After having some health issues that were not being taken care of at my primary care physician I was given Dr Singh\\'s name. I am so happy with Rancho Wellness. Brenda K I have been a patient of Dr. Singh for almost 5 years. I love love love this man. He is the kindest dr. I have ever had. Everyone in the office is helpful and is eager to get you in when you are not well. One caution, but only one. When Dr. Singh was out ill for awhile, the office fell apart and it was not the same place. However, if you can be patient with the process, Dr. Singh is worth the wait. Dr. Singh is great! His bedside manner and professionalism is impeccable! I would highly recommend him to everyone! His staff members are also excellent! Dr. Singh has been my physician for almost 20 years. He is absolutely the best! I also see Barbara Santa Cruz for my \"female \" things or when dr is not available. She too is great! The office staff is fantastic, caring but efficient. Thanks everyone for all the help and support over the years! I have seen Barbara Santa Cruz for at least twenty years. I would say the first 15 she was great, she would take her time with me and listen to me. About five years ago, my arthritis became severe and caused me so much pain that I could not function effectively as a teacher. Barbara gave me prescriptions for pain and sent me to an orthopedist. When the pain did not subside I asked to see a Rheumatologist. Barbara insisted that he would do the same thing as the orthopedist and would not write the referral. I went back to the wonderful orthopedist that I consider saved me a lifetime of pain and joint damage and he said he would write the referral. On my first visit with the Rheumatologist I was diagnosed with a severe form of damaging arthritis that I had suffered with for years. I was put on a specific medication, and have regular monitoring office visits and blood work. If it had not been for the orthopedist I would still have pain. I was so disappointed that Barbara did not take my complaints of pain seriously. There have been other issues that she did not deal with effectively either. I think she has lost her compassion somewhere along the line and I would not recommend for anyone to see her. Dr. Singh is the best doctor I have ever been to. I have been seeing him for 7 years and have no complaints whatsoever. I have gotten to know the staff fairly well over the past years and get all along well with all of them.Dr Singh is compassionate, attentive and caring and he has a great manner. I cannot imagine ever going anywhere else. I am a fairly new patient of Dr Singh and am very impressed with the care I have received so far. From the girls at the front desk to the medical assistants in the back, this is by far the best family doctor I have ever had. With a friendly smile, Dr Singh is attentive and informative. I would highly recommend this office to anyone of all ages. They even have a patient portal which makes it easy to ask questions and request prescriptions refills !!! Visited this morning for my Annual Health Checkup. Nurse Practitioner was great in evaluating and suggesting Lab Works. 4 Star as the App and other follow-ups are not there yet.... I have been going to Dr. Singh\\'s office for several years now. I would definitely recommend him. He is kind, caring, considerate, listens and takes the time to explain things. I have also seen Barbara Santa Cruz and she also has the same traits. It is so nice to feel like they truly care about their patients. The office staff is also really great. One of the best things I like about this office is when you are ill and need to get in right away, they will get you in the same day. Also, once you get there, the wait time is never very long. I am very happy with Rancho Wellness. Rancho Wellness, Dr. Singh is the BEST Doctor. His staff are friendly and they always follow through. The waiting time has never been long. He is very thorough. I highly recommend him if you are looking for a Dr that cares. I have been with Rancho Wellness for about four years. I was looking for a doctor who could help me through some really challenging conditions. I found Dr. Singh and he is wonderful! One day, I was feeling horrible and was given a same day appointment with Barbara Santa Cruz, one of the Physician Assistants. I had no problem with seeing a PA as I have learned that many are as good, if not better, than an MD. My only hesitation is that I am not good with change. I am so happy I went to that appointment with Barbara. She is excellent with a great bedside manner. She took time to listen and gave me good, sound advice and treatment. I have seen her several times (sometimes at my lowest) and Barbara Santa Cruz has been consistently patient, informative, professional and kind. Rancho Wellness has provided our family with real \"family doctors\" and that is a great feeling. Being able to have excellent choices within the same practice is a win-win!!! We Rancho Wellness! If you want doctors and support staff who listen and care about you, this is the place. I have been a patient for over a decade and leave the office every time knowing I am in VERY good hands. You can tell they have hand-picked like-minded people who work together to support your health, not to maximize profit. I am thankful to have found Rancho Wellness. I love Dr. Singh! He is very attentive, knowledgeable and amicable. The staff is great as well. Waiting was only 5 mins. Clean and efficient office. Best Dr. Nurses and staff. I was with Aspen Medical Center for many years and can say what a difference. My uncle has been with Dr Singh for many years and loves him. He referred me to him. My husband and I are so happy we did. They are amazing! They actually answer their phones, we have not been put on internal hold at all. The nurses and Dr take the time to listen and go over any concerns. We never had that at the other office. We are extremely happy! Great experience, short wait (not common for a doctor\\'s office) and very good customer service. DO NOT choose this health facility if you are in pain and need immediate attention. I had an MRI which revealed two torn ligaments in my knee. I want to Rancho Wellness 3 weeks ago and I am still waiting for the doctor to call me with my referral for surgery. Choose a location that is not so busy that the doctors forget to call you. I now have to go to the emergency room just to get attention. that is a black man this office never made me feel comfortable and distaff are rude on the phone. By far the Best Doctor I have had! Ranch wellness in Rancho is superior to all, they truly care and make sure you feel valued and respected. Dr. Barbara Santa Cruz is the BEST! thank you for your kindness. Dr.Singh is the best doctor , always take time for every patient and never rushes you .He is very very kind . The office is very clean and everyone is so nice there. Extremely disappointed in care of my UTI from this office. Urine specimen was submitted over 11 days prior to receiving results only after several phone calls were made. This office does not care about patients. Had to suffer needlessly. On top of this, no culture and sensitivity was done on specimen, so there should have been no reason for waiting.This is poor care and so disappointing. I had my first visit as a new patient, of which I have had a difficult journey learning the PPO world of finding a doctor out of the \"phone book\", and am very pleased to have landed with Rancho Wellness. Barbara Santa Cruz was very helpful and encouraging about beginning a health plan that works for me. The visit was with minimal wait time, quiet and professional environment, and very friendly staff. I am happy to continue my care through this group. Dr. Singh is great and truly cares about his patients. Also responds very quickly everytime you need help. Bottom line, if you want to sleep knowing that you are well taken care of...switch to Rancho Wellness and let Dr. Singh and his associates guide you to a healthy life. You will be looked in the eye, listened to, asked relevant questions and referred ONLY to those who will take the same care with your health. You will not wait weeks (or most times even hours) for an appointment, you will not sit in a waiting room annoyed or delayed, you will not be given misinformation, you will not have to urge them to finish paperwork for referrals or prescriptions and you will never feel like a number of a dollar sign. I am such a difficult and picky person. it is an honor to be taken care of by this carefully selected staff. Thank you Dr. Singh for your vision and your heart. This applies only to genius ERIK-DNP(Do Not Practice). He is good at everything except what he is supposed to be. Everything was good until I asked him to order some lab work for me. He do not want to write what I ask for as he thinks it is not needed. I told him I pay lot of money for insurance and getting blood work once an year does not hurt me. Genius says it is going to cost $10,000.00 for the insurance. What the hell? I am not one of those people living on tax payers money. Probably I pay more taxes than you, genius. Where did you get that $10k from? Quest charges my insurance $1500 for all the blood work I had requested and it is 100% covered. Then he starts saying what if something comes positive. that is the point damn it. If something is not right, I will start taking treatment right now before it goes out of hands. This guy wants to order my history from my previous primary physician and I should pay copay again for second visit which can be taken care during the first visit itself. People like this should be banned from clinics. This guy has no idea about preventive care. He asked me why I need to test my insulin? What the fuck? To check my blood sugar levels and to keep it below 90. Anyone can get diabetes. DNP??? Really? I know better than you. Love love dr barbara Santa cruz. She was so helpful and encouraging and really makes you feel important and at ease. The whole entire staff were sweet helpful and efficient.. I was in and out in no time the whole process smooth and easy and most importantly I did not feel like a number that they needed to check off.. Soooo where do I begin... I have been a patient of dr singhs since 2013, I want to say I found him on yelp. I first started with a physical but I had some issues going on that I was not understanding. ( I am learning my body, I do not have the greatest pain tolerance, sigh.) Dr Singh is really great at paying attention to your symptoms and never do you feel as if he is not trying to do what is best for you.! we have been through quite a few things and he is even calmed my nerves because let us just say I am a little bit of a spazz and like to google things. He remembers little details about you and you just know you are in good hands every time! He will not jump the gun with medication and if he does he explains why or why not I should go that route. Not too many doctors care that much. Front office staff in the rancho location seem to be a lot more friendly than in the upland office I believe they rotate the girls to both offices but maybe they act better when Dr Singh is in the office lol it happens! Nobody has ever been awful but a couple of the medical assistants are not always in a good mood but usually they are better in the morning so maybe they are just full from lunch if you come in the afternoon.\\u200d Monique and misty are always great and efficient, they have never given me attitude or I have never felt that they were being judgmental because like I said I am a bit spazzy and my pain tolerance can suck sometimes! Again everyone tries to help you or gets someone to help you. what is awesome is that they have a service where you can directly email doctor an issue and he will either have the girls respond or he will respond to you. Saves lots of time and helps with peace of mind. They can even schedule you for video visits dependent on your insurance.Okay so I have seen 2 other providers here, I seen Barbra Santa I believe was her name but was not the best visit and she was in a hurry plus was not in the best mood. it is been a couple years though so hopefully she is gotten a lot better since then. I recently saw Heidi the PA and thought she was great and thorough. She digged into my chart and was talking about blood tests I did in January and got off track a little lol but overall I would go to her again. I have referred many people here and I will continue to do so. I work in the medical field so I am pretty familiar with the ins and outs of how an office runs. They usually do not make you wait more than 5 min which is pretty awesome considering they are pretty busy. Make sure you ask ahead of time about your co pays. I have a ppo and even with some of my same day appointments they wait till I get there to tell me I have to pay $20 or $100 which they could of told me once they were scheduling me. I only say that because if I am leaving work to rush there for a same day appointments than I can be absent minded and forget my wallet (which has happened before) sometimes the girls have to ask if I can call in my payment which I have forgotten before and if they would of prepared me we could of just not dealt with the back and forth. We figure it out though and they do their best to work with me. Thank you Dr Singh for all that you do.! This receptionist that attended me has a negative attitude. Just because of her I will not get a doctor here. Rancho Wellness is what modern primary care should be. The physical facility, the staff and the doctor employee current technology to manage your health care. They do not herd patients in like cattle. You are given an appointment time and you will generally be taken in at that time or very close to that time. The most important and impressive part of this office is Dr Singh. This doctor genuinely cares about me and my health. He is patient and kind. He asks questions and he answers my questions. He is totally modern and old fashion (he gives this old grandma hugs for goodness sake) at the same time. If you and your family need a primary care doctor do not look any further. You will. E very happy with Dr. Singh. I am so happy I found him. Dr. Singh is a wonderful doctor and his office staff is very professional. I highly recommend Rancho Wellness for anyone looking for outstanding care. Today was my first appointment with Dr Singh. I have seen 3 other General Practice Doctors in the area over the last ten years or so and have not ever left a visit feeling satisfied or really happy about the Doctor/visit. I was so happy with my visit with Dr Singh today! He just made me feel like he really cared. I did not feel like he was in a hurry to get the visit over with. He listened to me and addressed all of my concerns. I am very excited to have finally found a Doctor I am happy with. The staff is also nice. I would recommend Dr Singh to anyone looking for a Doctor Good for doctor. Keep that gem with you please. DO NOT LET HIM GO. You said I am my request is insurance fraud. Your perception is wrong, I have every right to get whatever blood test I need to make sure I am perfectly satisfied. Last year I was looking for new Primary Doctor after changing insurance through work. My Co-worker recommended Rancho Wellness and I am glad she did. I have had great experiences and am very pleased with level of medical care I have received. There is also an online site that let us you message the Medical team or you can view labwork results. Very modern and functional to have a website inclusion option for your use. Highly recommend making an appointment, or even changing Doctor offices if not happy with your current Physician; you will not regret it. New patient and I could not be happier. Barbara is so attentive and caring. She is really helping through my medical issues. The staff is very friendly. Dr. Singh is one of the most competent and skilled physicians that I have ever encountered.Barbara Santa Cruz is arguably the most incompetent, self-important, condescending, sanctimonious and rude person that I have ever encountered.Five stars for Dr. Singh.Avoid Santa Cruz as though your life depends on it, as it actually may. Love this healthcare provider!! Dr Singh is the best and his staff is awesome!!! I saw Erica on my visit yesterday, June 2....she is absolutely the best!! No pressure on time, addressed all concerns and made me feel comfortable to talk about anything...... thank you Rancho Wellness!! Ravinder Singh attempted to scold me once I forgot to call to reschedule my appointment. I tried to explain myself and he continued to rudely shut me up and speak over me, he then hung up. The docs in this office are always professional and very helpful. Had an issue with a lady in the office but as always dr. Singh responded as quickly as he could. Took care of the situation and that is why I still give them a 5 star Dr. Singh is one of the best physicians I have come across. He is knowledgeable, friendly and genuinely cares about you! Explains why he is so busy! This review is only for the office staff, not Dr Singh. He is wonderful and I have valued his care and direction for the past 3 years. I am however done with his rude and incompetent staff. They do not return calls, when you get one on the phone they are rude and condescending, completely unhelpful and I have had enough. I have officially changed physicians. it is a shame really, but I refuse to be treated this way. Dr Singh if you see this review, I hope you do something about your office staff because you sir are a good doctor and should not be loosing patients for their wrongdoing. Would not recommend- very opinionated, arrogrant in my opinion, and also in my opinion should make no diagnoses on a young teenagers mental health- was clueless and made completely inaccurate assumptions. Wish we had never made a visit. First time for my husband.They were great in taking care of him.He is picky usually about where he goes and so far is very happy. Dr Singh and his team are amazing, and I never feel like just another number that you get with most Doctors offices. Only 1 PA I do not particularly care for, but I just call and get someone else. I have been working with Dr Eschweiler and he has been amazing in handling my anxiety with care. The ladies at the front desk are always sweet, as well as the MA\\'s. We definitely will not be going anywhere as long as they are in business. I never have any issues coming here. The front staff are always sweet and accommodating. Dr Singh has probably been the nicest doctor I have ever had. He never rushes you, he always listens and acts like he cares. Jessica the PA is always amazing as well whenever I have had to see her. If I can give Dr. Sing 20 Stars I would give. I have never seen a Doctor so caring and want the best for his patients. The way he treats me and my mom is like we are part of his family. He is treating my mom with her chronic Migraines and is extremely patient with her. I never have any issues with reception staff or the nurses, they are all very nice and even got my mom in when they were super busy. The waits are not bad as well. The online portal is so easy and convincing to use. All the staff are very patient in answering questions and most of the times will answer my question over the phone instead of bringing me in. Dr Sing has become our family doctor and I am forever grateful for his service. Doctors like him are rare these days and need to be appreciated. He wants to do his best for the comfort of his patients and he treats them like humans not as clients for money. I have never wrote this long of a post but I am just so grateful for him! May God bless you and your family for helping others. On behalf of my sister, Vicky Morse and my brother in law Jim Morse... we just wanted to give a huge shout out to BLANCA!! She is the best receptionist that we all have ever dealt with here. She is kind and extremely helpful to all of us, especially Vicky and Jim. When ever they are in need of anything, Blanca is always ready and eager to help! She is a great asset to Dr Singh and his practice here. We are so grateful for Blanca and appreciate everything she has done for us! Sincerely, Vicky & Jim Morse I made an appointment for my mom to see Barbara Santa Cruz and it went great! She got along with my mom which was amazing lol. She was able very attentive and gave my mom some new medication that is been working. We also got her order for bloodwork done as well, the whole works, which she really needed. The entire office staff are all extremely nice and helpful. I have called about insurance that was brand new and they were on top of it, which made it easier for us. My mom still has a future appointment to see her!I would highly recommend coming here! The PA sucks big time! she is a big bully and made my visit to that office hell! May she get it back like she gave it! I love this office! The staff are nice, they are really quick and most importantly it is CLEAN! Highly recommend coming here. For the past few months I have been able to contact this office and get an appointment same day for my elderly parents , husband and kids. Today I call and the girl on the phone seemed bothered and unhappy to be there I can feel her energy through the phone. I asked to speak to Norma as she always helps me with appointments and is always so patient with me and my parents but she was not there today. Please Dr Singh and Barbara be cautious of who answers your phones ! Dr. Singh has excellent bedside manners and never rushes you. His office takes a thorough questionnaire of their patients in order to treat you as a whole person rather than simply prescribing meds right away. I did a blood test, titer test, TB skin test, EKG and physical and Dr. Singh and his staff were professional and kind throughout my visits. The blood test is through Quest Diagnostic. You do sign papers saying if you do not receive your results then you will be responsible in getting in touch with the office, so I am not understanding other reviews who are complaining about that. Both offices in Rancho and Upland are clean and well equipped. I have not had to wait longer than ten minutes from my appointment time to be seen. they are online so you can fill out your forms beforehand and access your medical files through their web portal. Appointments are easy to set up. Most insurance plans are accepted. I was diagnosed being pre-diabetic and lacking vitamin D so I have made dietary changes along with taking vitamin D supplements and will see the Dr. again in a few months. I hope to see Dr. Singh as my family\\'s primary for a long time to come. He is a younger doctor than my previous primary physician. I will definitely be referring him to others. I had my first visit today with Dr. Singh and he was very very personable and optimistic and listed to all my concerns. The staff was pleasurable and very nice! I am glad I found Dr. Singh! If you saw my previous rant about another doctor place in the same lot, well Rancho Wellness is a few doors down and on that same day I walked over in tears bc I was so frustrated with my then-current Doctor. Rancho Wellness assured me that they do their best to get patients in and out of their office in 30 minutes. I got my insurance changed and last week I was REALLY sick. My throat was tender and I was able to get same day appointment without a hassle. I was there 10 min early of my appointment; was in the doc office on my appointment time (vitals taken and copy paid by this time). Heidi (who is awesome btw) came in, checked me out and knew exactly what to prescribe me. We chit chat a bit about my usual symptoms, walked out at 20 min after I had arrived there to find the front desk confirming my pharmacy to send the Rx to. I was in my pharmacy drive-thru line and home ALL within 40 minutes. Oh and I felt better with the medicine prescribed within 2 days. I love this place and so do my employer because I was back to work within a day! Heidi is the best! The staff is all groovy too. This was my second time here and Wait time was not too bad and staff was friendly. it is easy and convenient to get an appointment Thank you Dr. Singh and brandy for fixing my issue with my medication refill. Brandy was very friendly and helpful on the phone Rancho Wellness was there for me when I could not find any other doctor\\'s office to lift a finger. The staff is professional and caring and they truly understood my needs. They did not make me jump through hoops and I felt that I was in good hands. I did not have to wait a long time to see the doctor and she was quick, thorough and gave me just what I needed. I will be staying with Rancho Wellness for as long as possible. They simply forgot about me. I went to get a physical for the first time. They said they need to do blood work first. Sure no problem makes sense. Rhey took blood and Made a follow up appointment for the actual physical physical. A week later they cancel on me dr. Is out. They will call me when he comes back to reschedule. that is five months ago. They have never called me as promised. it is been five months now. I had to call them and to get an appointment. I called today the doctor has been back awhile now they just did not bother to call me or make an effort. Bad customer service they never even apologized not at least a simple \"sorry we forgot about you\". Nothing just a whatever \" well do you want an appointment now or what\" attitude. omg. Michelle in Senior Patient Care is beyond competent... and patient. At 4:30 pm the Friday before Christmas Monday, i left a VoiceMail asking for a callback. No emergency...just needed to know the order in which i needed to get the labwork done, schedule my annual well woman exam, and what might/might not be covered by Medicare. Within 15 minutes i received a DETAILED Voicemail from Michelle answering all my questions, even leaving her phone number. Now 4:45 pm she returns my second call and answers my 2nd round of questions in person, with clarity and patience. I have NEVER EVER had this professional and caring service from ANY doctor\\'s office. Michelle and her office staff are too good to be true. One happy patient here I have been a patient here since 2007 and Dr. Singh has always been my favorite doctor. He is professional, caring, compassionate and the best doctor I have ever had. I have a rare, chronic condition, and in the past many of my doctors have not believed all of my symptoms or taken me seriously. One even told me it was psychological. But Dr. Singh has always taken me seriously and has helped me to manage my symptoms effectively for my chronic condition and I cannot imagine going to anyone else.I have recommended him to all of my friends and relatives because he really does have a gift. Most of them now go to Rancho Wellness. He was recently out of the office for a few months and I had the opportunity to meet the other two female doctors there. They are also really nice, but I am glad he is finally back. His office staff have always been pleasant to me as well. I would recommend him to anyone and everyone. Some doctors just have a gift, and he is one of them. Dr. Barbara Santa Cruz diagnosed in 15 seconds what was wrong with my knee pain. I have seen numerous doctors who have charged me an arm and a leg to say that it was all in my head. She went straight to the site of the pain without me telling her where it was. She made a plan of attack and I am eager to start it. I would definitely recommend her and her lovely staff members. The office staff is not very friendly or warm at all. I was a new patient and my first impression was that they were pretty cold. I was seen by a Nurse practitioner and she was great. I have no complaints in that area. Overall not a horrible experience but I think the staff need to work on their interaction with the public. A smile never hurt anyone. Rancho Wellness is a medical office I considered due to the excellent reviews on Yelp...and they did not disappoint! The appointment availability was within a few days and I was advised that I can print and fill out the forms online to be seen faster when I arrived. The office was clean, spacious and was not crowded. I did not have to wait long to be seen and I had a great conversation with the PA regarding my medical issue. I did not feel rushed, and I felt a genuine concern for my health by the PA. They have a lab within their office which helped me avoid driving around to get blood work done. I personally like the options they have for online appointments, which is helpful since I do not live too close to the office. But when you find a good doctor, it is definitely worth a drive. I am officially making this my doctor and have already referred them to a few people!! I was referred to Dr. Singh by my coworker. Everyone is honestly so nice to me every time I go.I never feel like I am feeling rushed when addressing all of my concerns to Dr. Singh, he always listens to everything I have to say! Best doctor ever!!! Clean office. Efficient and friendly staff and I received great care. I love this doctor and all of the office staff. Both the Rancho Cucamonga office and the Upland office. I highly recommend !!! Dr Singh always takes his time and I never feel rushed.He is warm and caring.He is the first Doctor I feel comfortable with. I am so happy I found this place . I never have to wait long to see the Doctor. The staff is also very friendly . I highly recommend this place I am adjusting my review a bit simply based on the fact that Dr. Singh personally reached out to me and apologized for his employee Christina\\'s childish behavior. Thank you again Dr. Singh Just to elaborate on the lack of professionalism and lazy work ethic of staff. I had blood work done about 4 months ago. There were some irregular findings but no one contacted me from Dr. Singh\\'s office, so I did not worry about it. After dropping Dr. Singh and finding a new dr, I had more blood work done. Within 3 days my Dr called me and asked if I could come in as soon as possible. Same irregular findings. Turns out I have Chronic Kidney disease. Definitely not Dr. Singh\\'s fault or even the fault of his staff, but this again proves when the Dr. Was out, his weak staff did not get the job done Wow. I just got off the phone with some rude, immature employee by the name of Christina at Rancho Wellness who has convinced me to change my doctor. Her mouth was going so much, I had to hang up in her face. She calls to tell me once again I cannot see Dr. Singh but she could not keep quiet long enough for me to ask her a question. Unprofessional to say the least. The downside of employing children. I would never visit this office again Finally found a doc that I loved....and then we move across the country. That is why I feel blessed to have found Dr. Singh and the staff at Rancho Wellness so quickly after we settled in CA. They were all so kind, compassionate, and helpful; traits that are becoming more difficult to find. Here are just a few things I really liked: was able to get an appointment quickly, short wait time in waiting room, and Dr Singh took the time to listen to me (no rush). Had an initial visit with Dr Singh today. The staff was attentive and very professional. The experience only got better after I met the doctor. He was very easy to talk to and was reassuring about everything. He throughly takes me through my family history and my current exercise/eating. Regiment. We scheduled a full physical coupled with some advanced testing to ensure we monitored issues that have been prevalent in my family. The visit went so well I recommended him to my wife. I have been coming here for Picosure Laser treatment for tattoo removal and melasma which I have all over my face. it is one of the hardest skin conditions to treat as most of you may know (if you suffer like I do.) Dr. Singh is extremely professional! I was scared at first but he really makes me very comfortable and explains the process of how Picosure laser works. He removed a hideous tattoo I had on my hip. I was so surprised how quickly he was able to remove it in only 3 sessions. I have gone to Dr. Tat off a few years prior for a different tattoo which was the same in size, and it took over a year (12 sessions) to remove. Picosure laser is truly amazing! As for my melasma, I have had so far 4/5 treatments. I am super impressed with my results! it is just about gone and I could not be happier. I also really love the way the laser makes my skin feel afterwards. I also notice the pores on my face have shrunk so much they are barely noticeable. I will be keeping up with the Picosure laser every year to maintain my results. Thank you so much Ranch Wellness! You guys are honestly the best of the best! The staff at Rancho Wellness are always friendly and courteous. They always go the extra mile for patients!!!! Great customer service! Provide virtual visits for traveling patients like myself. Called in my prescription and I was able to have it transferred to the state I was visiting. Would not recommend going. The PA is very judgmental and not kind. Never saw the doctor. They never call you for blood results. You have to get ahold of them. I do not have words to express my level of gratitude for finding such an amazing medical team.I moved to the IE 3 years ago and I have struggled since then to find a Doctor to go to. Unfortunately, I was never able to see the Doctor I had originally elected and was given a 3 minute consultation by the P.A\\'sI am glad I took a leap of faith and ventured off in search of a new Doctor. Yesterday was my very first visit with this \"new Doctor\" of mine, Dr. Singh, and I can wholeheartedly say that I felt like I was having a conversation with my friend. I felt peace and comfort as he and his assistant went through the discovery process with me. They were both extremely attentive and personable. The administrative team, as a whole, were professional and thorough. I have to say that they won me over as we discussed the \"passion\" behind Celine Dion\\'s music :)I look forward to a healthy journey with my new \"team\" First time patient and very positive interaction with staff and doctor. I was seen within 15 minutes of my appointment time. I felt like Dr Singh was empathetic, listened to my concerns and took his time with me. I appreciate his approach to wellness and his holistic approach. I feel good about selecting Rancho Wellness! Love my experience! I felt very comfortable! Doctor took a personal interest and made me feel I had her attention! She was thorough!!! Definitely coming back and staying. I have heard many good things about the dr. and staff, from close friends. But from my own experience, I can say I highly recommend!!! Love this office!!!!! MD Singh is caring and understanding. does not just rush you and moves on to the next patient. Most of staff are nice. Easy to talk to you and explain things. You do not wait long for your apt. One time I was about 20 minutes and they were still able to see me. Definitely recommend for a primary care doctor!!! I just moved to Rancho from Texas, and I was very hesitant to find a new doctor. I found Dr. Singh on yelp and I am so glad I did. He was WONDERFUL and I would highly recommend him. He spent at least 15 minutes talking with me, did not rush me, addressed my concerns and helped to explain them so I understood better. The office staff was also very friendly and helpful. Great place! First off let me say that it takes A LOT for me to want to leave a review let alone give a detailed one but I feel that this office deserves the praise. I first visited Rancho Wellness a few years ago just for a routine check up during college....I recently found myself in a series of urgent care visits and was ultimately hospitalized. After bouncing around and being disappointed by another\\'s doctors office, I decided to come back to Rancho Wellness and I am SO HAPPY THAT I DID. The staff here is very professional and they process matters very quickly. After my current visit I had been taken off of work and needed disability/insurance claims taken care of and Brandy has been on it! I have been to other offices where I literally had to call over and over and harass them to make sure it was done. This was not the case at Rancho Wellness, my visit was toward the end of the work week and Brandy called me first thing Monday to let me know everything was processed and ready to go! I was so relieved! She has also been helpful in correcting any errors and providing any additional information to insure my claim for work is handled. She is very communicative, patient and gets things done fast! As far as medical care I felt very taken care of and my needs were met. I was worried that because I had not been there in years I would have to start over and gain rapport but they picked back up right where I left off and focused on what had changed with my health and brought me in. I was a also little worried that I would not be able to see the infamous Dr. Singh or Barbara Cruz but Heidi is just as knowledgeable and professional as her counterparts. I truly felt they were one big family working for the greater good of their patients. She made it clear that they work as a team and if i needed to reach out to her or any of the doctors I could. This made me feel at ease and that I was not just another patient being shuffled in. She wanted to get to the root of my health issues and asked questions that other doctors seemed to not even care about. She was very sympathetic about what I have been through and came up with a quick and workable plan to get me back up to health. I will be having my 2nd visit soon and I honestly cannot wait to get my blood work results and discuss more. I also noticed a friendly atmosphere amongst the women in the office. I have been to other offices where you could tell who did not like who and it is so awkward. My visit was right before Thanksgiving and the check out nurses asked me my plans and we had a good chat as I was getting situated after my visit. I also overheard them talking about lunch and how they have this thing where every Friday the doctors order out for team and they were telling me that they really enjoy it. I say that to say all of these things matter when running an establishment where you care for others and you never know who is watching! Thank you Heidi, Brandy and the rest of the staff at Rancho Wellness keep up the great work! I came here based on a number of recommendations and was not disappointed! The nurse practitioner, Barbara, was on time, thorough, and genuinely caring. I emailed the office through their patient portal at a later time, and a doctor got back to me right away! BEST DOCTORS AND SERVICE !! I scheduled an appointment for my husband and he loved it. Dr. Heidi Larson is very caring and listens to your needs. She answered all of our questions without rushing us out. it is such a relief knowing we have found the right doctors. I have been to many doctors and clinics, and I have always left each and every one of them frustrated with the lack of professionalism and empathy. It would take countless days to weeks just to get into see a doctor, and when you finally saw the doctor he or she were so busy with other patients that they had no time to concern themselves with your medical issues. When I called to make an appointment with Rancho Wellness, the secretary not only acknowledged my issues but she did her best to get me in to see the doctor within 24 hours. I got to the clinic, and I saw the doctor within 15 minutes of arrival. Most of the staff in this clinic actually care about you and work together as a team of professionals to get you in file and in a room to see an MD. The doctor I saw is Dr. Singh. My heart broke into tears, but they were tears of joy. I have finally found a doctor that cares enough about my health and educates me fully on it. He educated me on my condition brilliantly, he told me all of my options, he listened to my concerns with an empathetic ear, and before he left my room he made sure that all of my questions and concerns were resolved. I felt like a human walking out of the clinic, and I felt optimistic about my health. There were some rude people working here, but who cares! My doctor is amazing and totally worth the tiny trouble. Amazing people. So so helpful . Dr Ravinder Singh and his staff is the best. The doctors here prescribed me pain medication for the bulging disc in my low back . After a couple months of taking the medication I decided i wanted to stop because i did not like the way i felt and was having some side affects and changes in my mood. So i stopped taking it and became very sick. When i called the doctors office Barbara (Dr Singh partner) told the receptionist to tell me it sounded like i was going through withdrawals and I should have tapered off. Well i did not know i was suppose to taper off none of the doctors had told me that So I try to taper off and then when I message the doctor for more Dr Singh tells me to make an appointment to come in that the online portal is not the place to talk about it when his partner days earlier told me in the portal to taper off . In the past when I have made appointments with dr Singh it takes a couple weeks to see him because he is so busy .. I do not know why they could not give me more to taper off when it was their suggestion in the first place. So now I am going to get incredible Sick again (worst feeling of my life btw) I just think they need to set the proper expectations with their clients when it comes to side affects and how to get off meds instead of people trying to figure it out on their own.. I might have been upset talking to your receptionist but never yelled at her . I would never yell at someone for just passing a message to me so I apologize if she thought my bad attitude was yelling.i am not a bully your just trying to make yourself look better and please stop harassing me and calling me.. I have gotten 4 calls this morning .. I am not going to take this review down (below is a pic of them blowing me up this morning 909-483-7800 look it up 4 calls in 2 hours) You clearly do not want to help me since you blocked me from your online portal chat (pic below) I will find help somewhere else .. Hope you help out your current clients better than you did with me .. Take care I love rancho wellness, my family has been a wellness family for a good 10+ years and they are a fantastic caring group of people. They treat you with respect, they value your time, they care about you and your family, they listen and help in any way possible. The are a technology savvy organization and very efficient. I love that they have a special phone extension (#8) exclusively for \"Seniors\" that makes our lives easier! We love our doctors and recommend them to you ! You will love them too!! Eileen Dr. Singh is the best ! Attentive , thorough and prompt . We are always able to get an appointment and the staff are very quick at responding to calls and messages . DR Singh and his staff are the best. He is always looking for best care for his patients I have been going to him for over a decade and he always takes he times answers all my questions and treats me well. I have had a few medical problems over the years but he always keeps me tuned up and always addressing problems with solutions. I have been treated like family in his office and I would highly recommend if you need a physician call his office 1st. Thank you Dr. Singh for all your great health care over the years. Dr Singh i have been with now for about 10 years. He is a very good doctor takes time with his patients and is always trying his best to do whatever he can to make his patients as comfortable as possible. He is very knowledgeable very caring and even has a sense of humor. There is no one I trust more than him with any of my medical concerns. I grade him a a plus doctor. Larry Fry Best Primary Dr In town, Dr Barbara took her time to answer all the questions I have and update all the Lab work, staff was very helpful and I was able to walk out the office knowing i have finally find a good Doctor and I am in a good hand. Rancho Wellness is an amazing place. I had a sports injury and was told it would be one week before I could get an appointment to see my family doctor. I remember seeing great reviews for Rancho Wellness I called them instead. I was given an appointment for the next day. I was so impressed with the staff. They were all awesome and friendly and very helpful. My wait time was minimal. The PA Heidi was amazing! She was thorough and answered all my questions and concerns. I had the pleasent opportunity to meet Dr. Singh in the hallway. He was cordial and approachable. He invited me to give them feedback in the patient portal if there was anything I was not satisfied with. I do not think I have EVER been asked about a doctor for my feedback about their service. I am impressed.I was also impressed that they have a lab onsite. I did not have to run around to get blood drawn. They sent my prescriptions electronically to my pharmacy so getting my prescription was quick and simple.I love this place. I am so glad I found them. I highly recommend them! I look forward to going there often, um well not too often, I would rather stay healthy. :-) But when I do need doctor I am looking forward to coming back to Rancho Wellness! My family has been visiting Dr. Singh\\'s office for a number of years and we really want to recommend his office. He has wonderful staff who treats us like people instead of like a number. They are always smiling and ready to help us in our needs. They are so respectful and understand our needs and check up on us after a doctor\\'s visit just to make sure everything is going well with our health. In some cases they even give us last minute appointments even though they may have a busy schedule. We simply love all the staff and Dr. Singh, Barbara and Jessica from the Upland office as well. Our mind is set and we will never ever switch to another doctor.Regards ~ Gita & Madhu Bhatt I was referred to the office by patients that absolutely praise the office and especially Dr. Singh. The entire staff is friendly and positive, making for a pleasant experience and comfortable atmosphere. Dr. Singh was very quick to see me and thoroughly asked many questions along with providing detailed explanations for certain symptoms I had been experiencing. By the end of the appointment, I felt more knowledgeable about my health and trusted the medical opinion of Dr. Singh who was able to pinpoint very successful solutions to my concerns. He made sure to explain my new prescriptions, the side affects, and also provide me with samples to try before investing to make sure the medication would work for me. Dr. Singh shows genuine care towards his patients and I highly recommend Rancho Wellness to anyone in need of a highly experienced, trustworthy doctors office. Temporarily moved to Rancho Cucamonga and needed paperwork done as soon as possible. I found Rancho Wellness through yelp and Heidi and staff are awesome. Very professional and prompt. My old clinic in San Diego (Scripps Clinic)-Staff were horrible in relaying information and getting information to/from my previous Provider. They were not very educated compared to the staff at Rancho Wellness. Very disappointed because I did have a great relationship with my previous Provider since 2009.Heidi is great! She has attended to my needs and is so easy to talk to about any concerns I may be having. She is very professional, caring, and thorough. I am glad she is my current Provider! It will definitely suck when I have to move back though. But as of now, I am in good hands!'"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-RSO_9Qhl3S"
      },
      "source": [
        "# same as previous code to add polarity and subjectivity score\n",
        "df_sa_by_office['polarity'] = df_sa_by_office['expanded'].apply(lambda row: TextBlob (row).sentiment[0])\n",
        "df_sa_by_office['subjectivity'] = df_sa_by_office['expanded'].apply(lambda row: TextBlob (row).sentiment[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRwbBpK1yXaj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e778628c-847c-4d4a-86e1-0620df188089"
      },
      "source": [
        "# let's add the compound score \n",
        "# we can see that because the texts are much longer now, NLTK compound score does not work well.\n",
        "%time df_sa_by_office['NLTK_Compound'] = df_sa_by_office['expanded'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "\n",
        "df_sa_by_office.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 22s, sys: 3.32 s, total: 5min 25s\n",
            "Wall time: 5min 26s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>NLTK_Compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>0.283586</td>\n",
              "      <td>0.536893</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>I have been going here for several years now. ...</td>\n",
              "      <td>0.127707</td>\n",
              "      <td>0.476129</td>\n",
              "      <td>0.9998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>The baby center at St Bernardines is awesome. ...</td>\n",
              "      <td>0.249169</td>\n",
              "      <td>0.515605</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>I feel the need to post a review about Doctor ...</td>\n",
              "      <td>0.069367</td>\n",
              "      <td>0.473503</td>\n",
              "      <td>0.9970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Love Dr. Ali! She is very knowledgeable and is...</td>\n",
              "      <td>0.188873</td>\n",
              "      <td>0.621907</td>\n",
              "      <td>0.9895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   doctorID  rounded_rating  ... subjectivity  NLTK_Compound\n",
              "0         0             4.5  ...     0.536893         1.0000\n",
              "1         1             3.0  ...     0.476129         0.9998\n",
              "2         2             5.0  ...     0.515605         1.0000\n",
              "3         3             2.5  ...     0.473503         0.9970\n",
              "4         4             3.0  ...     0.621907         0.9895\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Eh-lQSdeS-dz",
        "outputId": "cb0c46d9-ad01-4b02-fcd3-ed986a0c050e"
      },
      "source": [
        "#each doctor's office's overall review polarity vs. rating\n",
        "df_sa_by_office.plot.scatter(x='polarity', y='rounded_rating', title= \"Polarity vs. Rating\");\n",
        "\n",
        "plot.show(block=True);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "_r15cWqTUZKx",
        "outputId": "2c52d813-03b7-4de6-e9c8-f43219886ee7"
      },
      "source": [
        "#each doctor's office's overall review polarity vs. rating\n",
        "df_sa_by_office.plot.scatter(x='subjectivity', y='rounded_rating', c='green', \n",
        "                             title= \"Subjectivity vs. Rating\");\n",
        "\n",
        "plot.show(block=True);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "-YtF1qTBidBU",
        "outputId": "db5c85c8-024d-4a06-f561-d529d249cf4f"
      },
      "source": [
        "#examining high subjectivity reviews for 5-star and 1-star rated doctors\n",
        "#leaving aside the outliers, we are examining reviews with a subjectivity of 0.69 - 0.71 \n",
        "\n",
        "df_sa_by_office.loc[((df_sa_by_office['subjectivity']  > 0.69) & (df_sa_by_office['subjectivity']  < 0.71))\n",
        "& ((df_sa_by_office['rounded_rating']  == 5.0) | (df_sa_by_office['rounded_rating'] == 1.0))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>NLTK_Compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>91</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Dr. Ransom is awesome! I am making appt with h...</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>0.704167</td>\n",
              "      <td>0.9320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>171</td>\n",
              "      <td>5.0</td>\n",
              "      <td>They are friendly, fast, and knowledgeable. I ...</td>\n",
              "      <td>0.385417</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.8979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>562</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Dr. Owais A Zaidi is by far the most heartless...</td>\n",
              "      <td>-0.004545</td>\n",
              "      <td>0.703030</td>\n",
              "      <td>-0.6047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>834</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Tamara has many tools in her delightful room a...</td>\n",
              "      <td>0.486417</td>\n",
              "      <td>0.708778</td>\n",
              "      <td>0.9969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1258</th>\n",
              "      <td>1331</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Nicole was AWESOME!!! I had my session earlier...</td>\n",
              "      <td>0.560417</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.9477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1277</th>\n",
              "      <td>1350</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Dr. Le has been my HIV doc for over 3 years no...</td>\n",
              "      <td>0.332143</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.9601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>1686</td>\n",
              "      <td>5.0</td>\n",
              "      <td>I have been seeing Dr.singhvi now for about 2 ...</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.695427</td>\n",
              "      <td>0.9813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1824</th>\n",
              "      <td>1914</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Pretty funny finding a Dr on yelp , but anythi...</td>\n",
              "      <td>0.195455</td>\n",
              "      <td>0.697727</td>\n",
              "      <td>0.9911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1955</th>\n",
              "      <td>2047</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Update: The gal on the phone had said Dr. Swee...</td>\n",
              "      <td>0.392526</td>\n",
              "      <td>0.692812</td>\n",
              "      <td>0.9962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2042</th>\n",
              "      <td>2139</td>\n",
              "      <td>5.0</td>\n",
              "      <td>I have had two surgeries here and both times t...</td>\n",
              "      <td>0.380556</td>\n",
              "      <td>0.692222</td>\n",
              "      <td>0.9764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2249</th>\n",
              "      <td>2353</td>\n",
              "      <td>1.0</td>\n",
              "      <td>This Dr office is a joke you have to wait a mi...</td>\n",
              "      <td>-0.009615</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>-0.8192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      doctorID  rounded_rating  ... subjectivity  NLTK_Compound\n",
              "88          91             5.0  ...     0.704167         0.9320\n",
              "165        171             5.0  ...     0.700000         0.8979\n",
              "519        562             1.0  ...     0.703030        -0.6047\n",
              "781        834             5.0  ...     0.708778         0.9969\n",
              "1258      1331             5.0  ...     0.700000         0.9477\n",
              "1277      1350             5.0  ...     0.692857         0.9601\n",
              "1602      1686             5.0  ...     0.695427         0.9813\n",
              "1824      1914             5.0  ...     0.697727         0.9911\n",
              "1955      2047             5.0  ...     0.692812         0.9962\n",
              "2042      2139             5.0  ...     0.692222         0.9764\n",
              "2249      2353             1.0  ...     0.700000        -0.8192\n",
              "\n",
              "[11 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "DSdVCrPekZmc",
        "outputId": "f5105e08-b3a1-488a-c28a-36356a17d4b4"
      },
      "source": [
        "#high subjectivity reviews from a 5-star rated doctor's office\n",
        "\n",
        "df_sa_by_office['expanded'][781]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tamara has many tools in her delightful room and being, and I am particularly fond of her crystal bowls. The only thing missing with this wonderful light worker are her wings. Wait. There they are. Had a beautiful experience. Completely work every minute.Feeling so amazing since then. I will definetly be returning. Come see Tamara in her beautiful room relax and get healed Wonderful! You will love it. She is very gifted Tamara is an Exceptional practitioner and Extraordinary Artist. When you combine the healing arts in a unique environment, you get a therapeutic experience that has to be enjoyed to be believed!!! By merging Massage, Reiki and Sound bath she has come up with the ultimate in relaxation. I highly recommend it! Tamara is the best reiki healer I have ever had and I have had reiki healings for over 20 years. I recommend her highly. THE BEST! Tamara is pure magic.. such a loving soul. The experience of having her healing energy either through her sound baths or one on one Reiki is a true gift of the Spirit.'"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "WnIvLRpDkuAM",
        "outputId": "ef3499fd-8fcf-4a5e-b6dc-388379b5e525"
      },
      "source": [
        "#high subjectivity reviews from a 1-star rated doctor's office\n",
        "\n",
        "df_sa_by_office['expanded'][519]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Dr. Owais A Zaidi is by far the most heartless, souless, egotistical ICU doctor I have ever encountered. He will let a patient lay in the bed of the hospital and insist that there is no level of higher care needed, all while the national ranking of San Antonio is one of the worst in the country. San Antonio really needs to be investigated for infection. The city of Upland and all of the Inland Empire deserve better care than they are getting from this outdated hospital. Just Google San Antonio's national ranking and see for yourself. Patient beware! Terrible billing practices! No bill sent after adjustments for insurance asking for payment until a collections letter was received in the mail saying the client has given all the extension of time they feel is justified. The collection company is trying to collect interest on top of the principal when no statements have been received. They just sell the account to a collections agency and move on potentially damaging peoples credit.\""
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "gIRWlOr3m2Yy",
        "outputId": "30d43fd4-3b1a-4ae1-b869-5178a7e7f253"
      },
      "source": [
        "#examining low subjectivity reviews for 5-star and 1-star rated doctors\n",
        "#leaving aside the outliers, we are examining reviews with a subjectivity of 0.25 - 0.30 \n",
        "\n",
        "df_sa_by_office.loc[((df_sa_by_office['subjectivity']  > 0.20) & (df_sa_by_office['subjectivity']  < 0.30))\n",
        "& ((df_sa_by_office['rounded_rating']  == 5.0) | (df_sa_by_office['rounded_rating'] == 1.0))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>NLTK_Compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>393</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Very relaxing environment. Came to get a foot ...</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.7501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>412</td>\n",
              "      <td>1.0</td>\n",
              "      <td>This Dr.'s Office is so unprofessional! Today ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.283333</td>\n",
              "      <td>-0.7974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>556</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Approximately 3 years ago I got a breast lift ...</td>\n",
              "      <td>-0.069048</td>\n",
              "      <td>0.236081</td>\n",
              "      <td>-0.3291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>564</td>\n",
              "      <td>1.0</td>\n",
              "      <td>I am trying to change doctor's and my new medi...</td>\n",
              "      <td>0.093074</td>\n",
              "      <td>0.257468</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644</th>\n",
              "      <td>692</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Primecare of Chino Valley manages my grandmoth...</td>\n",
              "      <td>0.019839</td>\n",
              "      <td>0.273744</td>\n",
              "      <td>-0.6204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>743</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Had to switch Dr.'s after I was assigned him f...</td>\n",
              "      <td>-0.108333</td>\n",
              "      <td>0.238889</td>\n",
              "      <td>-0.7558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1326</th>\n",
              "      <td>1399</td>\n",
              "      <td>5.0</td>\n",
              "      <td>My kids have been seeing Dr. Aljilani for the ...</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1377</th>\n",
              "      <td>1451</td>\n",
              "      <td>5.0</td>\n",
              "      <td>I just recently had a consultation with Dr. Ba...</td>\n",
              "      <td>0.062121</td>\n",
              "      <td>0.251515</td>\n",
              "      <td>0.9377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403</th>\n",
              "      <td>1478</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Do not go here. I have to go here because my i...</td>\n",
              "      <td>-0.071429</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.3182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>1484</td>\n",
              "      <td>1.0</td>\n",
              "      <td>My wife has worked in the medical field for ma...</td>\n",
              "      <td>-0.068750</td>\n",
              "      <td>0.289583</td>\n",
              "      <td>-0.9071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>1737</td>\n",
              "      <td>1.0</td>\n",
              "      <td>If you want nothing but common drug therapy an...</td>\n",
              "      <td>0.026852</td>\n",
              "      <td>0.286420</td>\n",
              "      <td>-0.9958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1680</th>\n",
              "      <td>1766</td>\n",
              "      <td>1.0</td>\n",
              "      <td>My husband went to his Primary care doctor at ...</td>\n",
              "      <td>0.113920</td>\n",
              "      <td>0.291193</td>\n",
              "      <td>0.4588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1975</th>\n",
              "      <td>2067</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Dr Slagle has really helped me with both nutri...</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.287500</td>\n",
              "      <td>0.8173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2142</th>\n",
              "      <td>2244</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Office staff, medical care, and office procedu...</td>\n",
              "      <td>-0.106771</td>\n",
              "      <td>0.263542</td>\n",
              "      <td>0.3682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2297</th>\n",
              "      <td>2402</td>\n",
              "      <td>5.0</td>\n",
              "      <td>On-site medical services to the employees and ...</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>0.3612</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      doctorID  rounded_rating  ... subjectivity  NLTK_Compound\n",
              "373        393             5.0  ...     0.266667         0.7501\n",
              "390        412             1.0  ...     0.283333        -0.7974\n",
              "514        556             1.0  ...     0.236081        -0.3291\n",
              "521        564             1.0  ...     0.257468         0.0000\n",
              "644        692             1.0  ...     0.273744        -0.6204\n",
              "695        743             1.0  ...     0.238889        -0.7558\n",
              "1326      1399             5.0  ...     0.250000         0.0000\n",
              "1377      1451             5.0  ...     0.251515         0.9377\n",
              "1403      1478             1.0  ...     0.214286         0.3182\n",
              "1409      1484             1.0  ...     0.289583        -0.9071\n",
              "1653      1737             1.0  ...     0.286420        -0.9958\n",
              "1680      1766             1.0  ...     0.291193         0.4588\n",
              "1975      2067             5.0  ...     0.287500         0.8173\n",
              "2142      2244             1.0  ...     0.263542         0.3682\n",
              "2297      2402             5.0  ...     0.240000         0.3612\n",
              "\n",
              "[15 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "j6Ccl0uVm9Or",
        "outputId": "a7cb09f0-57b3-4e4b-c99d-dd2d4c05bcc5"
      },
      "source": [
        "#low subjectivity reviews from a 5-star rated doctor's office\n",
        "\n",
        "df_sa_by_office['expanded'][2297]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'On-site medical services to the employees and visitors of the Molycorp Minerals mine. Staff is available 24-hours a day (excluding some holidays). Services provided include: first-aid, urine drug screening and testing, breath alcohol testing, respirator mask fit testing and spirometry/pulmonary function testing, audiology exams etc. There is always an ICEMA (San Bernardino County EMS) accredited, California state licensed paramedic on staff to respond to medical emergencies within the complex.'"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "cN9bcqBin36s",
        "outputId": "ba6114ca-8e1d-4387-bdf4-87f0514c927e"
      },
      "source": [
        "#low subjectivity reviews from a 1-star rated doctor's office\n",
        "\n",
        "df_sa_by_office['expanded'][514]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Approximately 3 years ago I got a breast lift tummy tuck and fat transfer to the buttocks. I paid $17,000 when the nurse put my girdle on she did not explain anything to me so the girdle left me dog ears on the sides of my hips and Dr.Smiley did not transfer all the fat to my buttocks my left side was a lot smaller than the right side so I went back to Dr.Smiley and he said I would now have to pay $3900 to fix it.So now I am in $20,900 so I went in for the second time I have waited a year and a half and the results are actually worse so now I have to buy long shirts to cover my butt because you can see a really big indentation on my left side of my butt. So I had went in today 3/10/2017 and waited for 4 hrs and when I had called previously they promised me $3900 to fix the problem once and for all but when I went in the price changed to $7200 .I could not believe that Dr.Smiley would even charge me after all this .Then they sent me home after waiting 4 hrs and said someone would call me and never did ..'"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "9VCbh7tEXIBW",
        "outputId": "98e5706a-88e9-48de-d5fb-2590c3e31f22"
      },
      "source": [
        "#is there any relationship between polarity and subjectivity?\n",
        "df_sa_by_office.plot.scatter(x='polarity', y='subjectivity', c='orange', \n",
        "                             title= 'Polarity vs. Subjectivity');\n",
        "\n",
        "plot.show(block=True);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "JL2HFaQrW4X3",
        "outputId": "c78a28bd-c770-459c-f15f-a566e2d007da"
      },
      "source": [
        "#each doctor's office's overall review compound score vs. rating\n",
        "df_sa_by_office.plot.scatter(x='NLTK_Compound', y='rounded_rating', c='purple', \n",
        "                   title= 'NLTK Compound vs. Rating');\n",
        "\n",
        "plot.show(block=True);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDSTjd0LGV88"
      },
      "source": [
        "df_sa_by_office.to_pickle('SA_by_business')\n",
        "df_sa_by_office.to_csv('SA_by_business.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0eQj_BSUiId"
      },
      "source": [
        "# Task 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6v-BWQQz-zL"
      },
      "source": [
        "LDA - Siyu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2i8SdCLy1jH"
      },
      "source": [
        "## Data Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "HRVE235505bs",
        "outputId": "6f4cc246-eddd-4dcd-e889-485095e514aa"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 327 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 419 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 471 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 604 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 655 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 696 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 706 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 747 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 788 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 798 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 839 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 890 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 942 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 972 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 8.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 41.8 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.20.0\n",
            "  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 177 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=0b454430fd645e703e9499fdc611a51a475839a7cec02c27112c29fa9bb330c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.16 numpy-1.21.2 pandas-1.3.2 pyLDAvis-3.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1k0Rmb_v0t3"
      },
      "source": [
        "from nltk import FreqDist\n",
        "import string\n",
        "import gensim\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim import models\n",
        "from gensim.models import CoherenceModel, LsiModel\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIP6LLBrukSc"
      },
      "source": [
        "#df_review_final = pd.read_json('/content/drive/MyDrive/Colab_Notebooks/review_and_final.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCSvTjGI-tvd",
        "outputId": "8639c2a1-b3de-4bcb-bbf4-be09766bad94"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df_review_final = pd.read_pickle('prepared_data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRB8snHJvArM",
        "outputId": "9dd9c876-578b-473a-feed-cce22f535e5c"
      },
      "source": [
        "# use FreqDis() to find the frequency disctibution for each token\n",
        "%time fdist_reviews= FreqDist([token for sublist in df_review_final['final'].values for token in sublist])\n",
        "print('The total number of tokens:',fdist_reviews.N())\n",
        "print('The number of unique tokens:', len(fdist_reviews))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.52 s, sys: 30.4 ms, total: 2.55 s\n",
            "Wall time: 2.55 s\n",
            "The total number of tokens: 2832965\n",
            "The number of unique tokens: 41542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nfH4N7Zv_lD"
      },
      "source": [
        "fdist_reviews_list = sorted(fdist_reviews.items(), key=lambda x:x[1], reverse= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biOWUVQAwIr0"
      },
      "source": [
        "token_list = [token[0] for token in fdist_reviews_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDGGpAz0wMsr",
        "outputId": "4afc996d-8a0c-4d48-e436-3977adce4b43"
      },
      "source": [
        "# let's see if any tokens are not characters \n",
        "none_char_token = [token for token in token_list if all(j.isdigit() or j in string.punctuation for j in token)]\n",
        "\n",
        "print(len(none_char_token))\n",
        "# we can see many of these tokens are either count number, or time or date. An NER tagger would have captured that.\n",
        "# You can explore these entity types in your final project, and to see if you can these to improve your feature engineering. \n",
        "print(*none_char_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "839\n",
            "... !!! .... !!!! ..... !!!!! ??? ...... !!!!!! ???? *** !!!!!!! ?!?! ....... ?!? !). \"... --- ...\" :-) !!!!!!!! )... **** ?\". !), !.. ?!! !?! ????? .). ?). !!!!!!!!! \".. $$$ ??!! ........ ....\" !... ?!!! \".... (?) .!! !\". !!!!!!!!!! ?!\" ...( ?\", ***** ......... ??! .!!! ...) !!\" ?... ).... ..\" !!!) !!) ,,, ??\" :). .\". .......... !!. +++ .*** ?????? ?!?!?! ?!?!? !!!\" $$$. !?!? $$. !!!!!!!!!!! ).. ****** ??!!! \"). \"..... ..( !!.. :// .** ??) ..!! !!). ?!) \"!!! ??????? ;-) !!!!!!!!!!!!!! ..., ...? ___ ,,,, !!!!!!!!!!!!! !!?? ,... ---- !!!. ........... \"!! .., )!! \"), $$$$ !?? !!!!!!!!!!!!!!! !!!!!!!!!!!! ???\" )!!! ?), ).\" !.... ???!!! !?\" ............. !!? ,.. ???!! ?.... '... ???????? !** !!!' )-- ----- '.. ...: ?!). !??? !!, ?!!!!! ?!?\" ,..... ???) !\", ...!!! ?!?? !!!??? .\"... ....) ?!!!! !*** .\"( ____ !!!... :)! ???! :(. ............ ?.. .....( :-( ...). ?!.. !?!?! ??!!!! !!!!!!!!!!!!!!!!!! ...\". !!!!!!!!!!!!!!!!!!! ..! !!!!\" :-/ )..... \"!!!! .--- .-- +-- ??... ............... .------- .), !!!!) $$$$$$ ....? (???) ....$ !!!, !!!). ,,,,, ...$ .\", ?\"\" .\"\" (?), ?!... \"?? ....( $... ...! !!!!!!!!!!!!!!!!! ...& ...??? :))) .!!!!! %). !...... ******** !:) !!!? ++++ ...), ..? ????) $$$$. (~$ !!!!**** ....\". !?!! !!... ?!?!?!? .!!!!!!! \"\"\"\" \"...... )!!!!! :/. ?????????? %... !!!!!!!!!!!!!!!! ??!!?? \"??? !!!!. .?? .**** ..!!! \".\" !!!!!!!!!!!!!!!!!!!!!! (?). '!! ******* ...' ...!!!!! ?!?!?!?!?!??! $$.-- !)... :)\" !!!!!. ....! ****. @$$ :'( .!!!!!! !\"), ?!!!!!! !!!- :)) ,.... (?!) .:) !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! !)( .:: !!??! !!- ):- --------- !!!.... ......\" !!* !!!!?? ...*** !--- (-: ?!?!) ??!!. \"?! ........................... :)- ...'. ------------------------ !;) !'. :......... ?!?!?!?!? !#+*$@?!$ ?!), ?!!? (... _______ .****** !\".... --!!!!!! $$$! .../ ?!?!?!?!?! -... ------ ??!? \"\"\" .??? ...- ,\"... +++++ !!!!... ..). ;). ?\"...\" +++++++++ .............. .:( .?\" ((( ))) !****** .,, !,, !!' ......! !!??? !!), ????). .***** ???.. %#* ...,\" !); ......;) !!!!???). ?'. :)... \"?!?! ?!!' .!!!! ?!??? !(: )\". .\") .....\" !!!.. .!\" $#@&) _____________________________________________10 ...!! **. _____ '.... ..) $$$$!!!! ?\".. (!) @@@ (!), \"** !).... !## .,.. $$$$$. *****. ,-- \".- !!?\" .*********************************** *********************************** ?!\". ?!?!. !!!...... ***. -------------------------------- .************************ ....'. *********************** \".( :-)! \\];, :)**** ::))). /.\\ .!. .....\"\" !\".................... ????. ---! .----------------------------------------------------------------------------------------------- ?!?!?\" ..!!!! @#$ ^%^$%^$ :))))))) ;)] ?...\" !!!~ @!*# ).- !!!!??!!!??! ???... ++++++++++. $$$$$$. ....?! ?!?* :,) -#' :)). :(((. :)* ...???!!!! %.... !......... :,( @$$. :,)!! .(. ...................................... !!!!!!!!!!!!!!!!!!!! .....!!! ...@ ?!!!!!!! ),,, ......., !\"** ***( )***** !#! :-- \"?!??! #%* ................................................................................................................................................................................................................................................................. ???..... !-- ?\") .......( !)...... !\".. !!!)... ,\". ...?) .!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! !!!!!!!!( ;-( ???!!!!! ')!!! =^.^= ,...... !******************** !**** +), ??, !?... ---------------------------- ----------------------- !!*** ??!??!?! ].\" !!!!!!!!***** ?); .?) .......- %!! !!!???!!! ......\", ....:/ :).. ;).... !== ????, --, ***..... ****..... ....). ??????!! ...?! *****, (((( ))))!!! ~~* --& ****...\" ^^^ ?????!?!? \".? @$@#!. @$# $+-+ 0_0 :-/, +++++! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! +%. \"?,\" !...\". @$! ,......... ???!!!??? ????!!! :\"\"\" !!!!? $$) ()!! $$, ****\" .....=) =). %*@-$. !*&% $$!!! ;);) .???? :*** !!!!!) ).., .......) !!!!.... %^$# ////// ???. ??!\" ...\"..... !!!).... ???( ................. ****!!!!!!!!!! !!!!!!!!!!!****** &@$$! '??\" ..~~ $*# ....:-( )???? *). ..................... ....................... .,... ::. ?!?!! ,.,.,. $$$, !:-) (!!), ;)) ;))). $$; $#@ &^*. :((( !!!!!!~~~~ &&& (!!) ...?). !!!!)... (?!?) !!!!). !!!!,, !!!????? !?) ...?\" .?. *$%@ @$% --------------------- -:) ????????????? ++++++++++ .,( .,) !?)) \"\".. !)* ???* !!!:) .---- !??\" ....,, !.!.! \"!!!!!!! *#% \"!!!... !!!!!!!!!!!!!!!!!!!!!!!!!! ??).. !?!?? .\"................ \"!... .\"...... \"&\" **!!!!! :....... !!!( ??). !!???!? ?!.!.! ).( *****... *****************************************************- (?.) ?!& \"?, ?!?!), ??!?? !!!.\". !!!!, ...-$$$$$. ............?! ).......... ..................................... ...**** #$@ ++. :)!! !!!: +++. \",\" #.... !??, !!??, !?, ???# .!? \"...( +++++++ :-\\ ****!! ....************* ???!\" ??????????? ++++++++ (???). )-: /(\",\")\\ (:) :~.~~ ).' :\"( === )\"..... ?!. :):) :-[!!!!! .// ????!!!! \"---- !!: ^&%!!! \"-- **% **#% !!!!!!) -&- ??( ...)- !!!!' \"......... (??) ***? -.-... ?..\" .\".. ????!!!!) ?'\" ?!?!?? ???} .?! !\"). :... .,. ..!!.. ..!!!... ...???... ?????????*** !....... \"!!!!! !!)- ,........... )). ?!'. ;'( )........ ????!! .(?) ....!! ::::: :::: !!!!!\" $%#@ ??????) !!!!!!!!. @:( .)* $$$$$$$$. ????????? ?!?) !!!!!\". -.- !!********* +... -------- %#. !?)... ?\"\"\" :)))) %?! $!@# ........,,..................,. ......???? **\" *********** ******************************************* ,\"# .)... $&@?!!!!!!!!! ?!!!); !!!!!!( +/- --\" !): ...?? $$$$$ ~~~~ ..:( \"!!!. =-) \"?\" !\"\" **, .); +!! .'... ..$ )...... '..... (??? ;-)))) ?!!?? \"....... ......, ------------------------------------- \"....\" .************************** ************************** \".' !!^ ..?! \"??!! ???!? $$!!!! .-------------------------------------------------------- ?????!!!!! ~!!! ..* ------------------------------------------------ ***?!?! !!!)) ?????} .....) ,,,,,,,!!!!!! ,!!! ??!?! !!..... -@$$ ?!.\" !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. ???!!!! ?????! ????\" ???**!! **( )** !\"--- !..... \"?. !?&@$?! :-). !!!--- !!!!!!??? ?!, ?!?!.... !!| ????), .## %##% !?!?!?? !!!!!????? !!!?? -********** \"\". (/(\",\")\\) $&@/? ................................* .............................................. ?\".... ;-).... ?!!\" ,,!!! $&* $$! #-- .....$ :-/) .(\" .----- ????\". ...,, ???!?! !?$!?!!!!? !!!..... ..\". #@!@. ...***** #). !?!?!?! !!!!!!!!!!) ......*** ***- ***! ................... ????????\" **! ::: :(( -.... *&!! !!!!!!!). !?!?!??? !?!???? !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! ?!?!?!?!?!?! ..& !!?! ---------- :(, ?\"- (??). ...[ ::) !\") (-) ----------- ,?! .\"- ....), ..,, !+#$#$ #$&@ -???$ )!* !&@#. ....!!! $&@! .------ !!?!? .......?!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCMNXQWIwfMw"
      },
      "source": [
        "cleaned_token_list = []\n",
        "for token_list in df_review_final['final']:\n",
        "    tmp_list = []\n",
        "    for token in token_list:\n",
        "        if token not in none_char_token:\n",
        "            tmp_list.append(token)\n",
        "    cleaned_token_list.append(tmp_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeECIikPwwl9",
        "outputId": "a44e6ec7-15ca-4ef6-cbc4-5226794ec07f"
      },
      "source": [
        "# after removing digits and punctuations, get the empty list\n",
        "for i, token_list in enumerate(cleaned_token_list):\n",
        "  if len(token_list) == 0:\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1536\n",
            "11970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9sOPS-qxMfE"
      },
      "source": [
        "# remove empty list from df\n",
        "df_review_final = df_review_final.drop([1536, 11970], axis=0)\n",
        "df_review_final = df_review_final.reset_index().drop(['index'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCNYDgSR3uDJ"
      },
      "source": [
        "weird_list = []\n",
        "cleaned_list = []\n",
        "for i, token_list in enumerate(df_review_final['final']):\n",
        "  tmp_list = []\n",
        "  for word in token_list:\n",
        "    if '�' in word:\n",
        "      weird_list.append(word)\n",
        "    else:\n",
        "      tmp_list.append(word)\n",
        "  cleaned_list.append(tmp_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HlscgX35iyg"
      },
      "source": [
        "df_review_final['CleanedFinal'] = cleaned_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "gaMyrj0A5r_N",
        "outputId": "0a403bd7-5a5b-472f-df48-617befdcea8e"
      },
      "source": [
        "df_review_final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>username</th>\n",
              "      <th>rating</th>\n",
              "      <th>date_of_review</th>\n",
              "      <th>review_content</th>\n",
              "      <th>rounded_rating</th>\n",
              "      <th>expanded</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>final</th>\n",
              "      <th>CleanedFinal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Whitney W.</td>\n",
              "      <td>5</td>\n",
              "      <td>5/27/2021</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>After seeing 4.5 stars reviews on a family pra...</td>\n",
              "      <td>[After, seeing, 4, ., 5, stars, reviews, on, a...</td>\n",
              "      <td>[(After, IN), (seeing, VBG), (4, CD), (., .), ...</td>\n",
              "      <td>[After, see, 4, ., 5, star, review, on, a, fam...</td>\n",
              "      <td>[see, star, review, family, practice, rancho, ...</td>\n",
              "      <td>[see, star, review, family, practice, rancho, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Kristin R.</td>\n",
              "      <td>5</td>\n",
              "      <td>9/30/2015</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I just switched this month to Rancho Wellness ...</td>\n",
              "      <td>[I, just, switched, this, month, to, Rancho, W...</td>\n",
              "      <td>[(I, PRP), (just, RB), (switched, VBD), (this,...</td>\n",
              "      <td>[I, just, switch, this, month, to, Rancho, Wel...</td>\n",
              "      <td>[switch, month, rancho, wellness, base, referr...</td>\n",
              "      <td>[switch, month, rancho, wellness, base, referr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Allyson F.</td>\n",
              "      <td>5</td>\n",
              "      <td>2/24/2017</td>\n",
              "      <td>I love Rancho wellness, they are very  organiz...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>I love Rancho wellness, they are very organize...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, are, very...</td>\n",
              "      <td>[(I, PRP), (love, VBP), (Rancho, NNP), (wellne...</td>\n",
              "      <td>[I, love, Rancho, wellness, ,, they, be, very,...</td>\n",
              "      <td>[love, rancho, wellness, organize, professiona...</td>\n",
              "      <td>[love, rancho, wellness, organize, professiona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Brian J.</td>\n",
              "      <td>5</td>\n",
              "      <td>10/12/2016</td>\n",
              "      <td>Large waiting room and welcoming  staff. Dr. S...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>Large waiting room and welcoming staff. Dr. Si...</td>\n",
              "      <td>[Large, waiting, room, and, welcoming, staff, ...</td>\n",
              "      <td>[(Large, JJ), (waiting, VBG), (room, NN), (and...</td>\n",
              "      <td>[Large, wait, room, and, welcoming, staff, ., ...</td>\n",
              "      <td>[large, wait, room, welcoming, staff, singh, p...</td>\n",
              "      <td>[large, wait, room, welcoming, staff, singh, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Stephani P.</td>\n",
              "      <td>5</td>\n",
              "      <td>6/2/2015</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>My family and I have been going to Dr. Singh f...</td>\n",
              "      <td>[My, family, and, I, have, been, going, to, Dr...</td>\n",
              "      <td>[(My, PRP$), (family, NN), (and, CC), (I, PRP)...</td>\n",
              "      <td>[My, family, and, I, have, be, go, to, Dr, ., ...</td>\n",
              "      <td>[family, singh, year, actually, follow, anothe...</td>\n",
              "      <td>[family, singh, year, actually, follow, anothe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52684</th>\n",
              "      <td>52686</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>10/21/2015</td>\n",
              "      <td>I live in LA but was on a trip in Palm Springs...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I live in LA but was on a trip in Palm Springs...</td>\n",
              "      <td>[I, live, in, LA, but, was, on, a, trip, in, P...</td>\n",
              "      <td>[(I, PRP), (live, VBP), (in, IN), (LA, NNP), (...</td>\n",
              "      <td>[I, live, in, LA, but, be, on, a, trip, in, Pa...</td>\n",
              "      <td>[live, trip, palm, springs, urgent, care, cold...</td>\n",
              "      <td>[live, trip, palm, springs, urgent, care, cold...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52685</th>\n",
              "      <td>52687</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>9/9/2015</td>\n",
              "      <td>These people are crooks!!!So to get started th...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>These people are crooks!!!So to get started th...</td>\n",
              "      <td>[These, people, are, crooks, !!!, So, to, get,...</td>\n",
              "      <td>[(These, DT), (people, NNS), (are, VBP), (croo...</td>\n",
              "      <td>[These, people, be, crooks, !!!, So, to, get, ...</td>\n",
              "      <td>[people, crooks, !!!, get, start, charge, copa...</td>\n",
              "      <td>[people, crooks, !!!, get, start, charge, copa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52686</th>\n",
              "      <td>52688</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>11/13/2014</td>\n",
              "      <td>Sadly, I think the previous comments and revie...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>Sadly, I think the previous comments and revie...</td>\n",
              "      <td>[Sadly, ,, I, think, the, previous, comments, ...</td>\n",
              "      <td>[(Sadly, RB), (,, ,), (I, PRP), (think, VBP), ...</td>\n",
              "      <td>[Sadly, ,, I, think, the, previous, comment, a...</td>\n",
              "      <td>[sadly, think, previous, comment, review, true...</td>\n",
              "      <td>[sadly, think, previous, comment, review, true...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52687</th>\n",
              "      <td>52689</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>7/23/2014</td>\n",
              "      <td>I've been waiting for just over an hour now an...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I have been waiting for just over an hour now ...</td>\n",
              "      <td>[I, have, been, waiting, for, just, over, an, ...</td>\n",
              "      <td>[(I, PRP), (have, VBP), (been, VBN), (waiting,...</td>\n",
              "      <td>[I, have, be, wait, for, just, over, an, hour,...</td>\n",
              "      <td>[wait, hour, call, back, two, people, since, f...</td>\n",
              "      <td>[wait, hour, call, back, two, people, since, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52688</th>\n",
              "      <td>52690</td>\n",
              "      <td>2606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>12/6/2011</td>\n",
              "      <td>I went here for a broken foot, because they ad...</td>\n",
              "      <td>1.5</td>\n",
              "      <td>I went here for a broken foot, because they ad...</td>\n",
              "      <td>[I, went, here, for, a, broken, foot, ,, becau...</td>\n",
              "      <td>[(I, PRP), (went, VBD), (here, RB), (for, IN),...</td>\n",
              "      <td>[I, go, here, for, a, broken, foot, ,, because...</td>\n",
              "      <td>[broken, foot, advertise, state, art, digital,...</td>\n",
              "      <td>[broken, foot, advertise, state, art, digital,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52689 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ...                                       CleanedFinal\n",
              "0               0  ...  [see, star, review, family, practice, rancho, ...\n",
              "1               1  ...  [switch, month, rancho, wellness, base, referr...\n",
              "2               2  ...  [love, rancho, wellness, organize, professiona...\n",
              "3               3  ...  [large, wait, room, welcoming, staff, singh, p...\n",
              "4               4  ...  [family, singh, year, actually, follow, anothe...\n",
              "...           ...  ...                                                ...\n",
              "52684       52686  ...  [live, trip, palm, springs, urgent, care, cold...\n",
              "52685       52687  ...  [people, crooks, !!!, get, start, charge, copa...\n",
              "52686       52688  ...  [sadly, think, previous, comment, review, true...\n",
              "52687       52689  ...  [wait, hour, call, back, two, people, since, f...\n",
              "52688       52690  ...  [broken, foot, advertise, state, art, digital,...\n",
              "\n",
              "[52689 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwW49H_cy5za"
      },
      "source": [
        "## Bigrams and id2word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ1aIUZUy8En"
      },
      "source": [
        "sent = df_review_final['CleanedFinal'] # Cleaned Tokens is already a list of list of words\n",
        "\n",
        "# train a bigram model from the stream of sentences. We set min_count as 25 here. \n",
        "bigram = Phrases(sent, min_count=25)\n",
        "\n",
        "# export the trained bigram model\n",
        "# Use this instead of Phrases because we do not need to update the bigram statistics any more.\n",
        "bigram_phraser = Phraser(bigram) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ1rdLYbzdsb"
      },
      "source": [
        "texts = bigram_phraser[sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwbxJjX4zgK1",
        "outputId": "3214e7f4-ea1a-47da-aab3-2daf539bd6ef"
      },
      "source": [
        "# let's first created a list of bigram phrases\n",
        "bigrams = []\n",
        "for text in texts:\n",
        "  for word in text: \n",
        "    if '_' in word:\n",
        "      bigrams.append(word)\n",
        "\n",
        "bigrams [:5] # see five bigrams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['super_nice',\n",
              " 'rancho_wellness',\n",
              " 'sister_law',\n",
              " 'planned_parenthood',\n",
              " 'birth_control']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp73k8M3zue2"
      },
      "source": [
        "id2word = gensim.corpora.Dictionary(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3woVu-7zw1X",
        "outputId": "5f3d5eef-2f63-4656-fa68-654e55446f5e"
      },
      "source": [
        "# filter very frequent and rare tokens \n",
        "# filter out tokens that appear in less than 50 documents (0.1% of our corpus documents), more than 0.5 documents (total corpus size, not absolute)\n",
        "id2word.filter_extremes(no_below=50, no_above=0.5) \n",
        "\n",
        "# create a corpus, which here is the BOW bigram model, same as the doc_term_frequency we discussed early  \n",
        "corpus = [id2word.doc2bow(doc) for doc in texts]\n",
        "\n",
        "# corpus as list of list with tuples of (token_id, frequency) example\n",
        "print(*corpus[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 1) (1, 1) (2, 1) (3, 1) (4, 1) (5, 1) (6, 1) (7, 2) (8, 1) (9, 1) (10, 1) (11, 1) (12, 1) (13, 1) (14, 1) (15, 2) (16, 1) (17, 2) (18, 1) (19, 1) (20, 2) (21, 1) (22, 1) (23, 1) (24, 1) (25, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvJpiN1co6k-"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(corpus, open('corpus.pkl','wb'))\n",
        "pickle.dump(id2word, open('id2word.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeEy3jtp0azV"
      },
      "source": [
        "## LSA Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0F_7viw8uE8"
      },
      "source": [
        "### BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6g_jJvO0dFZ"
      },
      "source": [
        "def create_gensim_lsa_BOW (corpus, id2word, n_topics, n_words):\n",
        "  \"\"\"\n",
        "  Input:  corpus: doc_term_matrix\n",
        "          is2word: dictionary \n",
        "          number of topics, and number of words associated with each topic\n",
        "  output: return LSA model\n",
        "  \"\"\"\n",
        "  lsa_model = models.LsiModel(corpus,\n",
        "                            id2word=id2word,\n",
        "                            num_topics=n_topics)\n",
        "  \n",
        "  for index, value in lsa_model.print_topics(num_topics=n_topics, num_words=n_words):\n",
        "    print('Topic ', index, ':', value)\n",
        "  \n",
        "  return lsa_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otyDGMUR0xfy"
      },
      "source": [
        "def compute_coherence_scores_bow (id2word, corpus_bow, texts, stop, start, step):\n",
        "  \"\"\"\n",
        "  Input:  id2word: a dictionary that maps token ID to tokens\n",
        "          corpus_bow: output from bowModel \n",
        "          texts: a list of list strings, tokenized texts\n",
        "          stop: Max number of topics \n",
        "          start: Min number of topics\n",
        "          step: number of topics to increment \n",
        "  Output: model_list: a list of LSA topic models \n",
        "          coherence_score: a list of coherence scores for each topic model\n",
        "  \"\"\"\n",
        "  coherence_score = []\n",
        "  model_list = []\n",
        "\n",
        "  for n_topics in range (start, stop, step):\n",
        "    lsa_model = models.LsiModel(corpus_bow, # we will use bow\n",
        "                                id2word=id2word,\n",
        "                                num_topics=n_topics)\n",
        "    \n",
        "    model_list.append(lsa_model)\n",
        "    \n",
        "    coherence_model_lsa = CoherenceModel(model=lsa_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    \n",
        "    coherence_score.append(coherence_model_lsa.get_coherence())\n",
        "  \n",
        "  return model_list, coherence_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOtMykqm03UZ",
        "outputId": "c4aa4056-c58a-41b8-b6fb-82d6d5d3ccc0"
      },
      "source": [
        "%time lsa_models = compute_coherence_scores_bow(id2word, corpus, texts, 16, start=2, step=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 50s, sys: 10.8 s, total: 4min 1s\n",
            "Wall time: 3min 41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "wQVnBS0m2OmM",
        "outputId": "38798444-5bb0-4f9f-96cd-744ad00ecd03"
      },
      "source": [
        "x = range(2, 16, 2)\n",
        "plt.plot(x, lsa_models[1])\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ro86MTY2-Sc",
        "outputId": "7b2bdfe4-6c91-46aa-9cae-8b14c82b57d9"
      },
      "source": [
        "lsa_models[0][2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.lsimodel.LsiModel at 0x7f33623a0f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "R6JnTN5D2i9z",
        "outputId": "1d7d0aa7-63d0-4d77-ea0e-ac8a7b388ee3"
      },
      "source": [
        "n_words = 10\n",
        "\n",
        "topic_words_6 = pd.DataFrame({})\n",
        "\n",
        "for i, topic in enumerate(lsa_models[0][2].get_topics()): # the term topic matrix with shape (num_topics, vocabulary_size)\n",
        "    top_feature_ids = topic.argsort()[-n_words:][::-1]\n",
        "    feature_values = topic[top_feature_ids]\n",
        "    words = [id2word[id] for id in top_feature_ids]\n",
        "    labels= [('Topic'+str(i)+' word'), ('Topic'+str(i)+' value') ]\n",
        "    topic_df = pd.DataFrame(zip(words, feature_values), columns=labels)\n",
        "    topic_words_6 = pd.concat([topic_words_6, topic_df], axis=1)\n",
        "\n",
        "topic_words_6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic0 word</th>\n",
              "      <th>Topic0 value</th>\n",
              "      <th>Topic1 word</th>\n",
              "      <th>Topic1 value</th>\n",
              "      <th>Topic2 word</th>\n",
              "      <th>Topic2 value</th>\n",
              "      <th>Topic3 word</th>\n",
              "      <th>Topic3 value</th>\n",
              "      <th>Topic4 word</th>\n",
              "      <th>Topic4 value</th>\n",
              "      <th>Topic5 word</th>\n",
              "      <th>Topic5 value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>get</td>\n",
              "      <td>0.292389</td>\n",
              "      <td>doctor</td>\n",
              "      <td>0.407971</td>\n",
              "      <td>nurse</td>\n",
              "      <td>0.338769</td>\n",
              "      <td>doctor</td>\n",
              "      <td>0.647560</td>\n",
              "      <td>office</td>\n",
              "      <td>0.379533</td>\n",
              "      <td>say</td>\n",
              "      <td>0.271196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>doctor</td>\n",
              "      <td>0.262781</td>\n",
              "      <td>staff</td>\n",
              "      <td>0.284596</td>\n",
              "      <td>hospital</td>\n",
              "      <td>0.271457</td>\n",
              "      <td>say</td>\n",
              "      <td>0.252345</td>\n",
              "      <td>staff</td>\n",
              "      <td>0.260486</td>\n",
              "      <td>would</td>\n",
              "      <td>0.262496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>time</td>\n",
              "      <td>0.219896</td>\n",
              "      <td>care</td>\n",
              "      <td>0.229561</td>\n",
              "      <td>come</td>\n",
              "      <td>0.141630</td>\n",
              "      <td>tell</td>\n",
              "      <td>0.164889</td>\n",
              "      <td>care</td>\n",
              "      <td>0.218053</td>\n",
              "      <td>tell</td>\n",
              "      <td>0.174627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>say</td>\n",
              "      <td>0.216918</td>\n",
              "      <td>time</td>\n",
              "      <td>0.188349</td>\n",
              "      <td>room</td>\n",
              "      <td>0.129623</td>\n",
              "      <td>would</td>\n",
              "      <td>0.083801</td>\n",
              "      <td>call</td>\n",
              "      <td>0.172467</td>\n",
              "      <td>wait</td>\n",
              "      <td>0.170054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>call</td>\n",
              "      <td>0.216729</td>\n",
              "      <td>patient</td>\n",
              "      <td>0.184341</td>\n",
              "      <td>care</td>\n",
              "      <td>0.124251</td>\n",
              "      <td>ask</td>\n",
              "      <td>0.061121</td>\n",
              "      <td>would</td>\n",
              "      <td>0.154238</td>\n",
              "      <td>patient</td>\n",
              "      <td>0.157997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tell</td>\n",
              "      <td>0.206495</td>\n",
              "      <td>office</td>\n",
              "      <td>0.159319</td>\n",
              "      <td>take</td>\n",
              "      <td>0.121552</td>\n",
              "      <td>pain</td>\n",
              "      <td>0.057178</td>\n",
              "      <td>patient</td>\n",
              "      <td>0.096905</td>\n",
              "      <td>ask</td>\n",
              "      <td>0.135594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>would</td>\n",
              "      <td>0.202713</td>\n",
              "      <td>great</td>\n",
              "      <td>0.132457</td>\n",
              "      <td>get</td>\n",
              "      <td>0.096694</td>\n",
              "      <td>nurse</td>\n",
              "      <td>0.055175</td>\n",
              "      <td>great</td>\n",
              "      <td>0.091816</td>\n",
              "      <td>office</td>\n",
              "      <td>0.090826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>see</td>\n",
              "      <td>0.180035</td>\n",
              "      <td>always</td>\n",
              "      <td>0.108132</td>\n",
              "      <td>pain</td>\n",
              "      <td>0.095865</td>\n",
              "      <td>give</td>\n",
              "      <td>0.051252</td>\n",
              "      <td>surgery</td>\n",
              "      <td>0.089035</td>\n",
              "      <td>come</td>\n",
              "      <td>0.077313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>take</td>\n",
              "      <td>0.164390</td>\n",
              "      <td>see</td>\n",
              "      <td>0.106668</td>\n",
              "      <td>one</td>\n",
              "      <td>0.088991</td>\n",
              "      <td>hospital</td>\n",
              "      <td>0.049982</td>\n",
              "      <td>always</td>\n",
              "      <td>0.068582</td>\n",
              "      <td>see</td>\n",
              "      <td>0.064095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>office</td>\n",
              "      <td>0.163494</td>\n",
              "      <td>good</td>\n",
              "      <td>0.071375</td>\n",
              "      <td>help</td>\n",
              "      <td>0.084270</td>\n",
              "      <td>know</td>\n",
              "      <td>0.049214</td>\n",
              "      <td>need</td>\n",
              "      <td>0.065417</td>\n",
              "      <td>hour</td>\n",
              "      <td>0.059098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Topic0 word  Topic0 value Topic1 word  ...  Topic4 value Topic5 word  Topic5 value\n",
              "0         get      0.292389      doctor  ...      0.379533         say      0.271196\n",
              "1      doctor      0.262781       staff  ...      0.260486       would      0.262496\n",
              "2        time      0.219896        care  ...      0.218053        tell      0.174627\n",
              "3         say      0.216918        time  ...      0.172467        wait      0.170054\n",
              "4        call      0.216729     patient  ...      0.154238     patient      0.157997\n",
              "5        tell      0.206495      office  ...      0.096905         ask      0.135594\n",
              "6       would      0.202713       great  ...      0.091816      office      0.090826\n",
              "7         see      0.180035      always  ...      0.089035        come      0.077313\n",
              "8        take      0.164390         see  ...      0.068582         see      0.064095\n",
              "9      office      0.163494        good  ...      0.065417        hour      0.059098\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cpdvDWq8xLV"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99_u7iCX-vrr",
        "outputId": "fe2352d2-1778-4094-f066-e54ce5585ff7"
      },
      "source": [
        "# vectorize the corpus using tfidf\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "\n",
        "%time corpus_tfidf = tfidf[corpus]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 312 µs, sys: 4 µs, total: 316 µs\n",
            "Wall time: 321 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcNQFKI_eii"
      },
      "source": [
        "def compute_coherence_scores_tfidf(id2word, corpus_tfidf, texts, stop, start, step):\n",
        "  \"\"\"\n",
        "  Input:  id2word: a dictionary that maps token ID to tokens\n",
        "          corpus_bow: output from bowModel \n",
        "          texts: a list of list strings, tokenized texts\n",
        "          stop: Max number of topics \n",
        "          start: Min number of topics\n",
        "          step: number of topics to increment \n",
        "  Output: model_list: a list of LSA topic models \n",
        "          coherence_score: a list of coherence scores for each topic model\n",
        "  \"\"\"\n",
        "  coherence_score = []\n",
        "  model_list = []\n",
        "\n",
        "  for n_topics in range (start, stop, step):\n",
        "    lsa_model = models.LsiModel(corpus_tfidf, # we will use bow\n",
        "                                id2word=id2word,\n",
        "                                num_topics=n_topics)\n",
        "    \n",
        "    model_list.append(lsa_model)\n",
        "    \n",
        "    coherence_model_lsa = CoherenceModel(model=lsa_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    \n",
        "    coherence_score.append(coherence_model_lsa.get_coherence())\n",
        "  \n",
        "  return model_list, coherence_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9yl96j8_jME",
        "outputId": "0fda32b5-9ce5-4319-ed1d-30aa28049d75"
      },
      "source": [
        "%time lsa_models_tfidf = compute_coherence_scores_tfidf(id2word, corpus_tfidf, texts, 16, start=2, step=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 58s, sys: 12.1 s, total: 5min 10s\n",
            "Wall time: 4min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2ulyJyRdA5ve",
        "outputId": "cf920439-9333-4c8f-d1a2-56e80f9f2e27"
      },
      "source": [
        "x = range(2, 16, 2)\n",
        "plt.plot(x, lsa_models_tfidf[1])\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "akL8uHkHBEEC",
        "outputId": "f4a07d62-8908-4b91-e9c1-3bcf4dc1174d"
      },
      "source": [
        "n_words = 10\n",
        "\n",
        "topic_words_8_tfidf = pd.DataFrame({})\n",
        "\n",
        "for i, topic in enumerate(lsa_models_tfidf[0][3].get_topics()): # the term topic matrix with shape (num_topics, vocabulary_size)\n",
        "    top_feature_ids = topic.argsort()[-n_words:][::-1]\n",
        "    feature_values = topic[top_feature_ids]\n",
        "    words = [id2word[id] for id in top_feature_ids]\n",
        "    labels= [('Topic'+str(i)+' word'), ('Topic'+str(i)+' value') ]\n",
        "    topic_df = pd.DataFrame(zip(words, feature_values), columns=labels)\n",
        "    topic_words_8_tfidf = pd.concat([topic_words_8_tfidf, topic_df], axis=1)\n",
        "\n",
        "topic_words_8_tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic0 word</th>\n",
              "      <th>Topic0 value</th>\n",
              "      <th>Topic1 word</th>\n",
              "      <th>Topic1 value</th>\n",
              "      <th>Topic2 word</th>\n",
              "      <th>Topic2 value</th>\n",
              "      <th>Topic3 word</th>\n",
              "      <th>Topic3 value</th>\n",
              "      <th>Topic4 word</th>\n",
              "      <th>Topic4 value</th>\n",
              "      <th>Topic5 word</th>\n",
              "      <th>Topic5 value</th>\n",
              "      <th>Topic6 word</th>\n",
              "      <th>Topic6 value</th>\n",
              "      <th>Topic7 word</th>\n",
              "      <th>Topic7 value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>doctor</td>\n",
              "      <td>0.158818</td>\n",
              "      <td>great</td>\n",
              "      <td>0.281958</td>\n",
              "      <td>hospital</td>\n",
              "      <td>0.205657</td>\n",
              "      <td>office</td>\n",
              "      <td>0.365569</td>\n",
              "      <td>surgery</td>\n",
              "      <td>0.483165</td>\n",
              "      <td>love</td>\n",
              "      <td>0.326254</td>\n",
              "      <td>always</td>\n",
              "      <td>0.218943</td>\n",
              "      <td>hospital</td>\n",
              "      <td>0.447596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wait</td>\n",
              "      <td>0.149808</td>\n",
              "      <td>always</td>\n",
              "      <td>0.202560</td>\n",
              "      <td>surgery</td>\n",
              "      <td>0.167264</td>\n",
              "      <td>call</td>\n",
              "      <td>0.282260</td>\n",
              "      <td>procedure</td>\n",
              "      <td>0.239521</td>\n",
              "      <td>son</td>\n",
              "      <td>0.230772</td>\n",
              "      <td>love</td>\n",
              "      <td>0.212772</td>\n",
              "      <td>nurse</td>\n",
              "      <td>0.267653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>office</td>\n",
              "      <td>0.143670</td>\n",
              "      <td>friendly</td>\n",
              "      <td>0.202515</td>\n",
              "      <td>pain</td>\n",
              "      <td>0.131978</td>\n",
              "      <td>appointment</td>\n",
              "      <td>0.208772</td>\n",
              "      <td>eye</td>\n",
              "      <td>0.165232</td>\n",
              "      <td>kid</td>\n",
              "      <td>0.159820</td>\n",
              "      <td>best</td>\n",
              "      <td>0.191703</td>\n",
              "      <td>call</td>\n",
              "      <td>0.204052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>call</td>\n",
              "      <td>0.138821</td>\n",
              "      <td>staff</td>\n",
              "      <td>0.194903</td>\n",
              "      <td>nurse</td>\n",
              "      <td>0.108746</td>\n",
              "      <td>phone</td>\n",
              "      <td>0.104886</td>\n",
              "      <td>appointment</td>\n",
              "      <td>0.119483</td>\n",
              "      <td>...</td>\n",
              "      <td>0.148563</td>\n",
              "      <td>year</td>\n",
              "      <td>0.178544</td>\n",
              "      <td>office</td>\n",
              "      <td>0.193070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>time</td>\n",
              "      <td>0.137892</td>\n",
              "      <td>love</td>\n",
              "      <td>0.170145</td>\n",
              "      <td>say</td>\n",
              "      <td>0.106122</td>\n",
              "      <td>always</td>\n",
              "      <td>0.087628</td>\n",
              "      <td>wait</td>\n",
              "      <td>0.117094</td>\n",
              "      <td>baby</td>\n",
              "      <td>0.138856</td>\n",
              "      <td>see</td>\n",
              "      <td>0.134608</td>\n",
              "      <td>son</td>\n",
              "      <td>0.160372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>get</td>\n",
              "      <td>0.137670</td>\n",
              "      <td>professional</td>\n",
              "      <td>0.162526</td>\n",
              "      <td>help</td>\n",
              "      <td>0.085892</td>\n",
              "      <td>insurance</td>\n",
              "      <td>0.085572</td>\n",
              "      <td>comfortable</td>\n",
              "      <td>0.114655</td>\n",
              "      <td>daughter</td>\n",
              "      <td>0.136067</td>\n",
              "      <td>wait</td>\n",
              "      <td>0.128091</td>\n",
              "      <td>appointment</td>\n",
              "      <td>0.147825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>appointment</td>\n",
              "      <td>0.134980</td>\n",
              "      <td>best</td>\n",
              "      <td>0.160032</td>\n",
              "      <td>care</td>\n",
              "      <td>0.081452</td>\n",
              "      <td>referral</td>\n",
              "      <td>0.075758</td>\n",
              "      <td>great</td>\n",
              "      <td>0.106323</td>\n",
              "      <td>nice</td>\n",
              "      <td>0.134929</td>\n",
              "      <td>appointment</td>\n",
              "      <td>0.126944</td>\n",
              "      <td>daughter</td>\n",
              "      <td>0.113326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>see</td>\n",
              "      <td>0.127340</td>\n",
              "      <td>care</td>\n",
              "      <td>0.149467</td>\n",
              "      <td>tell</td>\n",
              "      <td>0.080436</td>\n",
              "      <td>schedule</td>\n",
              "      <td>0.075633</td>\n",
              "      <td>lasik</td>\n",
              "      <td>0.103137</td>\n",
              "      <td>call</td>\n",
              "      <td>0.119063</td>\n",
              "      <td>surgery</td>\n",
              "      <td>0.121787</td>\n",
              "      <td>baby</td>\n",
              "      <td>0.103845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>staff</td>\n",
              "      <td>0.119932</td>\n",
              "      <td>highly_recommend</td>\n",
              "      <td>0.145887</td>\n",
              "      <td>could</td>\n",
              "      <td>0.076066</td>\n",
              "      <td>year</td>\n",
              "      <td>0.071033</td>\n",
              "      <td>pain</td>\n",
              "      <td>0.102732</td>\n",
              "      <td>nurse</td>\n",
              "      <td>0.115981</td>\n",
              "      <td>son</td>\n",
              "      <td>0.121364</td>\n",
              "      <td>child</td>\n",
              "      <td>0.093368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>say</td>\n",
              "      <td>0.117625</td>\n",
              "      <td>thank</td>\n",
              "      <td>0.131170</td>\n",
              "      <td>want</td>\n",
              "      <td>0.071561</td>\n",
              "      <td>week</td>\n",
              "      <td>0.068266</td>\n",
              "      <td>consultation</td>\n",
              "      <td>0.097057</td>\n",
              "      <td>say</td>\n",
              "      <td>0.110540</td>\n",
              "      <td>doctor</td>\n",
              "      <td>0.110951</td>\n",
              "      <td>day</td>\n",
              "      <td>0.090951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Topic0 word  Topic0 value  ...  Topic7 word  Topic7 value\n",
              "0       doctor      0.158818  ...     hospital      0.447596\n",
              "1         wait      0.149808  ...        nurse      0.267653\n",
              "2       office      0.143670  ...         call      0.204052\n",
              "3         call      0.138821  ...       office      0.193070\n",
              "4         time      0.137892  ...          son      0.160372\n",
              "5          get      0.137670  ...  appointment      0.147825\n",
              "6  appointment      0.134980  ...     daughter      0.113326\n",
              "7          see      0.127340  ...         baby      0.103845\n",
              "8        staff      0.119932  ...        child      0.093368\n",
              "9          say      0.117625  ...          day      0.090951\n",
              "\n",
              "[10 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUyzdynCBVLl"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQEPUzUmBWbO"
      },
      "source": [
        "def compute_lda_coherence_scores (id2word, texts, corpus, stop, start, step):\n",
        "  \n",
        "  \"\"\"\n",
        "  Input:  id2word: a dictionary that maps token ID to tokens\n",
        "          texts: a list of list strings\n",
        "          corpus: Gensim corpus which is a BOW model with bigrams\n",
        "          stop: Max number of topics \n",
        "          start: Min number of topics\n",
        "          step: number of topics to increment \n",
        "  Output: model_list: a list of LDA topic models \n",
        "          coherence_score: a list of coherence scores for each topic model\n",
        "  \"\"\"\n",
        "  coherence_score = []\n",
        "  model_list = []\n",
        "\n",
        "  for n_topics in range (start, stop, step):\n",
        "    chunksize = 5000\n",
        "    passes = 10\n",
        "    iterations = 400 # we increase its iterations\n",
        "    eval_every = None  # \n",
        "    \n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                            id2word=id2word,\n",
        "                                            num_topics=n_topics, \n",
        "                                            random_state=100,\n",
        "                                            chunksize=chunksize,\n",
        "                                            passes=passes,\n",
        "                                            alpha='auto',\n",
        "                                            eta ='auto',\n",
        "                                            iterations = iterations,\n",
        "                                            gamma_threshold=0.001,\n",
        "                                            per_word_topics=True,\n",
        "                                            eval_every = eval_every)\n",
        "    \n",
        "    model_list.append(lda_model)\n",
        "    \n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    \n",
        "    coherence_score.append(coherence_model_lda.get_coherence())\n",
        "  \n",
        "  return model_list, coherence_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "SoQwAWYMzKG6",
        "outputId": "0c9222cf-721c-493d-f736-6fe5c79dad6c"
      },
      "source": [
        "chunksize = 5000\n",
        "passes = 10\n",
        "iterations = 400 # we increase its iterations\n",
        "eval_every = None  # \n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                        id2word=id2word,\n",
        "                                        num_topics=6, \n",
        "                                        random_state=100,\n",
        "                                        chunksize=chunksize,\n",
        "                                        passes=passes,\n",
        "                                        alpha='auto',\n",
        "                                        eta ='auto',\n",
        "                                        iterations = iterations,\n",
        "                                        gamma_threshold=0.001,\n",
        "                                        per_word_topics=True,\n",
        "                                        eval_every = eval_every)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8e405609c96c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                         \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                         \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_yDS_Vg0vhp"
      },
      "source": [
        "# default print topic\n",
        "# we can see the topic is much easier to interpret, partly because of LAD algroithm, partly because we filtered extreme words\n",
        "for index, value in lda_model.print_topics():\n",
        "  print('Topic ', index, ':', value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34b8ORcztgaA"
      },
      "source": [
        "%time lda_models = compute_lda_coherence_scores(id2word, texts, corpus, 16, 4, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCaOdJrUCm8i"
      },
      "source": [
        "x = range(4, 16, 2)\n",
        "plt.plot(x, lda_models[1])\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7JFWwYYEsph"
      },
      "source": [
        "lda_models[0][5].save('/content/drive/MyDrive/Colab_Notebooks/lda_model_14.p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOjcH4ypFSxi"
      },
      "source": [
        "lda_model_14 = gensim.models.ldamodel.LdaModel.load('/content/drive/MyDrive/Colab_Notebooks/lda_model_14.p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aRpUi7dC0Ey"
      },
      "source": [
        "import IPython\n",
        "IPython.display.HTML(filename= '/content/drive/MyDrive/Colab_Notebooks/lda_model_14_vis.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRgn_w-_8-Dx"
      },
      "source": [
        "'''\n",
        "  topic 0: people are the major topics. The overall review is positive. Describe staff. \n",
        "  topic 1: A topic containing negative reviews. Insufficient nouns to describe the major topic.\n",
        "  topic 2: A topic focusing on making appointment. Describe the approach to make appointment.\n",
        "  topic 3: Still focus on making appointment. Describe experience of making appointment.\n",
        "  topic 4: Describe experience of meeting with doctor.\n",
        "  topic 5: Describe experience of getting treatment in medical center. No people showing up in this topic.\n",
        "  topic 6: A topic describing experience of patients' relatives. \n",
        "  topic 7: skin and vision are words related to medical field. Review tends to be positive.\n",
        "  topic 8: Insurance.\n",
        "  topic 9: reviewers related to this topic may have fever and inflammmation.\n",
        "  topic 10: reviewers related to this topic may have physical injury.\n",
        "  topic 11: Describe experience of getting treatment in center, and patients might be in emergency.\n",
        "  topic 12: this topic focuses on children.\n",
        "  topic 13: A topic related to surgery. \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5hy7qHSDUHQ"
      },
      "source": [
        "chunksize = 5000\n",
        "passes = 10\n",
        "iterations = 400 # we increase its iterations\n",
        "eval_every = None  # \n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                        id2word=id2word,\n",
        "                                        num_topics=14, \n",
        "                                        random_state=100,\n",
        "                                        chunksize=chunksize,\n",
        "                                        passes=passes,\n",
        "                                        alpha='auto',\n",
        "                                        eta ='auto',\n",
        "                                        iterations = iterations,\n",
        "                                        gamma_threshold=0.001,\n",
        "                                        per_word_topics=True,\n",
        "                                        eval_every = eval_every)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tEKM_4VEuvD"
      },
      "source": [
        "topic_range = list(range(0,15))\n",
        "topic_prob = []\n",
        "full_topics = []\n",
        "for i, row in enumerate(lda_model[corpus]):\n",
        "  row = row[0]\n",
        "  topics = []\n",
        "  probs = []\n",
        "  topics_prob = []\n",
        "  for topic in row:\n",
        "    topics.append(topic[0])\n",
        "    probs.append(topic[1])\n",
        "  topic_index = 0 \n",
        "  for topic_num in topic_range:\n",
        "    if topic_num in topics:\n",
        "      topics_prob.append(probs[topic_index])\n",
        "      topic_index += 1\n",
        "    else:\n",
        "      topics_prob.append(0.00)\n",
        "  full_topics.append(topics_prob)\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OebrhcHJSirL"
      },
      "source": [
        "full_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTFp0n17JHvK"
      },
      "source": [
        "topic_labels = ['Topic_' + str(i) for i in range (0, 15)]\n",
        "topic_df = pd.DataFrame(full_topics, index=df_review_final.index.tolist(), columns=topic_labels)\n",
        "topic_df = topic_df.reset_index()\n",
        "topic_df.rename(columns={'index':'Index'}, inplace=True)\n",
        "topic_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTUWKVy5XBSz"
      },
      "source": [
        "df_glove_attributes = pd.merge(left = df_w_glove, right = topic_df, left_on='Index', right_on='Index')\n",
        "df_glove_attributes_agg = df_glove_attributes.groupby([\t'doctorID',\t]).agg({'rating':'mean', 'Length' : 'mean', 'Lexical Diversity':'mean'}).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thiKg7-4ZIum"
      },
      "source": [
        "df_glove_attributes.to_pickle('df_w_attributes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b17aa0Q9ZYoO"
      },
      "source": [
        "for label in topic_labels:\n",
        "  df_glove_attributes_agg[label] = agg_cols(df_glove_attributes,label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNotvM-IaFIS"
      },
      "source": [
        "df_attributes_agg = pd.merge(left = df_glove_attributes_agg, right = df_sa_agg, left_on = \"doctorID\", right_on = \"doctorID\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7HVmM2GrcsN"
      },
      "source": [
        "df_ner_spacy = pd.read_pickle('df_ner_spacy.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hdQ7lfor5BF"
      },
      "source": [
        "df_ner_agg = df_ner_spacy.groupby('doctorID').agg({'ORG':'mean', 'GPE' :'mean', 'PERSON':'mean', 'MONEY':'mean'}).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6PcT1MIrrLp"
      },
      "source": [
        "df_attributes_agg = pd.merge(left = df_attributes_agg, right = df_ner_agg, left_on = \"doctorID\", right_on = \"doctorID\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCC3zAjltlCm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB3QpbA2adAi"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df_attributes_agg.to_pickle('df_w_attributes_agg')\n",
        "df_attributes_agg.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGm3EbCUkso"
      },
      "source": [
        "# Task 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFYS_vb20BPe"
      },
      "source": [
        "Supervised Learning - Sean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0fYZ_XMhO-f"
      },
      "source": [
        "Emperical models:\n",
        "* word embeddings\n",
        "* word embeddings SVD\n",
        "* NER tags, length, lexical diversity, sentiment, group probabilities?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJwdfatCi5zz",
        "outputId": "95dc77cb-b73a-45ae-f0e7-c21dc97556f5"
      },
      "source": [
        "# 4/1AX4XfWiwqqnbmlSsHv7Hg9CDf-amMUeZpLPFuDlcly2fTlHaBROfrOkcgpI\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ca3QXy6iauj",
        "outputId": "d04068bc-ab1d-48cb-f0c8-5fcd8b2917e5"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "import pandas as pd\n",
        "df_glove_agg = pd.read_csv('df_glove_agg.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "aW0IM-ywkFAW",
        "outputId": "1657adcc-6a07-4ee8-ee41-167d7b12e4d0"
      },
      "source": [
        "df_w_glove_agg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rating</th>\n",
              "      <th>AWE2</th>\n",
              "      <th>AWE3</th>\n",
              "      <th>AWE4</th>\n",
              "      <th>AWE5</th>\n",
              "      <th>AWE6</th>\n",
              "      <th>AWE7</th>\n",
              "      <th>AWE8</th>\n",
              "      <th>AWE9</th>\n",
              "      <th>AWE10</th>\n",
              "      <th>AWE11</th>\n",
              "      <th>AWE12</th>\n",
              "      <th>AWE13</th>\n",
              "      <th>AWE14</th>\n",
              "      <th>AWE15</th>\n",
              "      <th>AWE16</th>\n",
              "      <th>AWE17</th>\n",
              "      <th>AWE18</th>\n",
              "      <th>AWE19</th>\n",
              "      <th>AWE20</th>\n",
              "      <th>AWE21</th>\n",
              "      <th>AWE22</th>\n",
              "      <th>AWE23</th>\n",
              "      <th>AWE24</th>\n",
              "      <th>AWE25</th>\n",
              "      <th>AWE26</th>\n",
              "      <th>AWE27</th>\n",
              "      <th>AWE28</th>\n",
              "      <th>AWE29</th>\n",
              "      <th>AWE30</th>\n",
              "      <th>AWE31</th>\n",
              "      <th>AWE32</th>\n",
              "      <th>AWE33</th>\n",
              "      <th>AWE34</th>\n",
              "      <th>AWE35</th>\n",
              "      <th>AWE36</th>\n",
              "      <th>AWE37</th>\n",
              "      <th>AWE38</th>\n",
              "      <th>...</th>\n",
              "      <th>AWE162</th>\n",
              "      <th>AWE163</th>\n",
              "      <th>AWE164</th>\n",
              "      <th>AWE165</th>\n",
              "      <th>AWE166</th>\n",
              "      <th>AWE167</th>\n",
              "      <th>AWE168</th>\n",
              "      <th>AWE169</th>\n",
              "      <th>AWE170</th>\n",
              "      <th>AWE171</th>\n",
              "      <th>AWE172</th>\n",
              "      <th>AWE173</th>\n",
              "      <th>AWE174</th>\n",
              "      <th>AWE175</th>\n",
              "      <th>AWE176</th>\n",
              "      <th>AWE177</th>\n",
              "      <th>AWE178</th>\n",
              "      <th>AWE179</th>\n",
              "      <th>AWE180</th>\n",
              "      <th>AWE181</th>\n",
              "      <th>AWE182</th>\n",
              "      <th>AWE183</th>\n",
              "      <th>AWE184</th>\n",
              "      <th>AWE185</th>\n",
              "      <th>AWE186</th>\n",
              "      <th>AWE187</th>\n",
              "      <th>AWE188</th>\n",
              "      <th>AWE189</th>\n",
              "      <th>AWE190</th>\n",
              "      <th>AWE191</th>\n",
              "      <th>AWE192</th>\n",
              "      <th>AWE193</th>\n",
              "      <th>AWE194</th>\n",
              "      <th>AWE195</th>\n",
              "      <th>AWE196</th>\n",
              "      <th>AWE197</th>\n",
              "      <th>AWE198</th>\n",
              "      <th>AWE199</th>\n",
              "      <th>AWE200</th>\n",
              "      <th>AWE1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.450617</td>\n",
              "      <td>0.109580</td>\n",
              "      <td>-0.055467</td>\n",
              "      <td>0.032624</td>\n",
              "      <td>0.037863</td>\n",
              "      <td>0.068353</td>\n",
              "      <td>0.612281</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>-0.014816</td>\n",
              "      <td>-0.041297</td>\n",
              "      <td>-0.101164</td>\n",
              "      <td>-0.085488</td>\n",
              "      <td>-0.601401</td>\n",
              "      <td>-0.117590</td>\n",
              "      <td>0.053162</td>\n",
              "      <td>0.003640</td>\n",
              "      <td>-0.067281</td>\n",
              "      <td>0.082147</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>-0.107096</td>\n",
              "      <td>0.044002</td>\n",
              "      <td>0.049359</td>\n",
              "      <td>-0.116865</td>\n",
              "      <td>0.018457</td>\n",
              "      <td>-0.112021</td>\n",
              "      <td>0.879065</td>\n",
              "      <td>0.039489</td>\n",
              "      <td>0.179910</td>\n",
              "      <td>0.006141</td>\n",
              "      <td>-0.072227</td>\n",
              "      <td>0.076318</td>\n",
              "      <td>-0.186713</td>\n",
              "      <td>-0.030613</td>\n",
              "      <td>0.016760</td>\n",
              "      <td>-0.072900</td>\n",
              "      <td>0.061665</td>\n",
              "      <td>0.051384</td>\n",
              "      <td>0.016398</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063129</td>\n",
              "      <td>-0.071170</td>\n",
              "      <td>0.031078</td>\n",
              "      <td>0.008759</td>\n",
              "      <td>-0.084882</td>\n",
              "      <td>-0.113550</td>\n",
              "      <td>0.069989</td>\n",
              "      <td>0.046964</td>\n",
              "      <td>-0.036594</td>\n",
              "      <td>0.045713</td>\n",
              "      <td>-0.038768</td>\n",
              "      <td>0.079776</td>\n",
              "      <td>-0.029784</td>\n",
              "      <td>-0.094229</td>\n",
              "      <td>-0.133694</td>\n",
              "      <td>-0.020224</td>\n",
              "      <td>-0.172105</td>\n",
              "      <td>0.097750</td>\n",
              "      <td>-0.083468</td>\n",
              "      <td>0.055543</td>\n",
              "      <td>0.024740</td>\n",
              "      <td>0.052234</td>\n",
              "      <td>-0.016325</td>\n",
              "      <td>0.072179</td>\n",
              "      <td>0.100411</td>\n",
              "      <td>0.040954</td>\n",
              "      <td>-0.030364</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.094056</td>\n",
              "      <td>-0.054656</td>\n",
              "      <td>0.015817</td>\n",
              "      <td>0.100769</td>\n",
              "      <td>-0.119765</td>\n",
              "      <td>0.007846</td>\n",
              "      <td>-0.067108</td>\n",
              "      <td>0.077146</td>\n",
              "      <td>0.048946</td>\n",
              "      <td>0.017984</td>\n",
              "      <td>-0.055928</td>\n",
              "      <td>-0.004739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.810811</td>\n",
              "      <td>0.090133</td>\n",
              "      <td>-0.060930</td>\n",
              "      <td>0.026661</td>\n",
              "      <td>0.045295</td>\n",
              "      <td>0.027326</td>\n",
              "      <td>0.603496</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>0.009490</td>\n",
              "      <td>-0.014742</td>\n",
              "      <td>-0.067378</td>\n",
              "      <td>-0.084041</td>\n",
              "      <td>-0.580594</td>\n",
              "      <td>-0.093487</td>\n",
              "      <td>0.070746</td>\n",
              "      <td>-0.043194</td>\n",
              "      <td>-0.051184</td>\n",
              "      <td>0.096345</td>\n",
              "      <td>-0.026170</td>\n",
              "      <td>-0.138979</td>\n",
              "      <td>0.037236</td>\n",
              "      <td>0.048987</td>\n",
              "      <td>-0.073811</td>\n",
              "      <td>-0.022814</td>\n",
              "      <td>-0.104906</td>\n",
              "      <td>0.885185</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>0.171312</td>\n",
              "      <td>0.001509</td>\n",
              "      <td>-0.073263</td>\n",
              "      <td>0.063365</td>\n",
              "      <td>-0.157461</td>\n",
              "      <td>-0.018407</td>\n",
              "      <td>0.014403</td>\n",
              "      <td>-0.072707</td>\n",
              "      <td>0.078101</td>\n",
              "      <td>0.015396</td>\n",
              "      <td>0.041139</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044536</td>\n",
              "      <td>-0.074104</td>\n",
              "      <td>0.011204</td>\n",
              "      <td>0.055701</td>\n",
              "      <td>-0.092004</td>\n",
              "      <td>-0.145391</td>\n",
              "      <td>0.040438</td>\n",
              "      <td>0.061408</td>\n",
              "      <td>-0.012917</td>\n",
              "      <td>0.034453</td>\n",
              "      <td>-0.046430</td>\n",
              "      <td>0.042516</td>\n",
              "      <td>-0.029370</td>\n",
              "      <td>-0.103526</td>\n",
              "      <td>-0.126716</td>\n",
              "      <td>-0.010893</td>\n",
              "      <td>-0.185730</td>\n",
              "      <td>0.065239</td>\n",
              "      <td>-0.097023</td>\n",
              "      <td>0.022581</td>\n",
              "      <td>0.024610</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.025163</td>\n",
              "      <td>0.052349</td>\n",
              "      <td>0.119964</td>\n",
              "      <td>0.056785</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.060572</td>\n",
              "      <td>0.080504</td>\n",
              "      <td>-0.033145</td>\n",
              "      <td>0.028659</td>\n",
              "      <td>0.078784</td>\n",
              "      <td>-0.104174</td>\n",
              "      <td>0.013709</td>\n",
              "      <td>-0.071503</td>\n",
              "      <td>0.064676</td>\n",
              "      <td>0.080182</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.058888</td>\n",
              "      <td>0.017838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4.904762</td>\n",
              "      <td>0.057492</td>\n",
              "      <td>-0.032071</td>\n",
              "      <td>0.002374</td>\n",
              "      <td>-0.020498</td>\n",
              "      <td>0.049577</td>\n",
              "      <td>0.595520</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>0.010198</td>\n",
              "      <td>-0.019621</td>\n",
              "      <td>-0.100948</td>\n",
              "      <td>-0.005968</td>\n",
              "      <td>-0.577615</td>\n",
              "      <td>-0.062437</td>\n",
              "      <td>-0.002557</td>\n",
              "      <td>0.098014</td>\n",
              "      <td>-0.010020</td>\n",
              "      <td>0.054560</td>\n",
              "      <td>-0.052749</td>\n",
              "      <td>-0.064203</td>\n",
              "      <td>0.037048</td>\n",
              "      <td>0.028531</td>\n",
              "      <td>-0.062374</td>\n",
              "      <td>0.081435</td>\n",
              "      <td>-0.109872</td>\n",
              "      <td>0.939803</td>\n",
              "      <td>0.014482</td>\n",
              "      <td>0.209217</td>\n",
              "      <td>0.079155</td>\n",
              "      <td>-0.053990</td>\n",
              "      <td>0.061940</td>\n",
              "      <td>-0.113278</td>\n",
              "      <td>-0.063614</td>\n",
              "      <td>0.041954</td>\n",
              "      <td>-0.102660</td>\n",
              "      <td>0.031450</td>\n",
              "      <td>0.066701</td>\n",
              "      <td>0.024994</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064600</td>\n",
              "      <td>-0.051389</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.023759</td>\n",
              "      <td>-0.112784</td>\n",
              "      <td>-0.042657</td>\n",
              "      <td>0.032377</td>\n",
              "      <td>0.002203</td>\n",
              "      <td>0.029569</td>\n",
              "      <td>0.152568</td>\n",
              "      <td>0.015837</td>\n",
              "      <td>0.012060</td>\n",
              "      <td>-0.097442</td>\n",
              "      <td>-0.116197</td>\n",
              "      <td>-0.121550</td>\n",
              "      <td>-0.015339</td>\n",
              "      <td>-0.175676</td>\n",
              "      <td>0.036518</td>\n",
              "      <td>-0.135913</td>\n",
              "      <td>0.009221</td>\n",
              "      <td>0.021252</td>\n",
              "      <td>-0.038877</td>\n",
              "      <td>-0.019115</td>\n",
              "      <td>0.111799</td>\n",
              "      <td>-0.028155</td>\n",
              "      <td>0.045435</td>\n",
              "      <td>0.063098</td>\n",
              "      <td>0.003909</td>\n",
              "      <td>0.066870</td>\n",
              "      <td>-0.106063</td>\n",
              "      <td>0.050678</td>\n",
              "      <td>0.084019</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>0.045542</td>\n",
              "      <td>-0.073244</td>\n",
              "      <td>0.129461</td>\n",
              "      <td>0.045889</td>\n",
              "      <td>0.025196</td>\n",
              "      <td>-0.079230</td>\n",
              "      <td>-0.025036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2.416667</td>\n",
              "      <td>0.105665</td>\n",
              "      <td>-0.064177</td>\n",
              "      <td>0.024043</td>\n",
              "      <td>0.050930</td>\n",
              "      <td>0.014426</td>\n",
              "      <td>0.615250</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>0.020949</td>\n",
              "      <td>-0.056484</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>-0.049697</td>\n",
              "      <td>-0.619148</td>\n",
              "      <td>-0.102900</td>\n",
              "      <td>0.090590</td>\n",
              "      <td>-0.014152</td>\n",
              "      <td>-0.010258</td>\n",
              "      <td>0.075215</td>\n",
              "      <td>-0.044463</td>\n",
              "      <td>-0.141619</td>\n",
              "      <td>0.025299</td>\n",
              "      <td>0.041922</td>\n",
              "      <td>-0.042957</td>\n",
              "      <td>-0.008504</td>\n",
              "      <td>-0.136420</td>\n",
              "      <td>0.915835</td>\n",
              "      <td>0.063691</td>\n",
              "      <td>0.184653</td>\n",
              "      <td>-0.032327</td>\n",
              "      <td>-0.092390</td>\n",
              "      <td>0.063586</td>\n",
              "      <td>-0.156963</td>\n",
              "      <td>-0.029596</td>\n",
              "      <td>-0.011433</td>\n",
              "      <td>-0.048102</td>\n",
              "      <td>0.042451</td>\n",
              "      <td>-0.005934</td>\n",
              "      <td>0.045870</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.084272</td>\n",
              "      <td>-0.054205</td>\n",
              "      <td>0.028498</td>\n",
              "      <td>0.074691</td>\n",
              "      <td>-0.043320</td>\n",
              "      <td>-0.088574</td>\n",
              "      <td>0.034603</td>\n",
              "      <td>0.029922</td>\n",
              "      <td>-0.081509</td>\n",
              "      <td>0.042854</td>\n",
              "      <td>0.011205</td>\n",
              "      <td>0.029060</td>\n",
              "      <td>-0.063012</td>\n",
              "      <td>-0.101559</td>\n",
              "      <td>-0.107197</td>\n",
              "      <td>-0.023220</td>\n",
              "      <td>-0.147769</td>\n",
              "      <td>0.075356</td>\n",
              "      <td>-0.094736</td>\n",
              "      <td>0.024044</td>\n",
              "      <td>0.039865</td>\n",
              "      <td>0.064397</td>\n",
              "      <td>-0.008018</td>\n",
              "      <td>0.072907</td>\n",
              "      <td>0.087641</td>\n",
              "      <td>0.072423</td>\n",
              "      <td>-0.020360</td>\n",
              "      <td>0.055657</td>\n",
              "      <td>0.109679</td>\n",
              "      <td>-0.057609</td>\n",
              "      <td>0.006164</td>\n",
              "      <td>0.044117</td>\n",
              "      <td>-0.111159</td>\n",
              "      <td>0.013871</td>\n",
              "      <td>-0.073210</td>\n",
              "      <td>0.050130</td>\n",
              "      <td>0.073727</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>-0.046022</td>\n",
              "      <td>0.038018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.082836</td>\n",
              "      <td>-0.142219</td>\n",
              "      <td>0.091863</td>\n",
              "      <td>0.008776</td>\n",
              "      <td>0.022993</td>\n",
              "      <td>0.496680</td>\n",
              "      <td>0.012337</td>\n",
              "      <td>-0.051832</td>\n",
              "      <td>-0.047512</td>\n",
              "      <td>-0.153940</td>\n",
              "      <td>-0.058628</td>\n",
              "      <td>-0.631568</td>\n",
              "      <td>-0.112419</td>\n",
              "      <td>0.047878</td>\n",
              "      <td>-0.067804</td>\n",
              "      <td>-0.005677</td>\n",
              "      <td>0.102753</td>\n",
              "      <td>-0.063472</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>-0.096702</td>\n",
              "      <td>-0.053903</td>\n",
              "      <td>-0.121419</td>\n",
              "      <td>0.699968</td>\n",
              "      <td>0.095285</td>\n",
              "      <td>0.201253</td>\n",
              "      <td>0.020723</td>\n",
              "      <td>-0.041219</td>\n",
              "      <td>0.081264</td>\n",
              "      <td>-0.047501</td>\n",
              "      <td>-0.117670</td>\n",
              "      <td>0.023999</td>\n",
              "      <td>-0.120962</td>\n",
              "      <td>0.101529</td>\n",
              "      <td>0.109891</td>\n",
              "      <td>0.076047</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001705</td>\n",
              "      <td>-0.040244</td>\n",
              "      <td>0.052940</td>\n",
              "      <td>0.053133</td>\n",
              "      <td>-0.053241</td>\n",
              "      <td>-0.086791</td>\n",
              "      <td>0.024374</td>\n",
              "      <td>-0.005235</td>\n",
              "      <td>-0.038448</td>\n",
              "      <td>0.073409</td>\n",
              "      <td>-0.001047</td>\n",
              "      <td>-0.019316</td>\n",
              "      <td>0.021735</td>\n",
              "      <td>-0.098803</td>\n",
              "      <td>0.020090</td>\n",
              "      <td>-0.048372</td>\n",
              "      <td>-0.122186</td>\n",
              "      <td>0.046514</td>\n",
              "      <td>-0.057995</td>\n",
              "      <td>-0.045879</td>\n",
              "      <td>0.009031</td>\n",
              "      <td>0.035915</td>\n",
              "      <td>0.011018</td>\n",
              "      <td>0.094855</td>\n",
              "      <td>0.079106</td>\n",
              "      <td>0.024286</td>\n",
              "      <td>-0.030704</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.043099</td>\n",
              "      <td>-0.000574</td>\n",
              "      <td>0.019067</td>\n",
              "      <td>0.055732</td>\n",
              "      <td>-0.122686</td>\n",
              "      <td>-0.018306</td>\n",
              "      <td>-0.127239</td>\n",
              "      <td>0.117071</td>\n",
              "      <td>0.093071</td>\n",
              "      <td>0.058105</td>\n",
              "      <td>-0.142413</td>\n",
              "      <td>0.059764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2490</th>\n",
              "      <td>2490</td>\n",
              "      <td>2602</td>\n",
              "      <td>3.627907</td>\n",
              "      <td>0.056678</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>0.027493</td>\n",
              "      <td>0.052407</td>\n",
              "      <td>0.068345</td>\n",
              "      <td>0.578416</td>\n",
              "      <td>-0.019550</td>\n",
              "      <td>-0.020487</td>\n",
              "      <td>-0.001910</td>\n",
              "      <td>-0.043694</td>\n",
              "      <td>-0.046127</td>\n",
              "      <td>-0.616708</td>\n",
              "      <td>-0.103197</td>\n",
              "      <td>0.031103</td>\n",
              "      <td>0.018686</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>0.050676</td>\n",
              "      <td>-0.057047</td>\n",
              "      <td>-0.133830</td>\n",
              "      <td>0.024876</td>\n",
              "      <td>0.004445</td>\n",
              "      <td>-0.072272</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>-0.094578</td>\n",
              "      <td>0.957469</td>\n",
              "      <td>0.067008</td>\n",
              "      <td>0.197532</td>\n",
              "      <td>-0.003215</td>\n",
              "      <td>-0.077643</td>\n",
              "      <td>0.069441</td>\n",
              "      <td>-0.122064</td>\n",
              "      <td>-0.030265</td>\n",
              "      <td>-0.005515</td>\n",
              "      <td>-0.063978</td>\n",
              "      <td>0.054702</td>\n",
              "      <td>0.010844</td>\n",
              "      <td>0.026273</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064245</td>\n",
              "      <td>-0.056610</td>\n",
              "      <td>0.075562</td>\n",
              "      <td>0.064769</td>\n",
              "      <td>-0.059805</td>\n",
              "      <td>-0.117945</td>\n",
              "      <td>-0.012230</td>\n",
              "      <td>0.054759</td>\n",
              "      <td>-0.029953</td>\n",
              "      <td>0.055020</td>\n",
              "      <td>-0.010160</td>\n",
              "      <td>0.011240</td>\n",
              "      <td>-0.028164</td>\n",
              "      <td>-0.062938</td>\n",
              "      <td>-0.116394</td>\n",
              "      <td>-0.005935</td>\n",
              "      <td>-0.158633</td>\n",
              "      <td>0.076857</td>\n",
              "      <td>-0.080711</td>\n",
              "      <td>0.032272</td>\n",
              "      <td>0.044044</td>\n",
              "      <td>0.085706</td>\n",
              "      <td>0.001675</td>\n",
              "      <td>0.058613</td>\n",
              "      <td>0.095436</td>\n",
              "      <td>0.080023</td>\n",
              "      <td>-0.002176</td>\n",
              "      <td>0.087351</td>\n",
              "      <td>0.075854</td>\n",
              "      <td>-0.071634</td>\n",
              "      <td>0.016003</td>\n",
              "      <td>0.080881</td>\n",
              "      <td>-0.121475</td>\n",
              "      <td>0.041629</td>\n",
              "      <td>-0.096464</td>\n",
              "      <td>0.050183</td>\n",
              "      <td>0.050052</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>-0.044870</td>\n",
              "      <td>0.003801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2491</th>\n",
              "      <td>2491</td>\n",
              "      <td>2603</td>\n",
              "      <td>3.568182</td>\n",
              "      <td>0.064240</td>\n",
              "      <td>-0.112123</td>\n",
              "      <td>0.066053</td>\n",
              "      <td>0.056164</td>\n",
              "      <td>0.060306</td>\n",
              "      <td>0.577812</td>\n",
              "      <td>-0.016179</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.021948</td>\n",
              "      <td>-0.024541</td>\n",
              "      <td>-0.066753</td>\n",
              "      <td>-0.615053</td>\n",
              "      <td>-0.100780</td>\n",
              "      <td>0.040845</td>\n",
              "      <td>0.020933</td>\n",
              "      <td>-0.057925</td>\n",
              "      <td>0.070935</td>\n",
              "      <td>-0.026832</td>\n",
              "      <td>-0.139642</td>\n",
              "      <td>0.042404</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>-0.058934</td>\n",
              "      <td>-0.007043</td>\n",
              "      <td>-0.099441</td>\n",
              "      <td>0.960317</td>\n",
              "      <td>0.111114</td>\n",
              "      <td>0.214441</td>\n",
              "      <td>-0.002811</td>\n",
              "      <td>-0.048085</td>\n",
              "      <td>0.058234</td>\n",
              "      <td>-0.147535</td>\n",
              "      <td>-0.019297</td>\n",
              "      <td>0.013371</td>\n",
              "      <td>-0.053802</td>\n",
              "      <td>0.068896</td>\n",
              "      <td>-0.002958</td>\n",
              "      <td>0.072276</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.057869</td>\n",
              "      <td>-0.065796</td>\n",
              "      <td>0.064095</td>\n",
              "      <td>0.058642</td>\n",
              "      <td>-0.062993</td>\n",
              "      <td>-0.134788</td>\n",
              "      <td>-0.014483</td>\n",
              "      <td>0.039297</td>\n",
              "      <td>-0.031786</td>\n",
              "      <td>0.073128</td>\n",
              "      <td>0.006764</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>-0.036176</td>\n",
              "      <td>-0.077153</td>\n",
              "      <td>-0.089316</td>\n",
              "      <td>0.005797</td>\n",
              "      <td>-0.152313</td>\n",
              "      <td>0.087047</td>\n",
              "      <td>-0.106796</td>\n",
              "      <td>0.026582</td>\n",
              "      <td>0.043631</td>\n",
              "      <td>0.072984</td>\n",
              "      <td>-0.001705</td>\n",
              "      <td>0.070524</td>\n",
              "      <td>0.102812</td>\n",
              "      <td>0.101850</td>\n",
              "      <td>0.022718</td>\n",
              "      <td>0.111631</td>\n",
              "      <td>0.079243</td>\n",
              "      <td>-0.074038</td>\n",
              "      <td>0.003718</td>\n",
              "      <td>0.083287</td>\n",
              "      <td>-0.114277</td>\n",
              "      <td>0.015116</td>\n",
              "      <td>-0.095072</td>\n",
              "      <td>0.048425</td>\n",
              "      <td>0.070964</td>\n",
              "      <td>0.012766</td>\n",
              "      <td>-0.067173</td>\n",
              "      <td>0.008686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>2492</td>\n",
              "      <td>2604</td>\n",
              "      <td>2.441176</td>\n",
              "      <td>0.087743</td>\n",
              "      <td>-0.074101</td>\n",
              "      <td>0.031143</td>\n",
              "      <td>0.014160</td>\n",
              "      <td>0.044622</td>\n",
              "      <td>0.658050</td>\n",
              "      <td>0.023040</td>\n",
              "      <td>-0.023856</td>\n",
              "      <td>-0.000529</td>\n",
              "      <td>0.007301</td>\n",
              "      <td>-0.056878</td>\n",
              "      <td>-0.608873</td>\n",
              "      <td>-0.088152</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>-0.018370</td>\n",
              "      <td>-0.046578</td>\n",
              "      <td>0.075851</td>\n",
              "      <td>-0.059118</td>\n",
              "      <td>-0.114944</td>\n",
              "      <td>0.052351</td>\n",
              "      <td>0.010889</td>\n",
              "      <td>-0.057088</td>\n",
              "      <td>-0.028194</td>\n",
              "      <td>-0.063588</td>\n",
              "      <td>0.935718</td>\n",
              "      <td>0.056559</td>\n",
              "      <td>0.145997</td>\n",
              "      <td>0.016430</td>\n",
              "      <td>-0.077094</td>\n",
              "      <td>0.029316</td>\n",
              "      <td>-0.060867</td>\n",
              "      <td>-0.034266</td>\n",
              "      <td>0.025471</td>\n",
              "      <td>-0.112250</td>\n",
              "      <td>0.096995</td>\n",
              "      <td>-0.013451</td>\n",
              "      <td>0.016222</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108486</td>\n",
              "      <td>-0.042635</td>\n",
              "      <td>0.040778</td>\n",
              "      <td>0.069572</td>\n",
              "      <td>-0.057148</td>\n",
              "      <td>-0.095314</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>0.014888</td>\n",
              "      <td>-0.061426</td>\n",
              "      <td>0.038052</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.005368</td>\n",
              "      <td>-0.045913</td>\n",
              "      <td>-0.098609</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>-0.026113</td>\n",
              "      <td>-0.163115</td>\n",
              "      <td>0.047714</td>\n",
              "      <td>-0.059092</td>\n",
              "      <td>-0.000953</td>\n",
              "      <td>0.066118</td>\n",
              "      <td>0.071290</td>\n",
              "      <td>0.021352</td>\n",
              "      <td>0.038844</td>\n",
              "      <td>0.069540</td>\n",
              "      <td>0.061056</td>\n",
              "      <td>-0.009095</td>\n",
              "      <td>0.086287</td>\n",
              "      <td>0.071161</td>\n",
              "      <td>-0.041962</td>\n",
              "      <td>0.025281</td>\n",
              "      <td>0.079294</td>\n",
              "      <td>-0.082179</td>\n",
              "      <td>0.017093</td>\n",
              "      <td>-0.093254</td>\n",
              "      <td>0.031321</td>\n",
              "      <td>0.093794</td>\n",
              "      <td>0.016454</td>\n",
              "      <td>-0.053786</td>\n",
              "      <td>0.017475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>2493</td>\n",
              "      <td>2605</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>0.060966</td>\n",
              "      <td>-0.087064</td>\n",
              "      <td>0.046408</td>\n",
              "      <td>0.029821</td>\n",
              "      <td>0.040159</td>\n",
              "      <td>0.627986</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>-0.015949</td>\n",
              "      <td>0.013601</td>\n",
              "      <td>-0.004045</td>\n",
              "      <td>-0.050633</td>\n",
              "      <td>-0.638278</td>\n",
              "      <td>-0.092950</td>\n",
              "      <td>0.059763</td>\n",
              "      <td>-0.007455</td>\n",
              "      <td>-0.027335</td>\n",
              "      <td>0.050195</td>\n",
              "      <td>-0.072600</td>\n",
              "      <td>-0.118941</td>\n",
              "      <td>0.043963</td>\n",
              "      <td>0.028052</td>\n",
              "      <td>-0.020707</td>\n",
              "      <td>0.017013</td>\n",
              "      <td>-0.100314</td>\n",
              "      <td>0.924332</td>\n",
              "      <td>0.066779</td>\n",
              "      <td>0.184031</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>-0.087042</td>\n",
              "      <td>0.082218</td>\n",
              "      <td>-0.084034</td>\n",
              "      <td>-0.053479</td>\n",
              "      <td>0.020944</td>\n",
              "      <td>-0.107375</td>\n",
              "      <td>0.062292</td>\n",
              "      <td>0.004020</td>\n",
              "      <td>0.050163</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060331</td>\n",
              "      <td>-0.040871</td>\n",
              "      <td>0.023813</td>\n",
              "      <td>0.060147</td>\n",
              "      <td>-0.026382</td>\n",
              "      <td>-0.114827</td>\n",
              "      <td>-0.027009</td>\n",
              "      <td>0.027002</td>\n",
              "      <td>-0.049506</td>\n",
              "      <td>0.053767</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.020552</td>\n",
              "      <td>-0.024567</td>\n",
              "      <td>-0.095950</td>\n",
              "      <td>-0.102344</td>\n",
              "      <td>-0.014801</td>\n",
              "      <td>-0.154797</td>\n",
              "      <td>0.043244</td>\n",
              "      <td>-0.085830</td>\n",
              "      <td>0.016672</td>\n",
              "      <td>0.063306</td>\n",
              "      <td>0.065970</td>\n",
              "      <td>-0.007496</td>\n",
              "      <td>0.042143</td>\n",
              "      <td>0.109708</td>\n",
              "      <td>0.071727</td>\n",
              "      <td>-0.017870</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.060924</td>\n",
              "      <td>-0.063255</td>\n",
              "      <td>-0.002328</td>\n",
              "      <td>0.090009</td>\n",
              "      <td>-0.110579</td>\n",
              "      <td>0.055245</td>\n",
              "      <td>-0.057162</td>\n",
              "      <td>0.068616</td>\n",
              "      <td>0.101482</td>\n",
              "      <td>-0.015338</td>\n",
              "      <td>-0.062622</td>\n",
              "      <td>0.016577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>2494</td>\n",
              "      <td>2606</td>\n",
              "      <td>1.571429</td>\n",
              "      <td>0.102103</td>\n",
              "      <td>-0.087128</td>\n",
              "      <td>0.025585</td>\n",
              "      <td>0.010081</td>\n",
              "      <td>0.062728</td>\n",
              "      <td>0.650946</td>\n",
              "      <td>0.008557</td>\n",
              "      <td>-0.010532</td>\n",
              "      <td>0.013319</td>\n",
              "      <td>-0.033369</td>\n",
              "      <td>-0.055501</td>\n",
              "      <td>-0.609024</td>\n",
              "      <td>-0.084756</td>\n",
              "      <td>0.033530</td>\n",
              "      <td>-0.018587</td>\n",
              "      <td>-0.018317</td>\n",
              "      <td>0.088617</td>\n",
              "      <td>-0.088441</td>\n",
              "      <td>-0.114973</td>\n",
              "      <td>0.047151</td>\n",
              "      <td>0.022768</td>\n",
              "      <td>-0.068239</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>-0.096126</td>\n",
              "      <td>0.889699</td>\n",
              "      <td>0.070293</td>\n",
              "      <td>0.139533</td>\n",
              "      <td>0.015974</td>\n",
              "      <td>-0.090256</td>\n",
              "      <td>0.050364</td>\n",
              "      <td>-0.099643</td>\n",
              "      <td>-0.038802</td>\n",
              "      <td>0.019065</td>\n",
              "      <td>-0.100004</td>\n",
              "      <td>0.070795</td>\n",
              "      <td>0.012292</td>\n",
              "      <td>0.015073</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.093421</td>\n",
              "      <td>-0.063595</td>\n",
              "      <td>0.036156</td>\n",
              "      <td>0.075096</td>\n",
              "      <td>-0.039093</td>\n",
              "      <td>-0.078704</td>\n",
              "      <td>0.006561</td>\n",
              "      <td>0.020117</td>\n",
              "      <td>-0.029605</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>-0.004767</td>\n",
              "      <td>0.030869</td>\n",
              "      <td>-0.015064</td>\n",
              "      <td>-0.095884</td>\n",
              "      <td>-0.097213</td>\n",
              "      <td>-0.040490</td>\n",
              "      <td>-0.167914</td>\n",
              "      <td>0.057458</td>\n",
              "      <td>-0.062123</td>\n",
              "      <td>-0.006414</td>\n",
              "      <td>0.057779</td>\n",
              "      <td>0.056900</td>\n",
              "      <td>0.026271</td>\n",
              "      <td>0.029138</td>\n",
              "      <td>0.068665</td>\n",
              "      <td>0.062784</td>\n",
              "      <td>-0.039613</td>\n",
              "      <td>0.094178</td>\n",
              "      <td>0.077215</td>\n",
              "      <td>-0.044480</td>\n",
              "      <td>0.013093</td>\n",
              "      <td>0.072874</td>\n",
              "      <td>-0.092804</td>\n",
              "      <td>0.031574</td>\n",
              "      <td>-0.079673</td>\n",
              "      <td>0.076916</td>\n",
              "      <td>0.104879</td>\n",
              "      <td>0.033944</td>\n",
              "      <td>-0.061523</td>\n",
              "      <td>0.027545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2495 rows × 203 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  doctorID    rating  ...    AWE199    AWE200      AWE1\n",
              "0              0         0  4.450617  ...  0.017984 -0.055928 -0.004739\n",
              "1              1         1  2.810811  ...  0.002908 -0.058888  0.017838\n",
              "2              2         2  4.904762  ...  0.025196 -0.079230 -0.025036\n",
              "3              3         3  2.416667  ...  0.001317 -0.046022  0.038018\n",
              "4              4         4  3.000000  ...  0.058105 -0.142413  0.059764\n",
              "...          ...       ...       ...  ...       ...       ...       ...\n",
              "2490        2490      2602  3.627907  ...  0.008529 -0.044870  0.003801\n",
              "2491        2491      2603  3.568182  ...  0.012766 -0.067173  0.008686\n",
              "2492        2492      2604  2.441176  ...  0.016454 -0.053786  0.017475\n",
              "2493        2493      2605  1.750000  ... -0.015338 -0.062622  0.016577\n",
              "2494        2494      2606  1.571429  ...  0.033944 -0.061523  0.027545\n",
              "\n",
              "[2495 rows x 203 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "NA-UEMWSkNRW",
        "outputId": "33d270a1-7909-4cd4-bf8d-d6f2ecec73e9"
      },
      "source": [
        "df_glove_agg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rating</th>\n",
              "      <th>AWE1</th>\n",
              "      <th>AWE2</th>\n",
              "      <th>AWE3</th>\n",
              "      <th>AWE4</th>\n",
              "      <th>AWE5</th>\n",
              "      <th>AWE6</th>\n",
              "      <th>AWE7</th>\n",
              "      <th>AWE8</th>\n",
              "      <th>AWE9</th>\n",
              "      <th>AWE10</th>\n",
              "      <th>AWE11</th>\n",
              "      <th>AWE12</th>\n",
              "      <th>AWE13</th>\n",
              "      <th>AWE14</th>\n",
              "      <th>AWE15</th>\n",
              "      <th>AWE16</th>\n",
              "      <th>AWE17</th>\n",
              "      <th>AWE18</th>\n",
              "      <th>AWE19</th>\n",
              "      <th>AWE20</th>\n",
              "      <th>AWE21</th>\n",
              "      <th>AWE22</th>\n",
              "      <th>AWE23</th>\n",
              "      <th>AWE24</th>\n",
              "      <th>AWE25</th>\n",
              "      <th>AWE26</th>\n",
              "      <th>AWE27</th>\n",
              "      <th>AWE28</th>\n",
              "      <th>AWE29</th>\n",
              "      <th>AWE30</th>\n",
              "      <th>AWE31</th>\n",
              "      <th>AWE32</th>\n",
              "      <th>AWE33</th>\n",
              "      <th>AWE34</th>\n",
              "      <th>AWE35</th>\n",
              "      <th>AWE36</th>\n",
              "      <th>AWE37</th>\n",
              "      <th>AWE38</th>\n",
              "      <th>...</th>\n",
              "      <th>AWE163</th>\n",
              "      <th>AWE164</th>\n",
              "      <th>AWE165</th>\n",
              "      <th>AWE166</th>\n",
              "      <th>AWE167</th>\n",
              "      <th>AWE168</th>\n",
              "      <th>AWE169</th>\n",
              "      <th>AWE170</th>\n",
              "      <th>AWE171</th>\n",
              "      <th>AWE172</th>\n",
              "      <th>AWE173</th>\n",
              "      <th>AWE174</th>\n",
              "      <th>AWE175</th>\n",
              "      <th>AWE176</th>\n",
              "      <th>AWE177</th>\n",
              "      <th>AWE178</th>\n",
              "      <th>AWE179</th>\n",
              "      <th>AWE180</th>\n",
              "      <th>AWE181</th>\n",
              "      <th>AWE182</th>\n",
              "      <th>AWE183</th>\n",
              "      <th>AWE184</th>\n",
              "      <th>AWE185</th>\n",
              "      <th>AWE186</th>\n",
              "      <th>AWE187</th>\n",
              "      <th>AWE188</th>\n",
              "      <th>AWE189</th>\n",
              "      <th>AWE190</th>\n",
              "      <th>AWE191</th>\n",
              "      <th>AWE192</th>\n",
              "      <th>AWE193</th>\n",
              "      <th>AWE194</th>\n",
              "      <th>AWE195</th>\n",
              "      <th>AWE196</th>\n",
              "      <th>AWE197</th>\n",
              "      <th>AWE198</th>\n",
              "      <th>AWE199</th>\n",
              "      <th>AWE200</th>\n",
              "      <th>Length</th>\n",
              "      <th>Lexical Diversity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4.450617</td>\n",
              "      <td>-0.004739</td>\n",
              "      <td>0.109580</td>\n",
              "      <td>-0.055467</td>\n",
              "      <td>0.032624</td>\n",
              "      <td>0.037863</td>\n",
              "      <td>0.068353</td>\n",
              "      <td>0.612281</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>-0.014816</td>\n",
              "      <td>-0.041297</td>\n",
              "      <td>-0.101164</td>\n",
              "      <td>-0.085488</td>\n",
              "      <td>-0.601401</td>\n",
              "      <td>-0.117590</td>\n",
              "      <td>0.053162</td>\n",
              "      <td>0.003640</td>\n",
              "      <td>-0.067281</td>\n",
              "      <td>0.082147</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>-0.107096</td>\n",
              "      <td>0.044002</td>\n",
              "      <td>0.049359</td>\n",
              "      <td>-0.116865</td>\n",
              "      <td>0.018457</td>\n",
              "      <td>-0.112021</td>\n",
              "      <td>0.879065</td>\n",
              "      <td>0.039489</td>\n",
              "      <td>0.179910</td>\n",
              "      <td>0.006141</td>\n",
              "      <td>-0.072227</td>\n",
              "      <td>0.076318</td>\n",
              "      <td>-0.186713</td>\n",
              "      <td>-0.030613</td>\n",
              "      <td>0.016760</td>\n",
              "      <td>-0.072900</td>\n",
              "      <td>0.061665</td>\n",
              "      <td>0.051384</td>\n",
              "      <td>0.016398</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.071170</td>\n",
              "      <td>0.031078</td>\n",
              "      <td>0.008759</td>\n",
              "      <td>-0.084882</td>\n",
              "      <td>-0.113550</td>\n",
              "      <td>0.069989</td>\n",
              "      <td>0.046964</td>\n",
              "      <td>-0.036594</td>\n",
              "      <td>0.045713</td>\n",
              "      <td>-0.038768</td>\n",
              "      <td>0.079776</td>\n",
              "      <td>-0.029784</td>\n",
              "      <td>-0.094229</td>\n",
              "      <td>-0.133694</td>\n",
              "      <td>-0.020224</td>\n",
              "      <td>-0.172105</td>\n",
              "      <td>0.097750</td>\n",
              "      <td>-0.083468</td>\n",
              "      <td>0.055543</td>\n",
              "      <td>0.024740</td>\n",
              "      <td>0.052234</td>\n",
              "      <td>-0.016325</td>\n",
              "      <td>0.072179</td>\n",
              "      <td>0.100411</td>\n",
              "      <td>0.040954</td>\n",
              "      <td>-0.030364</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.094056</td>\n",
              "      <td>-0.054656</td>\n",
              "      <td>0.015817</td>\n",
              "      <td>0.100769</td>\n",
              "      <td>-0.119765</td>\n",
              "      <td>0.007846</td>\n",
              "      <td>-0.067108</td>\n",
              "      <td>0.077146</td>\n",
              "      <td>0.048946</td>\n",
              "      <td>0.017984</td>\n",
              "      <td>-0.055928</td>\n",
              "      <td>43.753086</td>\n",
              "      <td>1.105869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.810811</td>\n",
              "      <td>0.017838</td>\n",
              "      <td>0.090133</td>\n",
              "      <td>-0.060930</td>\n",
              "      <td>0.026661</td>\n",
              "      <td>0.045295</td>\n",
              "      <td>0.027326</td>\n",
              "      <td>0.603496</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>0.009490</td>\n",
              "      <td>-0.014742</td>\n",
              "      <td>-0.067378</td>\n",
              "      <td>-0.084041</td>\n",
              "      <td>-0.580594</td>\n",
              "      <td>-0.093487</td>\n",
              "      <td>0.070746</td>\n",
              "      <td>-0.043194</td>\n",
              "      <td>-0.051184</td>\n",
              "      <td>0.096345</td>\n",
              "      <td>-0.026170</td>\n",
              "      <td>-0.138979</td>\n",
              "      <td>0.037236</td>\n",
              "      <td>0.048987</td>\n",
              "      <td>-0.073811</td>\n",
              "      <td>-0.022814</td>\n",
              "      <td>-0.104906</td>\n",
              "      <td>0.885185</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>0.171312</td>\n",
              "      <td>0.001509</td>\n",
              "      <td>-0.073263</td>\n",
              "      <td>0.063365</td>\n",
              "      <td>-0.157461</td>\n",
              "      <td>-0.018407</td>\n",
              "      <td>0.014403</td>\n",
              "      <td>-0.072707</td>\n",
              "      <td>0.078101</td>\n",
              "      <td>0.015396</td>\n",
              "      <td>0.041139</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.074104</td>\n",
              "      <td>0.011204</td>\n",
              "      <td>0.055701</td>\n",
              "      <td>-0.092004</td>\n",
              "      <td>-0.145391</td>\n",
              "      <td>0.040438</td>\n",
              "      <td>0.061408</td>\n",
              "      <td>-0.012917</td>\n",
              "      <td>0.034453</td>\n",
              "      <td>-0.046430</td>\n",
              "      <td>0.042516</td>\n",
              "      <td>-0.029370</td>\n",
              "      <td>-0.103526</td>\n",
              "      <td>-0.126716</td>\n",
              "      <td>-0.010893</td>\n",
              "      <td>-0.185730</td>\n",
              "      <td>0.065239</td>\n",
              "      <td>-0.097023</td>\n",
              "      <td>0.022581</td>\n",
              "      <td>0.024610</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.025163</td>\n",
              "      <td>0.052349</td>\n",
              "      <td>0.119964</td>\n",
              "      <td>0.056785</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.060572</td>\n",
              "      <td>0.080504</td>\n",
              "      <td>-0.033145</td>\n",
              "      <td>0.028659</td>\n",
              "      <td>0.078784</td>\n",
              "      <td>-0.104174</td>\n",
              "      <td>0.013709</td>\n",
              "      <td>-0.071503</td>\n",
              "      <td>0.064676</td>\n",
              "      <td>0.080182</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.058888</td>\n",
              "      <td>48.432432</td>\n",
              "      <td>1.105801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4.904762</td>\n",
              "      <td>-0.025036</td>\n",
              "      <td>0.057492</td>\n",
              "      <td>-0.032071</td>\n",
              "      <td>0.002374</td>\n",
              "      <td>-0.020498</td>\n",
              "      <td>0.049577</td>\n",
              "      <td>0.595520</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>0.010198</td>\n",
              "      <td>-0.019621</td>\n",
              "      <td>-0.100948</td>\n",
              "      <td>-0.005968</td>\n",
              "      <td>-0.577615</td>\n",
              "      <td>-0.062437</td>\n",
              "      <td>-0.002557</td>\n",
              "      <td>0.098014</td>\n",
              "      <td>-0.010020</td>\n",
              "      <td>0.054560</td>\n",
              "      <td>-0.052749</td>\n",
              "      <td>-0.064203</td>\n",
              "      <td>0.037048</td>\n",
              "      <td>0.028531</td>\n",
              "      <td>-0.062374</td>\n",
              "      <td>0.081435</td>\n",
              "      <td>-0.109872</td>\n",
              "      <td>0.939803</td>\n",
              "      <td>0.014482</td>\n",
              "      <td>0.209217</td>\n",
              "      <td>0.079155</td>\n",
              "      <td>-0.053990</td>\n",
              "      <td>0.061940</td>\n",
              "      <td>-0.113278</td>\n",
              "      <td>-0.063614</td>\n",
              "      <td>0.041954</td>\n",
              "      <td>-0.102660</td>\n",
              "      <td>0.031450</td>\n",
              "      <td>0.066701</td>\n",
              "      <td>0.024994</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051389</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.023759</td>\n",
              "      <td>-0.112784</td>\n",
              "      <td>-0.042657</td>\n",
              "      <td>0.032377</td>\n",
              "      <td>0.002203</td>\n",
              "      <td>0.029569</td>\n",
              "      <td>0.152568</td>\n",
              "      <td>0.015837</td>\n",
              "      <td>0.012060</td>\n",
              "      <td>-0.097442</td>\n",
              "      <td>-0.116197</td>\n",
              "      <td>-0.121550</td>\n",
              "      <td>-0.015339</td>\n",
              "      <td>-0.175676</td>\n",
              "      <td>0.036518</td>\n",
              "      <td>-0.135913</td>\n",
              "      <td>0.009221</td>\n",
              "      <td>0.021252</td>\n",
              "      <td>-0.038877</td>\n",
              "      <td>-0.019115</td>\n",
              "      <td>0.111799</td>\n",
              "      <td>-0.028155</td>\n",
              "      <td>0.045435</td>\n",
              "      <td>0.063098</td>\n",
              "      <td>0.003909</td>\n",
              "      <td>0.066870</td>\n",
              "      <td>-0.106063</td>\n",
              "      <td>0.050678</td>\n",
              "      <td>0.084019</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>0.045542</td>\n",
              "      <td>-0.073244</td>\n",
              "      <td>0.129461</td>\n",
              "      <td>0.045889</td>\n",
              "      <td>0.025196</td>\n",
              "      <td>-0.079230</td>\n",
              "      <td>77.571429</td>\n",
              "      <td>1.171556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2.416667</td>\n",
              "      <td>0.038018</td>\n",
              "      <td>0.105665</td>\n",
              "      <td>-0.064177</td>\n",
              "      <td>0.024043</td>\n",
              "      <td>0.050930</td>\n",
              "      <td>0.014426</td>\n",
              "      <td>0.615250</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>0.020949</td>\n",
              "      <td>-0.056484</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>-0.049697</td>\n",
              "      <td>-0.619148</td>\n",
              "      <td>-0.102900</td>\n",
              "      <td>0.090590</td>\n",
              "      <td>-0.014152</td>\n",
              "      <td>-0.010258</td>\n",
              "      <td>0.075215</td>\n",
              "      <td>-0.044463</td>\n",
              "      <td>-0.141619</td>\n",
              "      <td>0.025299</td>\n",
              "      <td>0.041922</td>\n",
              "      <td>-0.042957</td>\n",
              "      <td>-0.008504</td>\n",
              "      <td>-0.136420</td>\n",
              "      <td>0.915835</td>\n",
              "      <td>0.063691</td>\n",
              "      <td>0.184653</td>\n",
              "      <td>-0.032327</td>\n",
              "      <td>-0.092390</td>\n",
              "      <td>0.063586</td>\n",
              "      <td>-0.156963</td>\n",
              "      <td>-0.029596</td>\n",
              "      <td>-0.011433</td>\n",
              "      <td>-0.048102</td>\n",
              "      <td>0.042451</td>\n",
              "      <td>-0.005934</td>\n",
              "      <td>0.045870</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.054205</td>\n",
              "      <td>0.028498</td>\n",
              "      <td>0.074691</td>\n",
              "      <td>-0.043320</td>\n",
              "      <td>-0.088574</td>\n",
              "      <td>0.034603</td>\n",
              "      <td>0.029922</td>\n",
              "      <td>-0.081509</td>\n",
              "      <td>0.042854</td>\n",
              "      <td>0.011205</td>\n",
              "      <td>0.029060</td>\n",
              "      <td>-0.063012</td>\n",
              "      <td>-0.101559</td>\n",
              "      <td>-0.107197</td>\n",
              "      <td>-0.023220</td>\n",
              "      <td>-0.147769</td>\n",
              "      <td>0.075356</td>\n",
              "      <td>-0.094736</td>\n",
              "      <td>0.024044</td>\n",
              "      <td>0.039865</td>\n",
              "      <td>0.064397</td>\n",
              "      <td>-0.008018</td>\n",
              "      <td>0.072907</td>\n",
              "      <td>0.087641</td>\n",
              "      <td>0.072423</td>\n",
              "      <td>-0.020360</td>\n",
              "      <td>0.055657</td>\n",
              "      <td>0.109679</td>\n",
              "      <td>-0.057609</td>\n",
              "      <td>0.006164</td>\n",
              "      <td>0.044117</td>\n",
              "      <td>-0.111159</td>\n",
              "      <td>0.013871</td>\n",
              "      <td>-0.073210</td>\n",
              "      <td>0.050130</td>\n",
              "      <td>0.073727</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>-0.046022</td>\n",
              "      <td>52.833333</td>\n",
              "      <td>1.160025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.059764</td>\n",
              "      <td>0.082836</td>\n",
              "      <td>-0.142219</td>\n",
              "      <td>0.091863</td>\n",
              "      <td>0.008776</td>\n",
              "      <td>0.022993</td>\n",
              "      <td>0.496680</td>\n",
              "      <td>0.012337</td>\n",
              "      <td>-0.051832</td>\n",
              "      <td>-0.047512</td>\n",
              "      <td>-0.153940</td>\n",
              "      <td>-0.058628</td>\n",
              "      <td>-0.631568</td>\n",
              "      <td>-0.112419</td>\n",
              "      <td>0.047878</td>\n",
              "      <td>-0.067804</td>\n",
              "      <td>-0.005677</td>\n",
              "      <td>0.102753</td>\n",
              "      <td>-0.063472</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>-0.096702</td>\n",
              "      <td>-0.053903</td>\n",
              "      <td>-0.121419</td>\n",
              "      <td>0.699968</td>\n",
              "      <td>0.095285</td>\n",
              "      <td>0.201253</td>\n",
              "      <td>0.020723</td>\n",
              "      <td>-0.041219</td>\n",
              "      <td>0.081264</td>\n",
              "      <td>-0.047501</td>\n",
              "      <td>-0.117670</td>\n",
              "      <td>0.023999</td>\n",
              "      <td>-0.120962</td>\n",
              "      <td>0.101529</td>\n",
              "      <td>0.109891</td>\n",
              "      <td>0.076047</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040244</td>\n",
              "      <td>0.052940</td>\n",
              "      <td>0.053133</td>\n",
              "      <td>-0.053241</td>\n",
              "      <td>-0.086791</td>\n",
              "      <td>0.024374</td>\n",
              "      <td>-0.005235</td>\n",
              "      <td>-0.038448</td>\n",
              "      <td>0.073409</td>\n",
              "      <td>-0.001047</td>\n",
              "      <td>-0.019316</td>\n",
              "      <td>0.021735</td>\n",
              "      <td>-0.098803</td>\n",
              "      <td>0.020090</td>\n",
              "      <td>-0.048372</td>\n",
              "      <td>-0.122186</td>\n",
              "      <td>0.046514</td>\n",
              "      <td>-0.057995</td>\n",
              "      <td>-0.045879</td>\n",
              "      <td>0.009031</td>\n",
              "      <td>0.035915</td>\n",
              "      <td>0.011018</td>\n",
              "      <td>0.094855</td>\n",
              "      <td>0.079106</td>\n",
              "      <td>0.024286</td>\n",
              "      <td>-0.030704</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.043099</td>\n",
              "      <td>-0.000574</td>\n",
              "      <td>0.019067</td>\n",
              "      <td>0.055732</td>\n",
              "      <td>-0.122686</td>\n",
              "      <td>-0.018306</td>\n",
              "      <td>-0.127239</td>\n",
              "      <td>0.117071</td>\n",
              "      <td>0.093071</td>\n",
              "      <td>0.058105</td>\n",
              "      <td>-0.142413</td>\n",
              "      <td>38.500000</td>\n",
              "      <td>1.047309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2490</th>\n",
              "      <td>2602</td>\n",
              "      <td>3.627907</td>\n",
              "      <td>0.003801</td>\n",
              "      <td>0.056678</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>0.027493</td>\n",
              "      <td>0.052407</td>\n",
              "      <td>0.068345</td>\n",
              "      <td>0.578416</td>\n",
              "      <td>-0.019550</td>\n",
              "      <td>-0.020487</td>\n",
              "      <td>-0.001910</td>\n",
              "      <td>-0.043694</td>\n",
              "      <td>-0.046127</td>\n",
              "      <td>-0.616708</td>\n",
              "      <td>-0.103197</td>\n",
              "      <td>0.031103</td>\n",
              "      <td>0.018686</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>0.050676</td>\n",
              "      <td>-0.057047</td>\n",
              "      <td>-0.133830</td>\n",
              "      <td>0.024876</td>\n",
              "      <td>0.004445</td>\n",
              "      <td>-0.072272</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>-0.094578</td>\n",
              "      <td>0.957469</td>\n",
              "      <td>0.067008</td>\n",
              "      <td>0.197532</td>\n",
              "      <td>-0.003215</td>\n",
              "      <td>-0.077643</td>\n",
              "      <td>0.069441</td>\n",
              "      <td>-0.122064</td>\n",
              "      <td>-0.030265</td>\n",
              "      <td>-0.005515</td>\n",
              "      <td>-0.063978</td>\n",
              "      <td>0.054702</td>\n",
              "      <td>0.010844</td>\n",
              "      <td>0.026273</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.056610</td>\n",
              "      <td>0.075562</td>\n",
              "      <td>0.064769</td>\n",
              "      <td>-0.059805</td>\n",
              "      <td>-0.117945</td>\n",
              "      <td>-0.012230</td>\n",
              "      <td>0.054759</td>\n",
              "      <td>-0.029953</td>\n",
              "      <td>0.055020</td>\n",
              "      <td>-0.010160</td>\n",
              "      <td>0.011240</td>\n",
              "      <td>-0.028164</td>\n",
              "      <td>-0.062938</td>\n",
              "      <td>-0.116394</td>\n",
              "      <td>-0.005935</td>\n",
              "      <td>-0.158633</td>\n",
              "      <td>0.076857</td>\n",
              "      <td>-0.080711</td>\n",
              "      <td>0.032272</td>\n",
              "      <td>0.044044</td>\n",
              "      <td>0.085706</td>\n",
              "      <td>0.001675</td>\n",
              "      <td>0.058613</td>\n",
              "      <td>0.095436</td>\n",
              "      <td>0.080023</td>\n",
              "      <td>-0.002176</td>\n",
              "      <td>0.087351</td>\n",
              "      <td>0.075854</td>\n",
              "      <td>-0.071634</td>\n",
              "      <td>0.016003</td>\n",
              "      <td>0.080881</td>\n",
              "      <td>-0.121475</td>\n",
              "      <td>0.041629</td>\n",
              "      <td>-0.096464</td>\n",
              "      <td>0.050183</td>\n",
              "      <td>0.050052</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>-0.044870</td>\n",
              "      <td>45.174419</td>\n",
              "      <td>1.091651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2491</th>\n",
              "      <td>2603</td>\n",
              "      <td>3.568182</td>\n",
              "      <td>0.008686</td>\n",
              "      <td>0.064240</td>\n",
              "      <td>-0.112123</td>\n",
              "      <td>0.066053</td>\n",
              "      <td>0.056164</td>\n",
              "      <td>0.060306</td>\n",
              "      <td>0.577812</td>\n",
              "      <td>-0.016179</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.021948</td>\n",
              "      <td>-0.024541</td>\n",
              "      <td>-0.066753</td>\n",
              "      <td>-0.615053</td>\n",
              "      <td>-0.100780</td>\n",
              "      <td>0.040845</td>\n",
              "      <td>0.020933</td>\n",
              "      <td>-0.057925</td>\n",
              "      <td>0.070935</td>\n",
              "      <td>-0.026832</td>\n",
              "      <td>-0.139642</td>\n",
              "      <td>0.042404</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>-0.058934</td>\n",
              "      <td>-0.007043</td>\n",
              "      <td>-0.099441</td>\n",
              "      <td>0.960317</td>\n",
              "      <td>0.111114</td>\n",
              "      <td>0.214441</td>\n",
              "      <td>-0.002811</td>\n",
              "      <td>-0.048085</td>\n",
              "      <td>0.058234</td>\n",
              "      <td>-0.147535</td>\n",
              "      <td>-0.019297</td>\n",
              "      <td>0.013371</td>\n",
              "      <td>-0.053802</td>\n",
              "      <td>0.068896</td>\n",
              "      <td>-0.002958</td>\n",
              "      <td>0.072276</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.065796</td>\n",
              "      <td>0.064095</td>\n",
              "      <td>0.058642</td>\n",
              "      <td>-0.062993</td>\n",
              "      <td>-0.134788</td>\n",
              "      <td>-0.014483</td>\n",
              "      <td>0.039297</td>\n",
              "      <td>-0.031786</td>\n",
              "      <td>0.073128</td>\n",
              "      <td>0.006764</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>-0.036176</td>\n",
              "      <td>-0.077153</td>\n",
              "      <td>-0.089316</td>\n",
              "      <td>0.005797</td>\n",
              "      <td>-0.152313</td>\n",
              "      <td>0.087047</td>\n",
              "      <td>-0.106796</td>\n",
              "      <td>0.026582</td>\n",
              "      <td>0.043631</td>\n",
              "      <td>0.072984</td>\n",
              "      <td>-0.001705</td>\n",
              "      <td>0.070524</td>\n",
              "      <td>0.102812</td>\n",
              "      <td>0.101850</td>\n",
              "      <td>0.022718</td>\n",
              "      <td>0.111631</td>\n",
              "      <td>0.079243</td>\n",
              "      <td>-0.074038</td>\n",
              "      <td>0.003718</td>\n",
              "      <td>0.083287</td>\n",
              "      <td>-0.114277</td>\n",
              "      <td>0.015116</td>\n",
              "      <td>-0.095072</td>\n",
              "      <td>0.048425</td>\n",
              "      <td>0.070964</td>\n",
              "      <td>0.012766</td>\n",
              "      <td>-0.067173</td>\n",
              "      <td>53.454545</td>\n",
              "      <td>1.147635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>2604</td>\n",
              "      <td>2.441176</td>\n",
              "      <td>0.017475</td>\n",
              "      <td>0.087743</td>\n",
              "      <td>-0.074101</td>\n",
              "      <td>0.031143</td>\n",
              "      <td>0.014160</td>\n",
              "      <td>0.044622</td>\n",
              "      <td>0.658050</td>\n",
              "      <td>0.023040</td>\n",
              "      <td>-0.023856</td>\n",
              "      <td>-0.000529</td>\n",
              "      <td>0.007301</td>\n",
              "      <td>-0.056878</td>\n",
              "      <td>-0.608873</td>\n",
              "      <td>-0.088152</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>-0.018370</td>\n",
              "      <td>-0.046578</td>\n",
              "      <td>0.075851</td>\n",
              "      <td>-0.059118</td>\n",
              "      <td>-0.114944</td>\n",
              "      <td>0.052351</td>\n",
              "      <td>0.010889</td>\n",
              "      <td>-0.057088</td>\n",
              "      <td>-0.028194</td>\n",
              "      <td>-0.063588</td>\n",
              "      <td>0.935718</td>\n",
              "      <td>0.056559</td>\n",
              "      <td>0.145997</td>\n",
              "      <td>0.016430</td>\n",
              "      <td>-0.077094</td>\n",
              "      <td>0.029316</td>\n",
              "      <td>-0.060867</td>\n",
              "      <td>-0.034266</td>\n",
              "      <td>0.025471</td>\n",
              "      <td>-0.112250</td>\n",
              "      <td>0.096995</td>\n",
              "      <td>-0.013451</td>\n",
              "      <td>0.016222</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042635</td>\n",
              "      <td>0.040778</td>\n",
              "      <td>0.069572</td>\n",
              "      <td>-0.057148</td>\n",
              "      <td>-0.095314</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>0.014888</td>\n",
              "      <td>-0.061426</td>\n",
              "      <td>0.038052</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.005368</td>\n",
              "      <td>-0.045913</td>\n",
              "      <td>-0.098609</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>-0.026113</td>\n",
              "      <td>-0.163115</td>\n",
              "      <td>0.047714</td>\n",
              "      <td>-0.059092</td>\n",
              "      <td>-0.000953</td>\n",
              "      <td>0.066118</td>\n",
              "      <td>0.071290</td>\n",
              "      <td>0.021352</td>\n",
              "      <td>0.038844</td>\n",
              "      <td>0.069540</td>\n",
              "      <td>0.061056</td>\n",
              "      <td>-0.009095</td>\n",
              "      <td>0.086287</td>\n",
              "      <td>0.071161</td>\n",
              "      <td>-0.041962</td>\n",
              "      <td>0.025281</td>\n",
              "      <td>0.079294</td>\n",
              "      <td>-0.082179</td>\n",
              "      <td>0.017093</td>\n",
              "      <td>-0.093254</td>\n",
              "      <td>0.031321</td>\n",
              "      <td>0.093794</td>\n",
              "      <td>0.016454</td>\n",
              "      <td>-0.053786</td>\n",
              "      <td>50.529412</td>\n",
              "      <td>1.124702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>2605</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>0.016577</td>\n",
              "      <td>0.060966</td>\n",
              "      <td>-0.087064</td>\n",
              "      <td>0.046408</td>\n",
              "      <td>0.029821</td>\n",
              "      <td>0.040159</td>\n",
              "      <td>0.627986</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>-0.015949</td>\n",
              "      <td>0.013601</td>\n",
              "      <td>-0.004045</td>\n",
              "      <td>-0.050633</td>\n",
              "      <td>-0.638278</td>\n",
              "      <td>-0.092950</td>\n",
              "      <td>0.059763</td>\n",
              "      <td>-0.007455</td>\n",
              "      <td>-0.027335</td>\n",
              "      <td>0.050195</td>\n",
              "      <td>-0.072600</td>\n",
              "      <td>-0.118941</td>\n",
              "      <td>0.043963</td>\n",
              "      <td>0.028052</td>\n",
              "      <td>-0.020707</td>\n",
              "      <td>0.017013</td>\n",
              "      <td>-0.100314</td>\n",
              "      <td>0.924332</td>\n",
              "      <td>0.066779</td>\n",
              "      <td>0.184031</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>-0.087042</td>\n",
              "      <td>0.082218</td>\n",
              "      <td>-0.084034</td>\n",
              "      <td>-0.053479</td>\n",
              "      <td>0.020944</td>\n",
              "      <td>-0.107375</td>\n",
              "      <td>0.062292</td>\n",
              "      <td>0.004020</td>\n",
              "      <td>0.050163</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040871</td>\n",
              "      <td>0.023813</td>\n",
              "      <td>0.060147</td>\n",
              "      <td>-0.026382</td>\n",
              "      <td>-0.114827</td>\n",
              "      <td>-0.027009</td>\n",
              "      <td>0.027002</td>\n",
              "      <td>-0.049506</td>\n",
              "      <td>0.053767</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.020552</td>\n",
              "      <td>-0.024567</td>\n",
              "      <td>-0.095950</td>\n",
              "      <td>-0.102344</td>\n",
              "      <td>-0.014801</td>\n",
              "      <td>-0.154797</td>\n",
              "      <td>0.043244</td>\n",
              "      <td>-0.085830</td>\n",
              "      <td>0.016672</td>\n",
              "      <td>0.063306</td>\n",
              "      <td>0.065970</td>\n",
              "      <td>-0.007496</td>\n",
              "      <td>0.042143</td>\n",
              "      <td>0.109708</td>\n",
              "      <td>0.071727</td>\n",
              "      <td>-0.017870</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.060924</td>\n",
              "      <td>-0.063255</td>\n",
              "      <td>-0.002328</td>\n",
              "      <td>0.090009</td>\n",
              "      <td>-0.110579</td>\n",
              "      <td>0.055245</td>\n",
              "      <td>-0.057162</td>\n",
              "      <td>0.068616</td>\n",
              "      <td>0.101482</td>\n",
              "      <td>-0.015338</td>\n",
              "      <td>-0.062622</td>\n",
              "      <td>49.250000</td>\n",
              "      <td>1.165649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>2606</td>\n",
              "      <td>1.571429</td>\n",
              "      <td>0.027545</td>\n",
              "      <td>0.102103</td>\n",
              "      <td>-0.087128</td>\n",
              "      <td>0.025585</td>\n",
              "      <td>0.010081</td>\n",
              "      <td>0.062728</td>\n",
              "      <td>0.650946</td>\n",
              "      <td>0.008557</td>\n",
              "      <td>-0.010532</td>\n",
              "      <td>0.013319</td>\n",
              "      <td>-0.033369</td>\n",
              "      <td>-0.055501</td>\n",
              "      <td>-0.609024</td>\n",
              "      <td>-0.084756</td>\n",
              "      <td>0.033530</td>\n",
              "      <td>-0.018587</td>\n",
              "      <td>-0.018317</td>\n",
              "      <td>0.088617</td>\n",
              "      <td>-0.088441</td>\n",
              "      <td>-0.114973</td>\n",
              "      <td>0.047151</td>\n",
              "      <td>0.022768</td>\n",
              "      <td>-0.068239</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>-0.096126</td>\n",
              "      <td>0.889699</td>\n",
              "      <td>0.070293</td>\n",
              "      <td>0.139533</td>\n",
              "      <td>0.015974</td>\n",
              "      <td>-0.090256</td>\n",
              "      <td>0.050364</td>\n",
              "      <td>-0.099643</td>\n",
              "      <td>-0.038802</td>\n",
              "      <td>0.019065</td>\n",
              "      <td>-0.100004</td>\n",
              "      <td>0.070795</td>\n",
              "      <td>0.012292</td>\n",
              "      <td>0.015073</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.063595</td>\n",
              "      <td>0.036156</td>\n",
              "      <td>0.075096</td>\n",
              "      <td>-0.039093</td>\n",
              "      <td>-0.078704</td>\n",
              "      <td>0.006561</td>\n",
              "      <td>0.020117</td>\n",
              "      <td>-0.029605</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>-0.004767</td>\n",
              "      <td>0.030869</td>\n",
              "      <td>-0.015064</td>\n",
              "      <td>-0.095884</td>\n",
              "      <td>-0.097213</td>\n",
              "      <td>-0.040490</td>\n",
              "      <td>-0.167914</td>\n",
              "      <td>0.057458</td>\n",
              "      <td>-0.062123</td>\n",
              "      <td>-0.006414</td>\n",
              "      <td>0.057779</td>\n",
              "      <td>0.056900</td>\n",
              "      <td>0.026271</td>\n",
              "      <td>0.029138</td>\n",
              "      <td>0.068665</td>\n",
              "      <td>0.062784</td>\n",
              "      <td>-0.039613</td>\n",
              "      <td>0.094178</td>\n",
              "      <td>0.077215</td>\n",
              "      <td>-0.044480</td>\n",
              "      <td>0.013093</td>\n",
              "      <td>0.072874</td>\n",
              "      <td>-0.092804</td>\n",
              "      <td>0.031574</td>\n",
              "      <td>-0.079673</td>\n",
              "      <td>0.076916</td>\n",
              "      <td>0.104879</td>\n",
              "      <td>0.033944</td>\n",
              "      <td>-0.061523</td>\n",
              "      <td>86.885714</td>\n",
              "      <td>1.219102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2495 rows × 204 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      doctorID    rating      AWE1  ...    AWE200     Length  Lexical Diversity\n",
              "0            0  4.450617 -0.004739  ... -0.055928  43.753086           1.105869\n",
              "1            1  2.810811  0.017838  ... -0.058888  48.432432           1.105801\n",
              "2            2  4.904762 -0.025036  ... -0.079230  77.571429           1.171556\n",
              "3            3  2.416667  0.038018  ... -0.046022  52.833333           1.160025\n",
              "4            4  3.000000  0.059764  ... -0.142413  38.500000           1.047309\n",
              "...        ...       ...       ...  ...       ...        ...                ...\n",
              "2490      2602  3.627907  0.003801  ... -0.044870  45.174419           1.091651\n",
              "2491      2603  3.568182  0.008686  ... -0.067173  53.454545           1.147635\n",
              "2492      2604  2.441176  0.017475  ... -0.053786  50.529412           1.124702\n",
              "2493      2605  1.750000  0.016577  ... -0.062622  49.250000           1.165649\n",
              "2494      2606  1.571429  0.027545  ... -0.061523  86.885714           1.219102\n",
              "\n",
              "[2495 rows x 204 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbJjK4EKltiI"
      },
      "source": [
        "df_glove_agg = df_glove_agg.assign(Recommend = [1 if x >= 4 else 0 for x in df_glove_agg['rating'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "l44RFqT1pISq",
        "outputId": "5510f3da-7e0c-479b-e94e-f137a5e2b8d7"
      },
      "source": [
        "df_glove_agg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doctorID</th>\n",
              "      <th>rating</th>\n",
              "      <th>AWE1</th>\n",
              "      <th>AWE2</th>\n",
              "      <th>AWE3</th>\n",
              "      <th>AWE4</th>\n",
              "      <th>AWE5</th>\n",
              "      <th>AWE6</th>\n",
              "      <th>AWE7</th>\n",
              "      <th>AWE8</th>\n",
              "      <th>AWE9</th>\n",
              "      <th>AWE10</th>\n",
              "      <th>AWE11</th>\n",
              "      <th>AWE12</th>\n",
              "      <th>AWE13</th>\n",
              "      <th>AWE14</th>\n",
              "      <th>AWE15</th>\n",
              "      <th>AWE16</th>\n",
              "      <th>AWE17</th>\n",
              "      <th>AWE18</th>\n",
              "      <th>AWE19</th>\n",
              "      <th>AWE20</th>\n",
              "      <th>AWE21</th>\n",
              "      <th>AWE22</th>\n",
              "      <th>AWE23</th>\n",
              "      <th>AWE24</th>\n",
              "      <th>AWE25</th>\n",
              "      <th>AWE26</th>\n",
              "      <th>AWE27</th>\n",
              "      <th>AWE28</th>\n",
              "      <th>AWE29</th>\n",
              "      <th>AWE30</th>\n",
              "      <th>AWE31</th>\n",
              "      <th>AWE32</th>\n",
              "      <th>AWE33</th>\n",
              "      <th>AWE34</th>\n",
              "      <th>AWE35</th>\n",
              "      <th>AWE36</th>\n",
              "      <th>AWE37</th>\n",
              "      <th>AWE38</th>\n",
              "      <th>...</th>\n",
              "      <th>AWE164</th>\n",
              "      <th>AWE165</th>\n",
              "      <th>AWE166</th>\n",
              "      <th>AWE167</th>\n",
              "      <th>AWE168</th>\n",
              "      <th>AWE169</th>\n",
              "      <th>AWE170</th>\n",
              "      <th>AWE171</th>\n",
              "      <th>AWE172</th>\n",
              "      <th>AWE173</th>\n",
              "      <th>AWE174</th>\n",
              "      <th>AWE175</th>\n",
              "      <th>AWE176</th>\n",
              "      <th>AWE177</th>\n",
              "      <th>AWE178</th>\n",
              "      <th>AWE179</th>\n",
              "      <th>AWE180</th>\n",
              "      <th>AWE181</th>\n",
              "      <th>AWE182</th>\n",
              "      <th>AWE183</th>\n",
              "      <th>AWE184</th>\n",
              "      <th>AWE185</th>\n",
              "      <th>AWE186</th>\n",
              "      <th>AWE187</th>\n",
              "      <th>AWE188</th>\n",
              "      <th>AWE189</th>\n",
              "      <th>AWE190</th>\n",
              "      <th>AWE191</th>\n",
              "      <th>AWE192</th>\n",
              "      <th>AWE193</th>\n",
              "      <th>AWE194</th>\n",
              "      <th>AWE195</th>\n",
              "      <th>AWE196</th>\n",
              "      <th>AWE197</th>\n",
              "      <th>AWE198</th>\n",
              "      <th>AWE199</th>\n",
              "      <th>AWE200</th>\n",
              "      <th>Length</th>\n",
              "      <th>Lexical Diversity</th>\n",
              "      <th>Recommend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4.450617</td>\n",
              "      <td>-0.004739</td>\n",
              "      <td>0.109580</td>\n",
              "      <td>-0.055467</td>\n",
              "      <td>0.032624</td>\n",
              "      <td>0.037863</td>\n",
              "      <td>0.068353</td>\n",
              "      <td>0.612281</td>\n",
              "      <td>0.007825</td>\n",
              "      <td>-0.014816</td>\n",
              "      <td>-0.041297</td>\n",
              "      <td>-0.101164</td>\n",
              "      <td>-0.085488</td>\n",
              "      <td>-0.601401</td>\n",
              "      <td>-0.117590</td>\n",
              "      <td>0.053162</td>\n",
              "      <td>0.003640</td>\n",
              "      <td>-0.067281</td>\n",
              "      <td>0.082147</td>\n",
              "      <td>-0.030303</td>\n",
              "      <td>-0.107096</td>\n",
              "      <td>0.044002</td>\n",
              "      <td>0.049359</td>\n",
              "      <td>-0.116865</td>\n",
              "      <td>0.018457</td>\n",
              "      <td>-0.112021</td>\n",
              "      <td>0.879065</td>\n",
              "      <td>0.039489</td>\n",
              "      <td>0.179910</td>\n",
              "      <td>0.006141</td>\n",
              "      <td>-0.072227</td>\n",
              "      <td>0.076318</td>\n",
              "      <td>-0.186713</td>\n",
              "      <td>-0.030613</td>\n",
              "      <td>0.016760</td>\n",
              "      <td>-0.072900</td>\n",
              "      <td>0.061665</td>\n",
              "      <td>0.051384</td>\n",
              "      <td>0.016398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031078</td>\n",
              "      <td>0.008759</td>\n",
              "      <td>-0.084882</td>\n",
              "      <td>-0.113550</td>\n",
              "      <td>0.069989</td>\n",
              "      <td>0.046964</td>\n",
              "      <td>-0.036594</td>\n",
              "      <td>0.045713</td>\n",
              "      <td>-0.038768</td>\n",
              "      <td>0.079776</td>\n",
              "      <td>-0.029784</td>\n",
              "      <td>-0.094229</td>\n",
              "      <td>-0.133694</td>\n",
              "      <td>-0.020224</td>\n",
              "      <td>-0.172105</td>\n",
              "      <td>0.097750</td>\n",
              "      <td>-0.083468</td>\n",
              "      <td>0.055543</td>\n",
              "      <td>0.024740</td>\n",
              "      <td>0.052234</td>\n",
              "      <td>-0.016325</td>\n",
              "      <td>0.072179</td>\n",
              "      <td>0.100411</td>\n",
              "      <td>0.040954</td>\n",
              "      <td>-0.030364</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>0.094056</td>\n",
              "      <td>-0.054656</td>\n",
              "      <td>0.015817</td>\n",
              "      <td>0.100769</td>\n",
              "      <td>-0.119765</td>\n",
              "      <td>0.007846</td>\n",
              "      <td>-0.067108</td>\n",
              "      <td>0.077146</td>\n",
              "      <td>0.048946</td>\n",
              "      <td>0.017984</td>\n",
              "      <td>-0.055928</td>\n",
              "      <td>43.753086</td>\n",
              "      <td>1.105869</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.810811</td>\n",
              "      <td>0.017838</td>\n",
              "      <td>0.090133</td>\n",
              "      <td>-0.060930</td>\n",
              "      <td>0.026661</td>\n",
              "      <td>0.045295</td>\n",
              "      <td>0.027326</td>\n",
              "      <td>0.603496</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>0.009490</td>\n",
              "      <td>-0.014742</td>\n",
              "      <td>-0.067378</td>\n",
              "      <td>-0.084041</td>\n",
              "      <td>-0.580594</td>\n",
              "      <td>-0.093487</td>\n",
              "      <td>0.070746</td>\n",
              "      <td>-0.043194</td>\n",
              "      <td>-0.051184</td>\n",
              "      <td>0.096345</td>\n",
              "      <td>-0.026170</td>\n",
              "      <td>-0.138979</td>\n",
              "      <td>0.037236</td>\n",
              "      <td>0.048987</td>\n",
              "      <td>-0.073811</td>\n",
              "      <td>-0.022814</td>\n",
              "      <td>-0.104906</td>\n",
              "      <td>0.885185</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>0.171312</td>\n",
              "      <td>0.001509</td>\n",
              "      <td>-0.073263</td>\n",
              "      <td>0.063365</td>\n",
              "      <td>-0.157461</td>\n",
              "      <td>-0.018407</td>\n",
              "      <td>0.014403</td>\n",
              "      <td>-0.072707</td>\n",
              "      <td>0.078101</td>\n",
              "      <td>0.015396</td>\n",
              "      <td>0.041139</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011204</td>\n",
              "      <td>0.055701</td>\n",
              "      <td>-0.092004</td>\n",
              "      <td>-0.145391</td>\n",
              "      <td>0.040438</td>\n",
              "      <td>0.061408</td>\n",
              "      <td>-0.012917</td>\n",
              "      <td>0.034453</td>\n",
              "      <td>-0.046430</td>\n",
              "      <td>0.042516</td>\n",
              "      <td>-0.029370</td>\n",
              "      <td>-0.103526</td>\n",
              "      <td>-0.126716</td>\n",
              "      <td>-0.010893</td>\n",
              "      <td>-0.185730</td>\n",
              "      <td>0.065239</td>\n",
              "      <td>-0.097023</td>\n",
              "      <td>0.022581</td>\n",
              "      <td>0.024610</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.025163</td>\n",
              "      <td>0.052349</td>\n",
              "      <td>0.119964</td>\n",
              "      <td>0.056785</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.060572</td>\n",
              "      <td>0.080504</td>\n",
              "      <td>-0.033145</td>\n",
              "      <td>0.028659</td>\n",
              "      <td>0.078784</td>\n",
              "      <td>-0.104174</td>\n",
              "      <td>0.013709</td>\n",
              "      <td>-0.071503</td>\n",
              "      <td>0.064676</td>\n",
              "      <td>0.080182</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.058888</td>\n",
              "      <td>48.432432</td>\n",
              "      <td>1.105801</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4.904762</td>\n",
              "      <td>-0.025036</td>\n",
              "      <td>0.057492</td>\n",
              "      <td>-0.032071</td>\n",
              "      <td>0.002374</td>\n",
              "      <td>-0.020498</td>\n",
              "      <td>0.049577</td>\n",
              "      <td>0.595520</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>0.010198</td>\n",
              "      <td>-0.019621</td>\n",
              "      <td>-0.100948</td>\n",
              "      <td>-0.005968</td>\n",
              "      <td>-0.577615</td>\n",
              "      <td>-0.062437</td>\n",
              "      <td>-0.002557</td>\n",
              "      <td>0.098014</td>\n",
              "      <td>-0.010020</td>\n",
              "      <td>0.054560</td>\n",
              "      <td>-0.052749</td>\n",
              "      <td>-0.064203</td>\n",
              "      <td>0.037048</td>\n",
              "      <td>0.028531</td>\n",
              "      <td>-0.062374</td>\n",
              "      <td>0.081435</td>\n",
              "      <td>-0.109872</td>\n",
              "      <td>0.939803</td>\n",
              "      <td>0.014482</td>\n",
              "      <td>0.209217</td>\n",
              "      <td>0.079155</td>\n",
              "      <td>-0.053990</td>\n",
              "      <td>0.061940</td>\n",
              "      <td>-0.113278</td>\n",
              "      <td>-0.063614</td>\n",
              "      <td>0.041954</td>\n",
              "      <td>-0.102660</td>\n",
              "      <td>0.031450</td>\n",
              "      <td>0.066701</td>\n",
              "      <td>0.024994</td>\n",
              "      <td>...</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.023759</td>\n",
              "      <td>-0.112784</td>\n",
              "      <td>-0.042657</td>\n",
              "      <td>0.032377</td>\n",
              "      <td>0.002203</td>\n",
              "      <td>0.029569</td>\n",
              "      <td>0.152568</td>\n",
              "      <td>0.015837</td>\n",
              "      <td>0.012060</td>\n",
              "      <td>-0.097442</td>\n",
              "      <td>-0.116197</td>\n",
              "      <td>-0.121550</td>\n",
              "      <td>-0.015339</td>\n",
              "      <td>-0.175676</td>\n",
              "      <td>0.036518</td>\n",
              "      <td>-0.135913</td>\n",
              "      <td>0.009221</td>\n",
              "      <td>0.021252</td>\n",
              "      <td>-0.038877</td>\n",
              "      <td>-0.019115</td>\n",
              "      <td>0.111799</td>\n",
              "      <td>-0.028155</td>\n",
              "      <td>0.045435</td>\n",
              "      <td>0.063098</td>\n",
              "      <td>0.003909</td>\n",
              "      <td>0.066870</td>\n",
              "      <td>-0.106063</td>\n",
              "      <td>0.050678</td>\n",
              "      <td>0.084019</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>0.045542</td>\n",
              "      <td>-0.073244</td>\n",
              "      <td>0.129461</td>\n",
              "      <td>0.045889</td>\n",
              "      <td>0.025196</td>\n",
              "      <td>-0.079230</td>\n",
              "      <td>77.571429</td>\n",
              "      <td>1.171556</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2.416667</td>\n",
              "      <td>0.038018</td>\n",
              "      <td>0.105665</td>\n",
              "      <td>-0.064177</td>\n",
              "      <td>0.024043</td>\n",
              "      <td>0.050930</td>\n",
              "      <td>0.014426</td>\n",
              "      <td>0.615250</td>\n",
              "      <td>-0.002282</td>\n",
              "      <td>0.020949</td>\n",
              "      <td>-0.056484</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>-0.049697</td>\n",
              "      <td>-0.619148</td>\n",
              "      <td>-0.102900</td>\n",
              "      <td>0.090590</td>\n",
              "      <td>-0.014152</td>\n",
              "      <td>-0.010258</td>\n",
              "      <td>0.075215</td>\n",
              "      <td>-0.044463</td>\n",
              "      <td>-0.141619</td>\n",
              "      <td>0.025299</td>\n",
              "      <td>0.041922</td>\n",
              "      <td>-0.042957</td>\n",
              "      <td>-0.008504</td>\n",
              "      <td>-0.136420</td>\n",
              "      <td>0.915835</td>\n",
              "      <td>0.063691</td>\n",
              "      <td>0.184653</td>\n",
              "      <td>-0.032327</td>\n",
              "      <td>-0.092390</td>\n",
              "      <td>0.063586</td>\n",
              "      <td>-0.156963</td>\n",
              "      <td>-0.029596</td>\n",
              "      <td>-0.011433</td>\n",
              "      <td>-0.048102</td>\n",
              "      <td>0.042451</td>\n",
              "      <td>-0.005934</td>\n",
              "      <td>0.045870</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028498</td>\n",
              "      <td>0.074691</td>\n",
              "      <td>-0.043320</td>\n",
              "      <td>-0.088574</td>\n",
              "      <td>0.034603</td>\n",
              "      <td>0.029922</td>\n",
              "      <td>-0.081509</td>\n",
              "      <td>0.042854</td>\n",
              "      <td>0.011205</td>\n",
              "      <td>0.029060</td>\n",
              "      <td>-0.063012</td>\n",
              "      <td>-0.101559</td>\n",
              "      <td>-0.107197</td>\n",
              "      <td>-0.023220</td>\n",
              "      <td>-0.147769</td>\n",
              "      <td>0.075356</td>\n",
              "      <td>-0.094736</td>\n",
              "      <td>0.024044</td>\n",
              "      <td>0.039865</td>\n",
              "      <td>0.064397</td>\n",
              "      <td>-0.008018</td>\n",
              "      <td>0.072907</td>\n",
              "      <td>0.087641</td>\n",
              "      <td>0.072423</td>\n",
              "      <td>-0.020360</td>\n",
              "      <td>0.055657</td>\n",
              "      <td>0.109679</td>\n",
              "      <td>-0.057609</td>\n",
              "      <td>0.006164</td>\n",
              "      <td>0.044117</td>\n",
              "      <td>-0.111159</td>\n",
              "      <td>0.013871</td>\n",
              "      <td>-0.073210</td>\n",
              "      <td>0.050130</td>\n",
              "      <td>0.073727</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>-0.046022</td>\n",
              "      <td>52.833333</td>\n",
              "      <td>1.160025</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.059764</td>\n",
              "      <td>0.082836</td>\n",
              "      <td>-0.142219</td>\n",
              "      <td>0.091863</td>\n",
              "      <td>0.008776</td>\n",
              "      <td>0.022993</td>\n",
              "      <td>0.496680</td>\n",
              "      <td>0.012337</td>\n",
              "      <td>-0.051832</td>\n",
              "      <td>-0.047512</td>\n",
              "      <td>-0.153940</td>\n",
              "      <td>-0.058628</td>\n",
              "      <td>-0.631568</td>\n",
              "      <td>-0.112419</td>\n",
              "      <td>0.047878</td>\n",
              "      <td>-0.067804</td>\n",
              "      <td>-0.005677</td>\n",
              "      <td>0.102753</td>\n",
              "      <td>-0.063472</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>-0.096702</td>\n",
              "      <td>-0.053903</td>\n",
              "      <td>-0.121419</td>\n",
              "      <td>0.699968</td>\n",
              "      <td>0.095285</td>\n",
              "      <td>0.201253</td>\n",
              "      <td>0.020723</td>\n",
              "      <td>-0.041219</td>\n",
              "      <td>0.081264</td>\n",
              "      <td>-0.047501</td>\n",
              "      <td>-0.117670</td>\n",
              "      <td>0.023999</td>\n",
              "      <td>-0.120962</td>\n",
              "      <td>0.101529</td>\n",
              "      <td>0.109891</td>\n",
              "      <td>0.076047</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052940</td>\n",
              "      <td>0.053133</td>\n",
              "      <td>-0.053241</td>\n",
              "      <td>-0.086791</td>\n",
              "      <td>0.024374</td>\n",
              "      <td>-0.005235</td>\n",
              "      <td>-0.038448</td>\n",
              "      <td>0.073409</td>\n",
              "      <td>-0.001047</td>\n",
              "      <td>-0.019316</td>\n",
              "      <td>0.021735</td>\n",
              "      <td>-0.098803</td>\n",
              "      <td>0.020090</td>\n",
              "      <td>-0.048372</td>\n",
              "      <td>-0.122186</td>\n",
              "      <td>0.046514</td>\n",
              "      <td>-0.057995</td>\n",
              "      <td>-0.045879</td>\n",
              "      <td>0.009031</td>\n",
              "      <td>0.035915</td>\n",
              "      <td>0.011018</td>\n",
              "      <td>0.094855</td>\n",
              "      <td>0.079106</td>\n",
              "      <td>0.024286</td>\n",
              "      <td>-0.030704</td>\n",
              "      <td>0.040649</td>\n",
              "      <td>0.043099</td>\n",
              "      <td>-0.000574</td>\n",
              "      <td>0.019067</td>\n",
              "      <td>0.055732</td>\n",
              "      <td>-0.122686</td>\n",
              "      <td>-0.018306</td>\n",
              "      <td>-0.127239</td>\n",
              "      <td>0.117071</td>\n",
              "      <td>0.093071</td>\n",
              "      <td>0.058105</td>\n",
              "      <td>-0.142413</td>\n",
              "      <td>38.500000</td>\n",
              "      <td>1.047309</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2490</th>\n",
              "      <td>2602</td>\n",
              "      <td>3.627907</td>\n",
              "      <td>0.003801</td>\n",
              "      <td>0.056678</td>\n",
              "      <td>-0.079002</td>\n",
              "      <td>0.027493</td>\n",
              "      <td>0.052407</td>\n",
              "      <td>0.068345</td>\n",
              "      <td>0.578416</td>\n",
              "      <td>-0.019550</td>\n",
              "      <td>-0.020487</td>\n",
              "      <td>-0.001910</td>\n",
              "      <td>-0.043694</td>\n",
              "      <td>-0.046127</td>\n",
              "      <td>-0.616708</td>\n",
              "      <td>-0.103197</td>\n",
              "      <td>0.031103</td>\n",
              "      <td>0.018686</td>\n",
              "      <td>-0.025482</td>\n",
              "      <td>0.050676</td>\n",
              "      <td>-0.057047</td>\n",
              "      <td>-0.133830</td>\n",
              "      <td>0.024876</td>\n",
              "      <td>0.004445</td>\n",
              "      <td>-0.072272</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>-0.094578</td>\n",
              "      <td>0.957469</td>\n",
              "      <td>0.067008</td>\n",
              "      <td>0.197532</td>\n",
              "      <td>-0.003215</td>\n",
              "      <td>-0.077643</td>\n",
              "      <td>0.069441</td>\n",
              "      <td>-0.122064</td>\n",
              "      <td>-0.030265</td>\n",
              "      <td>-0.005515</td>\n",
              "      <td>-0.063978</td>\n",
              "      <td>0.054702</td>\n",
              "      <td>0.010844</td>\n",
              "      <td>0.026273</td>\n",
              "      <td>...</td>\n",
              "      <td>0.075562</td>\n",
              "      <td>0.064769</td>\n",
              "      <td>-0.059805</td>\n",
              "      <td>-0.117945</td>\n",
              "      <td>-0.012230</td>\n",
              "      <td>0.054759</td>\n",
              "      <td>-0.029953</td>\n",
              "      <td>0.055020</td>\n",
              "      <td>-0.010160</td>\n",
              "      <td>0.011240</td>\n",
              "      <td>-0.028164</td>\n",
              "      <td>-0.062938</td>\n",
              "      <td>-0.116394</td>\n",
              "      <td>-0.005935</td>\n",
              "      <td>-0.158633</td>\n",
              "      <td>0.076857</td>\n",
              "      <td>-0.080711</td>\n",
              "      <td>0.032272</td>\n",
              "      <td>0.044044</td>\n",
              "      <td>0.085706</td>\n",
              "      <td>0.001675</td>\n",
              "      <td>0.058613</td>\n",
              "      <td>0.095436</td>\n",
              "      <td>0.080023</td>\n",
              "      <td>-0.002176</td>\n",
              "      <td>0.087351</td>\n",
              "      <td>0.075854</td>\n",
              "      <td>-0.071634</td>\n",
              "      <td>0.016003</td>\n",
              "      <td>0.080881</td>\n",
              "      <td>-0.121475</td>\n",
              "      <td>0.041629</td>\n",
              "      <td>-0.096464</td>\n",
              "      <td>0.050183</td>\n",
              "      <td>0.050052</td>\n",
              "      <td>0.008529</td>\n",
              "      <td>-0.044870</td>\n",
              "      <td>45.174419</td>\n",
              "      <td>1.091651</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2491</th>\n",
              "      <td>2603</td>\n",
              "      <td>3.568182</td>\n",
              "      <td>0.008686</td>\n",
              "      <td>0.064240</td>\n",
              "      <td>-0.112123</td>\n",
              "      <td>0.066053</td>\n",
              "      <td>0.056164</td>\n",
              "      <td>0.060306</td>\n",
              "      <td>0.577812</td>\n",
              "      <td>-0.016179</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.021948</td>\n",
              "      <td>-0.024541</td>\n",
              "      <td>-0.066753</td>\n",
              "      <td>-0.615053</td>\n",
              "      <td>-0.100780</td>\n",
              "      <td>0.040845</td>\n",
              "      <td>0.020933</td>\n",
              "      <td>-0.057925</td>\n",
              "      <td>0.070935</td>\n",
              "      <td>-0.026832</td>\n",
              "      <td>-0.139642</td>\n",
              "      <td>0.042404</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>-0.058934</td>\n",
              "      <td>-0.007043</td>\n",
              "      <td>-0.099441</td>\n",
              "      <td>0.960317</td>\n",
              "      <td>0.111114</td>\n",
              "      <td>0.214441</td>\n",
              "      <td>-0.002811</td>\n",
              "      <td>-0.048085</td>\n",
              "      <td>0.058234</td>\n",
              "      <td>-0.147535</td>\n",
              "      <td>-0.019297</td>\n",
              "      <td>0.013371</td>\n",
              "      <td>-0.053802</td>\n",
              "      <td>0.068896</td>\n",
              "      <td>-0.002958</td>\n",
              "      <td>0.072276</td>\n",
              "      <td>...</td>\n",
              "      <td>0.064095</td>\n",
              "      <td>0.058642</td>\n",
              "      <td>-0.062993</td>\n",
              "      <td>-0.134788</td>\n",
              "      <td>-0.014483</td>\n",
              "      <td>0.039297</td>\n",
              "      <td>-0.031786</td>\n",
              "      <td>0.073128</td>\n",
              "      <td>0.006764</td>\n",
              "      <td>0.007313</td>\n",
              "      <td>-0.036176</td>\n",
              "      <td>-0.077153</td>\n",
              "      <td>-0.089316</td>\n",
              "      <td>0.005797</td>\n",
              "      <td>-0.152313</td>\n",
              "      <td>0.087047</td>\n",
              "      <td>-0.106796</td>\n",
              "      <td>0.026582</td>\n",
              "      <td>0.043631</td>\n",
              "      <td>0.072984</td>\n",
              "      <td>-0.001705</td>\n",
              "      <td>0.070524</td>\n",
              "      <td>0.102812</td>\n",
              "      <td>0.101850</td>\n",
              "      <td>0.022718</td>\n",
              "      <td>0.111631</td>\n",
              "      <td>0.079243</td>\n",
              "      <td>-0.074038</td>\n",
              "      <td>0.003718</td>\n",
              "      <td>0.083287</td>\n",
              "      <td>-0.114277</td>\n",
              "      <td>0.015116</td>\n",
              "      <td>-0.095072</td>\n",
              "      <td>0.048425</td>\n",
              "      <td>0.070964</td>\n",
              "      <td>0.012766</td>\n",
              "      <td>-0.067173</td>\n",
              "      <td>53.454545</td>\n",
              "      <td>1.147635</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2492</th>\n",
              "      <td>2604</td>\n",
              "      <td>2.441176</td>\n",
              "      <td>0.017475</td>\n",
              "      <td>0.087743</td>\n",
              "      <td>-0.074101</td>\n",
              "      <td>0.031143</td>\n",
              "      <td>0.014160</td>\n",
              "      <td>0.044622</td>\n",
              "      <td>0.658050</td>\n",
              "      <td>0.023040</td>\n",
              "      <td>-0.023856</td>\n",
              "      <td>-0.000529</td>\n",
              "      <td>0.007301</td>\n",
              "      <td>-0.056878</td>\n",
              "      <td>-0.608873</td>\n",
              "      <td>-0.088152</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>-0.018370</td>\n",
              "      <td>-0.046578</td>\n",
              "      <td>0.075851</td>\n",
              "      <td>-0.059118</td>\n",
              "      <td>-0.114944</td>\n",
              "      <td>0.052351</td>\n",
              "      <td>0.010889</td>\n",
              "      <td>-0.057088</td>\n",
              "      <td>-0.028194</td>\n",
              "      <td>-0.063588</td>\n",
              "      <td>0.935718</td>\n",
              "      <td>0.056559</td>\n",
              "      <td>0.145997</td>\n",
              "      <td>0.016430</td>\n",
              "      <td>-0.077094</td>\n",
              "      <td>0.029316</td>\n",
              "      <td>-0.060867</td>\n",
              "      <td>-0.034266</td>\n",
              "      <td>0.025471</td>\n",
              "      <td>-0.112250</td>\n",
              "      <td>0.096995</td>\n",
              "      <td>-0.013451</td>\n",
              "      <td>0.016222</td>\n",
              "      <td>...</td>\n",
              "      <td>0.040778</td>\n",
              "      <td>0.069572</td>\n",
              "      <td>-0.057148</td>\n",
              "      <td>-0.095314</td>\n",
              "      <td>0.004086</td>\n",
              "      <td>0.014888</td>\n",
              "      <td>-0.061426</td>\n",
              "      <td>0.038052</td>\n",
              "      <td>-0.000973</td>\n",
              "      <td>0.005368</td>\n",
              "      <td>-0.045913</td>\n",
              "      <td>-0.098609</td>\n",
              "      <td>-0.101179</td>\n",
              "      <td>-0.026113</td>\n",
              "      <td>-0.163115</td>\n",
              "      <td>0.047714</td>\n",
              "      <td>-0.059092</td>\n",
              "      <td>-0.000953</td>\n",
              "      <td>0.066118</td>\n",
              "      <td>0.071290</td>\n",
              "      <td>0.021352</td>\n",
              "      <td>0.038844</td>\n",
              "      <td>0.069540</td>\n",
              "      <td>0.061056</td>\n",
              "      <td>-0.009095</td>\n",
              "      <td>0.086287</td>\n",
              "      <td>0.071161</td>\n",
              "      <td>-0.041962</td>\n",
              "      <td>0.025281</td>\n",
              "      <td>0.079294</td>\n",
              "      <td>-0.082179</td>\n",
              "      <td>0.017093</td>\n",
              "      <td>-0.093254</td>\n",
              "      <td>0.031321</td>\n",
              "      <td>0.093794</td>\n",
              "      <td>0.016454</td>\n",
              "      <td>-0.053786</td>\n",
              "      <td>50.529412</td>\n",
              "      <td>1.124702</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2493</th>\n",
              "      <td>2605</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>0.016577</td>\n",
              "      <td>0.060966</td>\n",
              "      <td>-0.087064</td>\n",
              "      <td>0.046408</td>\n",
              "      <td>0.029821</td>\n",
              "      <td>0.040159</td>\n",
              "      <td>0.627986</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>-0.015949</td>\n",
              "      <td>0.013601</td>\n",
              "      <td>-0.004045</td>\n",
              "      <td>-0.050633</td>\n",
              "      <td>-0.638278</td>\n",
              "      <td>-0.092950</td>\n",
              "      <td>0.059763</td>\n",
              "      <td>-0.007455</td>\n",
              "      <td>-0.027335</td>\n",
              "      <td>0.050195</td>\n",
              "      <td>-0.072600</td>\n",
              "      <td>-0.118941</td>\n",
              "      <td>0.043963</td>\n",
              "      <td>0.028052</td>\n",
              "      <td>-0.020707</td>\n",
              "      <td>0.017013</td>\n",
              "      <td>-0.100314</td>\n",
              "      <td>0.924332</td>\n",
              "      <td>0.066779</td>\n",
              "      <td>0.184031</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>-0.087042</td>\n",
              "      <td>0.082218</td>\n",
              "      <td>-0.084034</td>\n",
              "      <td>-0.053479</td>\n",
              "      <td>0.020944</td>\n",
              "      <td>-0.107375</td>\n",
              "      <td>0.062292</td>\n",
              "      <td>0.004020</td>\n",
              "      <td>0.050163</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023813</td>\n",
              "      <td>0.060147</td>\n",
              "      <td>-0.026382</td>\n",
              "      <td>-0.114827</td>\n",
              "      <td>-0.027009</td>\n",
              "      <td>0.027002</td>\n",
              "      <td>-0.049506</td>\n",
              "      <td>0.053767</td>\n",
              "      <td>0.003976</td>\n",
              "      <td>0.020552</td>\n",
              "      <td>-0.024567</td>\n",
              "      <td>-0.095950</td>\n",
              "      <td>-0.102344</td>\n",
              "      <td>-0.014801</td>\n",
              "      <td>-0.154797</td>\n",
              "      <td>0.043244</td>\n",
              "      <td>-0.085830</td>\n",
              "      <td>0.016672</td>\n",
              "      <td>0.063306</td>\n",
              "      <td>0.065970</td>\n",
              "      <td>-0.007496</td>\n",
              "      <td>0.042143</td>\n",
              "      <td>0.109708</td>\n",
              "      <td>0.071727</td>\n",
              "      <td>-0.017870</td>\n",
              "      <td>0.076087</td>\n",
              "      <td>0.060924</td>\n",
              "      <td>-0.063255</td>\n",
              "      <td>-0.002328</td>\n",
              "      <td>0.090009</td>\n",
              "      <td>-0.110579</td>\n",
              "      <td>0.055245</td>\n",
              "      <td>-0.057162</td>\n",
              "      <td>0.068616</td>\n",
              "      <td>0.101482</td>\n",
              "      <td>-0.015338</td>\n",
              "      <td>-0.062622</td>\n",
              "      <td>49.250000</td>\n",
              "      <td>1.165649</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>2606</td>\n",
              "      <td>1.571429</td>\n",
              "      <td>0.027545</td>\n",
              "      <td>0.102103</td>\n",
              "      <td>-0.087128</td>\n",
              "      <td>0.025585</td>\n",
              "      <td>0.010081</td>\n",
              "      <td>0.062728</td>\n",
              "      <td>0.650946</td>\n",
              "      <td>0.008557</td>\n",
              "      <td>-0.010532</td>\n",
              "      <td>0.013319</td>\n",
              "      <td>-0.033369</td>\n",
              "      <td>-0.055501</td>\n",
              "      <td>-0.609024</td>\n",
              "      <td>-0.084756</td>\n",
              "      <td>0.033530</td>\n",
              "      <td>-0.018587</td>\n",
              "      <td>-0.018317</td>\n",
              "      <td>0.088617</td>\n",
              "      <td>-0.088441</td>\n",
              "      <td>-0.114973</td>\n",
              "      <td>0.047151</td>\n",
              "      <td>0.022768</td>\n",
              "      <td>-0.068239</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>-0.096126</td>\n",
              "      <td>0.889699</td>\n",
              "      <td>0.070293</td>\n",
              "      <td>0.139533</td>\n",
              "      <td>0.015974</td>\n",
              "      <td>-0.090256</td>\n",
              "      <td>0.050364</td>\n",
              "      <td>-0.099643</td>\n",
              "      <td>-0.038802</td>\n",
              "      <td>0.019065</td>\n",
              "      <td>-0.100004</td>\n",
              "      <td>0.070795</td>\n",
              "      <td>0.012292</td>\n",
              "      <td>0.015073</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036156</td>\n",
              "      <td>0.075096</td>\n",
              "      <td>-0.039093</td>\n",
              "      <td>-0.078704</td>\n",
              "      <td>0.006561</td>\n",
              "      <td>0.020117</td>\n",
              "      <td>-0.029605</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>-0.004767</td>\n",
              "      <td>0.030869</td>\n",
              "      <td>-0.015064</td>\n",
              "      <td>-0.095884</td>\n",
              "      <td>-0.097213</td>\n",
              "      <td>-0.040490</td>\n",
              "      <td>-0.167914</td>\n",
              "      <td>0.057458</td>\n",
              "      <td>-0.062123</td>\n",
              "      <td>-0.006414</td>\n",
              "      <td>0.057779</td>\n",
              "      <td>0.056900</td>\n",
              "      <td>0.026271</td>\n",
              "      <td>0.029138</td>\n",
              "      <td>0.068665</td>\n",
              "      <td>0.062784</td>\n",
              "      <td>-0.039613</td>\n",
              "      <td>0.094178</td>\n",
              "      <td>0.077215</td>\n",
              "      <td>-0.044480</td>\n",
              "      <td>0.013093</td>\n",
              "      <td>0.072874</td>\n",
              "      <td>-0.092804</td>\n",
              "      <td>0.031574</td>\n",
              "      <td>-0.079673</td>\n",
              "      <td>0.076916</td>\n",
              "      <td>0.104879</td>\n",
              "      <td>0.033944</td>\n",
              "      <td>-0.061523</td>\n",
              "      <td>86.885714</td>\n",
              "      <td>1.219102</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2495 rows × 205 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      doctorID    rating      AWE1  ...     Length  Lexical Diversity  Recommend\n",
              "0            0  4.450617 -0.004739  ...  43.753086           1.105869          1\n",
              "1            1  2.810811  0.017838  ...  48.432432           1.105801          0\n",
              "2            2  4.904762 -0.025036  ...  77.571429           1.171556          1\n",
              "3            3  2.416667  0.038018  ...  52.833333           1.160025          0\n",
              "4            4  3.000000  0.059764  ...  38.500000           1.047309          0\n",
              "...        ...       ...       ...  ...        ...                ...        ...\n",
              "2490      2602  3.627907  0.003801  ...  45.174419           1.091651          0\n",
              "2491      2603  3.568182  0.008686  ...  53.454545           1.147635          0\n",
              "2492      2604  2.441176  0.017475  ...  50.529412           1.124702          0\n",
              "2493      2605  1.750000  0.016577  ...  49.250000           1.165649          0\n",
              "2494      2606  1.571429  0.027545  ...  86.885714           1.219102          0\n",
              "\n",
              "[2495 rows x 205 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEKNvFwHqBYh",
        "outputId": "81543ac6-444a-471a-c349-e7a3d163d1e0"
      },
      "source": [
        "!pip install scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score, make_scorer, roc_auc_score, precision_score, recall_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold,cross_val_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtfahS0_q5aV"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZSaJ7VzsKHJ"
      },
      "source": [
        "scorers = {\n",
        "    'f1_score': make_scorer(f1_score),\n",
        "    'accuracy_score': make_scorer(accuracy_score),\n",
        "    'auc_score': make_scorer(roc_auc_score)\n",
        "}\n",
        "\n",
        "def grid_search_logit(X_train, X_test, y_train, y_test, param_grid, refit_score='auc_score'):\n",
        "    \"\"\"\n",
        "    fits a GridSearchCV classifier using refit_score for optimization\n",
        "    prints classifier performance metrics\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=10, random_state = 4561) \n",
        "    log = LogisticRegression()\n",
        "    grid_search = GridSearchCV(log, param_grid, scoring=scorers, refit=refit_score,\n",
        "                               cv=skf, return_train_score=True, n_jobs = -1, verbose =1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print ('Best score: %0.3f'% grid_search.best_score_)\n",
        "    best_parameters =  grid_search.best_estimator_.get_params()\n",
        "    print ('Best parameters set:')\n",
        "    for param_name in sorted(param_grid.keys()):\n",
        "      print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
        "\n",
        "    predictions = grid_search.predict(X_test)\n",
        "    print (classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK1LftqEwUbw"
      },
      "source": [
        "def grid_search_gbc(X_train, X_test, y_train, y_test, param_grid, refit_score='auc_score'):\n",
        "    \"\"\"\n",
        "    fits a GridSearchCV classifier using refit_score for optimization\n",
        "    prints classifier performance metrics\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=10, random_state = 4561) \n",
        "    gbc = GradientBoostingClassifier()\n",
        "    grid_search = GridSearchCV(gbc, param_grid, scoring=scorers, refit=refit_score,\n",
        "                               cv=skf, return_train_score=True, n_jobs = -1, verbose =1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print ('Best score: %0.3f'% grid_search.best_score_)\n",
        "    best_parameters =  grid_search.best_estimator_.get_params()\n",
        "    print ('Best parameters set:')\n",
        "    for param_name in sorted(param_grid.keys()):\n",
        "      print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
        "\n",
        "    predictions = grid_search.predict(X_test)\n",
        "    print (classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJSpcbjo2fjB"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "def grid_search_SVM(X_train, X_test, y_train, y_test, param_grid, refit_score='auc_score'):\n",
        "    \"\"\"\n",
        "    fits a GridSearchCV classifier using refit_score for optimization\n",
        "    prints classifier performance metrics\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=10, random_state = 4561) \n",
        "    grid_search = GridSearchCV(SVC(), param_grid, scoring=scorers, refit=refit_score,\n",
        "                               cv=skf, return_train_score=True, n_jobs = -1, verbose =1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print ('Best score: %0.3f'% grid_search.best_score_)\n",
        "    best_parameters =  grid_search.best_estimator_.get_params()\n",
        "    print ('Best parameters set:')\n",
        "    for param_name in sorted(param_grid.keys()):\n",
        "      print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
        "\n",
        "    predictions = grid_search.predict(X_test)\n",
        "    print (classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XrnQiJzAF6F"
      },
      "source": [
        "def acc_report(model, X_test, y_test):\n",
        "  predictions = model.predict(X_test)\n",
        "  auc = roc_auc_score(y_test, predictions)\n",
        "  f1 = f1_score(y_test, predictions)\n",
        "  precision  = precision_score(y_test, predictions)\n",
        "  recall = (recall_score(y_test, predictions))\n",
        "  accuracy = (accuracy_score(y_test, predictions))\n",
        "  report = [[accuracy, precision, recall, auc, f1]]\n",
        "  report_df = pd.DataFrame(report, columns = ['accuracy', 'precision', 'recall',' auc', 'f1'])\n",
        "  return report_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0HfjpAZlrc6"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-dbdcZUphpK"
      },
      "source": [
        "X = df_glove_agg.iloc[:, 3:203]\n",
        "y = df_glove_agg['Recommend']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfnxcMQ0qCwU"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =  .25, random_state = 123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frQHL9cPs2nP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "97ede0f3-2ffe-4a4e-f0b0-caab22172d60"
      },
      "source": [
        "params = {'C':[0.1, 1, 10], 'penalty':['l1','l2' 'elasticnet', 'none']}\n",
        "grid_search_logit(X_train, X_test, y_train, y_test,params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1a6cd72dda6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'penalty'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'l2'\u001b[0m \u001b[0;34m'elasticnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrid_search_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-7be00c610a42>\u001b[0m in \u001b[0;36mgrid_search_logit\u001b[0;34m(X_train, X_test, y_train, y_test, param_grid, refit_score)\u001b[0m\n\u001b[1;32m     14\u001b[0m     grid_search = GridSearchCV(log, param_grid, scoring=scorers, refit=refit_score,\n\u001b[1;32m     15\u001b[0m                                cv=skf, return_train_score=True, n_jobs = -1, verbose =1)\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Best score: %0.3f'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnY239UTwZOD"
      },
      "source": [
        "params = {'learning_rate' : [0.01, 0.1, 1], 'n_estimators' :[100, 200, 500], 'max_features' : ['sqrt', 'log2'],'min_samples_leaf' :[20,30,40]}\n",
        "grid_search_gbc(X_train, X_test, y_train, y_test,params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP1IZ0vJ2t5x"
      },
      "source": [
        "params = {'C' : [0.1, 1, 10], 'gamma' : [1, 0.1, 0.01], 'kernel' : ['rbf', 'poly', 'sigmoid']}\n",
        "grid_search_SVM(X_train, X_test, y_train, y_test, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nctlpnrSAVW4",
        "outputId": "61f6827f-29eb-4fd3-a3e9-f75a67811491"
      },
      "source": [
        "log_best_att = LogisticRegression(C = 0.1, penalty='none')\n",
        "log_best_att.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='none',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV9jxdMyAZcI",
        "outputId": "91b95220-5868-40bd-d2d4-56edc8461890"
      },
      "source": [
        "gbc_best_att = GradientBoostingClassifier(learning_rate=.1, max_features = 'log2', min_samples_leaf=20,n_estimators=200)\n",
        "gbc_best_att.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features='log2', max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=20, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvkGuEonAg3b",
        "outputId": "d7bbeb9f-d7b0-411d-bd07-63ef785e5919"
      },
      "source": [
        "svm_best_att = SVC(C=10, gamma=1, kernel='rbf')\n",
        "svm_best_att.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
              "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
              "    verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "f039A_D_Apsp",
        "outputId": "f46bfd7a-6363-494a-a9bd-5ed405c4bcd3"
      },
      "source": [
        "acc_report(log_best_att, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.886218</td>\n",
              "      <td>0.816568</td>\n",
              "      <td>0.775281</td>\n",
              "      <td>0.852887</td>\n",
              "      <td>0.795389</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall       auc        f1\n",
              "0  0.886218   0.816568  0.775281  0.852887  0.795389"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "U79vrmvwAt4A",
        "outputId": "b7e87aa8-092c-4551-ba73-ca1e3ec18db5"
      },
      "source": [
        "acc_report(gbc_best_att, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.875</td>\n",
              "      <td>0.8125</td>\n",
              "      <td>0.730337</td>\n",
              "      <td>0.831536</td>\n",
              "      <td>0.769231</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall       auc        f1\n",
              "0     0.875     0.8125  0.730337  0.831536  0.769231"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "ReThBzq6AmkW",
        "outputId": "10a1e37a-f2e3-41ee-b247-3a4a03ef9272"
      },
      "source": [
        "acc_report(svm_best_att, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.895833</td>\n",
              "      <td>0.819209</td>\n",
              "      <td>0.814607</td>\n",
              "      <td>0.871429</td>\n",
              "      <td>0.816901</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall       auc        f1\n",
              "0  0.895833   0.819209  0.814607  0.871429  0.816901"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxko2uAtvbOz"
      },
      "source": [
        "### With SVD on GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJaMKgjYtudW"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0TY7iEnt1LM"
      },
      "source": [
        "def variance_explained(X, end=250, max_var = .60):\n",
        "  var_explained = []\n",
        "  for i in range(1, end):\n",
        "    svd = TruncatedSVD(n_components= i)\n",
        "    svd.fit(X) \n",
        "    var_explained.append(svd.explained_variance_ratio_.sum())\n",
        "  # Plot the variance explained \n",
        "  x_axis = [i for i in range(1, end)]\n",
        "  plt.plot(x_axis, var_explained)\n",
        "  plt.axhline(y=max_var, color='r', linestyle='-')\n",
        "  plt.axis ([1, end, 0, 1])\n",
        "  plt.title(\"Variance explained in N components\")\n",
        "  plt.xlabel(\"Number of components\")\n",
        "  plt.ylabel(\"Total variance explained\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pxGgEgYeubNK",
        "outputId": "c5b8b80f-1544-4ed3-d941-ae1f782556db"
      },
      "source": [
        "variance_explained(X, end = 100, max_var = .8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ3nnBEiuu9F",
        "outputId": "ca787476-98e4-44b9-835b-347762935d21"
      },
      "source": [
        "svd = TruncatedSVD(n_components=50)\n",
        "svd.fit(X)\n",
        "svd.explained_variance_ratio_.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8036009159771638"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymedFlGbu98B"
      },
      "source": [
        "X_svd = svd.transform(X)\n",
        "X_train_svd, X_test_svd, y_train_svd, y_test_svd = train_test_split(X_svd,y, test_size =  .25, random_state = 123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsV2FUAUvgNZ",
        "outputId": "f5cc92f0-316a-4ca8-d555-182b6127d144"
      },
      "source": [
        "params = {'C':[0.1, 1, 10], 'penalty':['l1','l2' 'elasticnet', 'none']}\n",
        "grid_search_logit(X_train_svd, X_test_svd, y_train_svd, y_test_svd,params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
            "Best score: 0.849\n",
            "Best parameters set:\n",
            "\tC: 0.1\n",
            "\tpenalty: 'none'\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       446\n",
            "           1       0.83      0.78      0.80       178\n",
            "\n",
            "    accuracy                           0.89       624\n",
            "   macro avg       0.87      0.86      0.86       624\n",
            "weighted avg       0.89      0.89      0.89       624\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    1.9s finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:360: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring().strip(b'\\x00').strip()\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG6xg4TcweOm",
        "outputId": "f1b4d0d6-631f-46f1-8b1d-269c0ab8e5e4"
      },
      "source": [
        "params = {'learning_rate' : [0.01, 0.1, 1], 'n_estimators' :[100, 200, 500], 'max_features' : ['sqrt', 'log2'],'min_samples_leaf' :[20,30,40]}\n",
        "grid_search_gbc(X_train_svd, X_test_svd, y_train_svd, y_test_svd,params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   38.7s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  6.4min\n",
            "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  7.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.832\n",
            "Best parameters set:\n",
            "\tlearning_rate: 0.1\n",
            "\tmax_features: 'sqrt'\n",
            "\tmin_samples_leaf: 40\n",
            "\tn_estimators: 500\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91       446\n",
            "           1       0.81      0.71      0.75       178\n",
            "\n",
            "    accuracy                           0.87       624\n",
            "   macro avg       0.85      0.82      0.83       624\n",
            "weighted avg       0.87      0.87      0.87       624\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdG-M1QI23gC",
        "outputId": "2e2be43c-9616-4a82-a7f5-eea95f32084b"
      },
      "source": [
        "params = {'C' : [0.1, 1, 10], 'gamma' : [1, 0.1, 0.01], 'kernel' : ['rbf', 'poly', 'sigmoid']}\n",
        "grid_search_SVM(X_train_svd, X_test_svd, y_train_svd, y_test_svd, params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   10.3s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   57.8s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.857\n",
            "Best parameters set:\n",
            "\tC: 10\n",
            "\tgamma: 0.1\n",
            "\tkernel: 'rbf'\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92       446\n",
            "           1       0.85      0.75      0.79       178\n",
            "\n",
            "    accuracy                           0.89       624\n",
            "   macro avg       0.88      0.85      0.86       624\n",
            "weighted avg       0.89      0.89      0.89       624\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q59-x-M_WK2w",
        "outputId": "d15d7adc-c48f-4f50-adbc-9f8dc764cdb3"
      },
      "source": [
        "log_best_att = LogisticRegression(C = 0.1, penalty='none')\n",
        "log_best_att.fit(X_train_svd,y_train_svd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='none',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZERbWR9WbMR",
        "outputId": "04364482-6a8d-46f8-a544-8d1c46b0c8f6"
      },
      "source": [
        "gbc_best_att = GradientBoostingClassifier(learning_rate=.1, max_features = 'log2', min_samples_leaf=40,n_estimators=500)\n",
        "gbc_best_att.fit(X_train_svd, y_train_svd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features='log2', max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=40, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCNHGf2FWnSV",
        "outputId": "44e64fa7-a52c-4164-ecb9-160797ac5efe"
      },
      "source": [
        "svm_best_att = SVC(C=10, gamma=1, kernel='rbf')\n",
        "svm_best_att.fit(X_train_svd,y_train_svd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
              "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
              "    verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "VZndMUUPWTYN",
        "outputId": "beab0808-c2c1-4e2c-f29c-cb1019840dea"
      },
      "source": [
        "acc_report(log_best_att, X_test_svd, y_test_svd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.891026</td>\n",
              "      <td>0.831325</td>\n",
              "      <td>0.775281</td>\n",
              "      <td>0.85625</td>\n",
              "      <td>0.802326</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall      auc        f1\n",
              "0  0.891026   0.831325  0.775281  0.85625  0.802326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "2wD29SGfWqAd",
        "outputId": "a0ac3fe5-dd43-405f-fc82-23fba24d2bf8"
      },
      "source": [
        "acc_report(gbc_best_att, X_test_svd, y_test_svd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.866987</td>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.674157</td>\n",
              "      <td>0.809052</td>\n",
              "      <td>0.743034</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall       auc        f1\n",
              "0  0.866987   0.827586  0.674157  0.809052  0.743034"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "jSXwcgW5WqdA",
        "outputId": "3120994c-4e30-47a0-bcda-bb9f60656a8c"
      },
      "source": [
        "acc_report(svm_best_att, X_test_svd, y_test_svd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.886218</td>\n",
              "      <td>0.836478</td>\n",
              "      <td>0.747191</td>\n",
              "      <td>0.844448</td>\n",
              "      <td>0.789318</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall       auc        f1\n",
              "0  0.886218   0.836478  0.747191  0.844448  0.789318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G_OwUvxa_Qa"
      },
      "source": [
        "### Other Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gl9qtRu9Om4",
        "outputId": "1bad4333-5f22-4503-8282-d9e08aa4a672"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "df_attributes_agg = pd.read_pickle('df_w_attributes_agg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts49aOdIbByJ"
      },
      "source": [
        "df_attributes_agg = df_attributes_agg.assign(Recommend = [1 if x >= 4 else 0 for x in df_attributes_agg['rating'] ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSsA2ghWbaY1"
      },
      "source": [
        "X_att = df_attributes_agg.iloc[:,2:-1]\n",
        "y = df_attributes_agg['Recommend']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViC6mdOXc9Ni"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_att,y, test_size =  .25, random_state = 123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQo6B9yydV4m",
        "outputId": "0959adf8-20f9-4220-8abc-401bcd50ee94"
      },
      "source": [
        "params = {'C':[0.1, 1, 10], 'penalty':['l1','l2' 'elasticnet', 'none']}\n",
        "grid_search_logit(X_train, X_test, y_train, y_test,params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    4.9s finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.875\n",
            "Best parameters set:\n",
            "\tC: 0.1\n",
            "\tpenalty: 'none'\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94       446\n",
            "           1       0.88      0.79      0.83       178\n",
            "\n",
            "    accuracy                           0.91       624\n",
            "   macro avg       0.90      0.87      0.88       624\n",
            "weighted avg       0.91      0.91      0.91       624\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:339: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py:360: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  task_str = task.tostring().strip(b'\\x00').strip()\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-YkPdh6dXS0",
        "outputId": "b29b8e8b-2941-4ed8-b867-15924c6345b1"
      },
      "source": [
        "params = {'learning_rate' : [0.01, 0.1, 1], 'n_estimators' :[100, 200, 500], 'max_features' : ['sqrt', 'log2'],'min_samples_leaf' :[20,30,40]}\n",
        "grid_search_gbc(X_train, X_test, y_train, y_test,params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   25.2s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  4.2min\n",
            "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  4.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.877\n",
            "Best parameters set:\n",
            "\tlearning_rate: 0.1\n",
            "\tmax_features: 'log2'\n",
            "\tmin_samples_leaf: 30\n",
            "\tn_estimators: 200\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94       446\n",
            "           1       0.90      0.79      0.84       178\n",
            "\n",
            "    accuracy                           0.92       624\n",
            "   macro avg       0.91      0.88      0.89       624\n",
            "weighted avg       0.92      0.92      0.91       624\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtyMZq1GzaUR",
        "outputId": "f04dcc61-c31c-46b0-d5c1-f202e8fa9fa8"
      },
      "source": [
        "params = {'C' : [0.1, 1, 10], 'gamma' : [1, 0.1, 0.01], 'kernel' : ['rbf', 'poly', 'sigmoid']}\n",
        "grid_search_SVM(X_train, X_test, y_train, y_test, params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    8.1s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   26.8s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.866\n",
            "Best parameters set:\n",
            "\tC: 10\n",
            "\tgamma: 0.01\n",
            "\tkernel: 'rbf'\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94       446\n",
            "           1       0.90      0.75      0.82       178\n",
            "\n",
            "    accuracy                           0.91       624\n",
            "   macro avg       0.90      0.86      0.88       624\n",
            "weighted avg       0.91      0.91      0.90       624\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:   34.3s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHWbHnfk95mm",
        "outputId": "cae339ce-0256-49d1-88a4-89a86d278666"
      },
      "source": [
        "log_best_att = LogisticRegression(C = 0.1, penalty='none')\n",
        "log_best_att.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='none',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkI6dX0e9tD6",
        "outputId": "fc09b06d-315f-4e58-8baa-4943329eb4ef"
      },
      "source": [
        "gbc_best_att = GradientBoostingClassifier(learning_rate=.1, max_features = 'log2', min_samples_leaf=30,n_estimators=200)\n",
        "gbc_best_att.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features='log2', max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=30, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0AAne9d9iqt",
        "outputId": "fb65e2f9-4cf3-4b34-fd3b-5ad88c8cd5ca"
      },
      "source": [
        "svm_best_att = SVC(C=10, gamma=0.01, kernel='rbf')\n",
        "svm_best_att.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6cmTlS-FAI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "nBry5EOQ-9AL",
        "outputId": "5ad720d2-a292-43d0-8f88-3f65c19cf47c"
      },
      "source": [
        "acc_report(log_best_att, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.908654</td>\n",
              "      <td>0.875776</td>\n",
              "      <td>0.792135</td>\n",
              "      <td>0.873646</td>\n",
              "      <td>0.831858</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall       auc        f1\n",
              "0  0.908654   0.875776  0.792135  0.873646  0.831858"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "7sG4cGY8_Bc9",
        "outputId": "2e72cefb-9d93-4ae5-83be-028b548160dd"
      },
      "source": [
        "acc_report(gbc_best_att, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.921474</td>\n",
              "      <td>0.90566</td>\n",
              "      <td>0.808989</td>\n",
              "      <td>0.887678</td>\n",
              "      <td>0.854599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall       auc        f1\n",
              "0  0.921474    0.90566  0.808989  0.887678  0.854599"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "uFSMCMYS_DLi",
        "outputId": "d4d3ce4b-ad0f-4f74-d09f-9f04f9d43c1d"
      },
      "source": [
        "acc_report(svm_best_att, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>auc</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.88141</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.730337</td>\n",
              "      <td>0.836021</td>\n",
              "      <td>0.778443</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision    recall       auc        f1\n",
              "0   0.88141   0.833333  0.730337  0.836021  0.778443"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5vUXiWrvqE8"
      },
      "source": [
        "### Combined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyBiMyNTwIv1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "8702fae7-8068-4e77-a335-d8e9b7442d42"
      },
      "source": [
        "df_attributes_agg = df_attributes_agg.drop(['Length','Lexical Diversity', 'Recommend'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3aba2b76b11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_attributes_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_attributes_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Lexical Diversity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Recommend'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_attributes_agg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tZKNrhHvr__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "020aa788-41d5-43b4-ea9a-542fcf29917d"
      },
      "source": [
        "df_comb= pd.merge(left = df_attributes_agg, right = df_glove_agg, left_on = \"doctorID\", right_on = \"doctorID\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b57bb89798b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_comb\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_attributes_agg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_glove_agg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"doctorID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"doctorID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_attributes_agg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh5uUwXXwQxw"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "\n",
        "df_comb.to_pickle('df_comb')\n",
        "df_comb.to_csv('df_comb.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4iJuTFNnjr3"
      },
      "source": [
        "df_comb = pd.read_pickle('df_comb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_LsCYWKn0p3"
      },
      "source": [
        "df_comb.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG8I3GpFybhk"
      },
      "source": [
        "X_comb = df_comb.iloc[:,2:-1]\n",
        "X_comb = X_comb.drop('rating_y', axis = 1)\n",
        "y = df_comb['Recommend']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf112VbXynY_"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_comb,y, test_size =  .25, random_state = 123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIzXrIRysfk"
      },
      "source": [
        "params = {'C':[0.1, 1, 10], 'penalty':['l1','l2' 'elasticnet', 'none']}\n",
        "grid_search_logit(X_train, X_test, y_train, y_test,params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCjUvFc7zKvz"
      },
      "source": [
        "params = {'learning_rate' : [0.01, 0.1, 1], 'n_estimators' :[100, 200, 500], 'max_features' : ['sqrt', 'log2'],'min_samples_leaf' :[20,30,40]}\n",
        "grid_search_gbc(X_train, X_test, y_train, y_test,params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJOXe1C-zN9r"
      },
      "source": [
        "params = {'C' : [0.1, 1, 10], 'gamma' : [1, 0.1, 0.01], 'kernel' : ['rbf', 'poly', 'sigmoid']}\n",
        "grid_search_SVM(X_train, X_test, y_train, y_test, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MTNQN3u128q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b98b16-12bc-47a8-efac-872c3305b4c2"
      },
      "source": [
        "gbc_best = GradientBoostingClassifier(learning_rate=.1, max_features = 'sqrt', min_samples_leaf=40,n_estimators=500)\n",
        "gbc_best.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features='sqrt', max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=40, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usUYG5SP270L"
      },
      "source": [
        "svm_best = SVC(C=1, gamma=0.01, kernel='poly')\n",
        "svm_best.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFedDKRuCn2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59223cf3-95b7-475f-c03a-cd0f249f162a"
      },
      "source": [
        "log_best = LogisticRegression(C = 0.1, penalty='none')\n",
        "log_best.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1505: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='none',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc2hGBL22eW-"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "pickle.dump(gbc_best,open('gbc_best.pkl','wb'))\n",
        "pickle.dump(log_best,open('log_best.pkl','wb'))\n",
        "pickle.dump(svm_best,open('svm_best.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT_imytjju8E",
        "outputId": "efff4b19-9a26-42fa-8f89-7a7f24b59a81"
      },
      "source": [
        "%cd /content/drive/MyDrive/NLP Final Project/\n",
        "import pickle\n",
        "gbc_best = pickle.load(open('gbc_best.pkl','rb'))\n",
        "log_best = pickle.load(open('log_best.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP45VDBa7N4O",
        "outputId": "2b0311bb-b34d-4c38-d982-632a729b5d9d"
      },
      "source": [
        "predictions = log_best.predict(X_test)\n",
        "print (classification_report(y_test, predictions))\n",
        "print(roc_auc_score(y_test, predictions))\n",
        "print(f1_score(y_test, predictions))\n",
        "print(precision_score(y_test, predictions))\n",
        "print(recall_score(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94       446\n",
            "           1       0.87      0.81      0.84       178\n",
            "\n",
            "    accuracy                           0.91       624\n",
            "   macro avg       0.90      0.88      0.89       624\n",
            "weighted avg       0.91      0.91      0.91       624\n",
            "\n",
            "0.8837607698896557\n",
            "0.8430232558139534\n",
            "0.8734939759036144\n",
            "0.8146067415730337\n",
            "0.9134615384615384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T8mr7a67Xfr",
        "outputId": "02485b34-3671-43c5-c618-50bbd9b32e3e"
      },
      "source": [
        "predictions = gbc_best.predict(X_test)\n",
        "print (classification_report(y_test, predictions))\n",
        "print(roc_auc_score(y_test, predictions))\n",
        "print(f1_score(y_test, predictions))\n",
        "print(precision_score(y_test, predictions))\n",
        "print(recall_score(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94       446\n",
            "           1       0.88      0.79      0.83       178\n",
            "\n",
            "    accuracy                           0.91       624\n",
            "   macro avg       0.90      0.87      0.88       624\n",
            "weighted avg       0.91      0.91      0.91       624\n",
            "\n",
            "0.8736458910666599\n",
            "0.831858407079646\n",
            "0.8757763975155279\n",
            "0.7921348314606742\n",
            "0.9086538461538461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "7Nv4bR7d6Rky",
        "outputId": "876d3e01-b92c-419c-a3e8-e7940bac8b2f"
      },
      "source": [
        "predictions = svm_best.predict(X_test)\n",
        "print (classification_report(y_test, predictions))\n",
        "print(roc_auc_score(y_test, predictions))\n",
        "print(f1_score(y_test, predictions))\n",
        "print(precision_score(y_test, predictions))\n",
        "print(recall_score(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-057a20a48629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'svm_best' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4DYGKR1nMSr"
      },
      "source": [
        "feat_imp = pd.Series(gbc_best.feature_importances_, X_comb.columns).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWTlkymzIDb_"
      },
      "source": [
        "mean = np.mean(feat_imp)\n",
        "sd = np.std(feat_imp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQC95YIiHw91"
      },
      "source": [
        "def z_score(score, mean, sd):\n",
        "  return (score - mean)/sd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-lKaLDMCQVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb90e75d-a334-4dcd-ccfe-8e7617044c05"
      },
      "source": [
        "feat_imp.apply(lambda x: z_score(x, mean, sd)).head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NLTK_Compound    9.036773\n",
              "AWE110           5.223874\n",
              "polarity         4.873447\n",
              "AWE128           4.256393\n",
              "AWE96            4.222326\n",
              "AWE114           3.448999\n",
              "Topic_9          2.799069\n",
              "AWE164           1.859992\n",
              "AWE63            1.659525\n",
              "AWE113           1.628144\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPIanXq4DKjB"
      },
      "source": [
        "feat_imp_log = pd.Series(log_best.coef_.tolist()[0], X_comb.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Lno4NjD8HI"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIN1BrMgDjeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b252beeb-b9bd-4888-e2fe-340471e231eb"
      },
      "source": [
        "feat_imp_log.apply(np.exp).head(14)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Topic_0     0.755599\n",
              "Topic_1     1.123145\n",
              "Topic_2     0.969296\n",
              "Topic_3     0.490265\n",
              "Topic_4     0.884547\n",
              "Topic_5     0.444070\n",
              "Topic_6     1.341638\n",
              "Topic_7     0.491607\n",
              "Topic_8     1.049300\n",
              "Topic_9     2.522715\n",
              "Topic_10    1.972786\n",
              "Topic_11    1.721536\n",
              "Topic_12    0.951497\n",
              "Topic_13    0.946457\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtQBV1jxUl5J"
      },
      "source": [
        "# Task 7"
      ]
    }
  ]
}